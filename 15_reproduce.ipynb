{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps to reproduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. Conversion\n",
    "\n",
    "- batch reading\n",
    "- storing in batches to .pkl files, divided by column sets:\n",
    "    - sf - simple features\n",
    "    - tr - train features\n",
    "    - af - array feautures\n",
    "    - afexp - array features normalized (i.e. array elements stored as independent rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from common import (\n",
    "    SIMPLE_FEATURE_COLS, ARR_FEATURE_COLS, ALL_TRAIN_COLS\n",
    ")\n",
    "\n",
    "class DatasetMetaData:\n",
    "    def __init__(self, origin_csv_filenames, chunk_filenames_pattern, origin_col_set):\n",
    "        self.origin_csv_filenames = origin_csv_filenames\n",
    "        self.chunk_filenames_pattern = chunk_filenames_pattern\n",
    "        self.origin_col_set = origin_col_set\n",
    "        self.is_test = 'test_' in chunk_filenames_pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Важно**. В коде ниже в поле `origin_csv_filenames` указаны пути к файлам с тренировочными данными! Имена файлов как в оригинале, файлы лежат в папке `data/`. Если у вас иначе, поправьте.\n",
    "\n",
    "То же самое с отконвертированными pickle-файлами - они будут сохранены в папке `data/`, см. chunk_filenames_pattern. Если нужно, поправьте под себя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io_tools import DatasetConverter\n",
    "\n",
    "meta_train = DatasetMetaData(\n",
    "    origin_csv_filenames=['data/train_part_1_v2.csv.gz', 'data/train_part_2_v2.csv.gz'],\n",
    "    chunk_filenames_pattern='data/train_{label}_{group}_{ind:03d}.pkl',\n",
    "    origin_col_set=SIMPLE_FEATURE_COLS + ARR_FEATURE_COLS + ALL_TRAIN_COLS\n",
    ")\n",
    "\n",
    "DatasetConverter.convert(meta_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Read train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as sts\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from common import *\n",
    "import warnings\n",
    "\n",
    "sns.set()\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "color_palette = sns.color_palette('deep') + sns.color_palette('husl', 6) + sns.color_palette('bright') + sns.color_palette('pastel')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000000, 63)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(421218, 1578782)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from io_tools import DatasetReader, count_classes\n",
    "\n",
    "def read_train(cols, rows, foi_expanded=True):\n",
    "    return DatasetReader.read_dataset(meta_train, cols + train_cols, rows, foi_expanded=foi_expanded)\n",
    "\n",
    "used_cols = xyz_cols + mom_cols + hit_type_cols + dxyz_cols + exy_cols + edxy_cols + hit_stats_cols + t_cols + ncl_cols + avg_cs_cols\n",
    "global_feature_importance = None\n",
    "train, train_foi = read_train(used_cols, 2000000)\n",
    "display(train.shape, count_classes(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. Feature transformers\n",
    "\n",
    "Here we declare all transformers. The code a little bit clunky, sorry. Extracted as is from `transformers/err.py`, `tranformers/cosine.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from common import xy_cols, xyz_cols, ex_cols, ey_cols, exy_cols, dx_cols, dy_cols, edxy_cols, z_cols, t_cols, mom_cols, N_STATIONS\n",
    "from pipeline import split_classes\n",
    "\n",
    "err_cols = ['ErrMSE', 'DLL']\n",
    "\n",
    "nerr_x_cols = ['NErr_X[%i]' % i for i in range(N_STATIONS)]\n",
    "nerr_y_cols = ['NErr_Y[%i]' % i for i in range(N_STATIONS)]\n",
    "nerr_xy_cols = nerr_x_cols + nerr_y_cols\n",
    "\n",
    "err_x_cols = ['Err_X[%i]' % i for i in range(N_STATIONS)]\n",
    "err_y_cols = ['Err_Y[%i]' % i for i in range(N_STATIONS)]\n",
    "err_z_cols = ['Err_Z[%i]' % i for i in range(N_STATIONS)]\n",
    "err_xy_cols = err_x_cols + err_y_cols\n",
    "err_xyz_cols = err_xy_cols + err_z_cols\n",
    "ez = np.array([15270., 16470., 17670., 18870.])\n",
    "\n",
    "def add_mse(data, features):\n",
    "    dxy = (data.loc[:, xy_cols].values - data.loc[:, exy_cols].values) / data.loc[:, dx_cols + dy_cols].values / 2.\n",
    "    D = np.nanmean(dxy**2, axis=1)\n",
    "    \n",
    "    data.loc[:, err_cols[0]] = D\n",
    "    features += [err_cols[0]]\n",
    "    return data\n",
    "\n",
    "def add_normed_err(data, features):\n",
    "    dxy = data.loc[:, xy_cols].values - data.loc[:, exy_cols].values\n",
    "    normed_errors = dxy / np.sqrt(data.loc[:, edxy_cols].values)\n",
    "    \n",
    "    for i in range(4):\n",
    "        data.loc[:, nerr_x_cols[i]] = normed_errors[:, i]\n",
    "        data.loc[:, nerr_y_cols[i]] = normed_errors[:, i + 4]\n",
    "    \n",
    "    features += nerr_xy_cols\n",
    "    return data\n",
    "\n",
    "def add_errs(data, features):\n",
    "    for err_col, e_col, col in zip (err_xy_cols, exy_cols, xy_cols):\n",
    "        data.loc[:, err_col] = data[e_col].values - data[col].values\n",
    "        \n",
    "    for i in range(4):\n",
    "        data.loc[:, err_z_cols[i]] = ez[i] - data[z_cols[i]].values\n",
    "    \n",
    "    features += err_xyz_cols\n",
    "\n",
    "def create_distr(data):\n",
    "    dts = [dt.loc[:, err_cols[0]] for dt in split_classes(data)]\n",
    "    l, r = np.min(data[err_cols[0]]) - 1e-5, np.max(data[err_cols[0]]) + 1e-5\n",
    "    bins = np.concatenate((\n",
    "        np.arange(l, 1, .02),\n",
    "        np.arange(1, 3, .04),\n",
    "        np.arange(3, 10, .1),\n",
    "        np.arange(10, 16, .4),\n",
    "        np.arange(16, 34, 1.),\n",
    "        np.arange(34, 66, 2),\n",
    "        np.arange(66, 120, 5.),\n",
    "        np.linspace(120, r, 3),\n",
    "    ))\n",
    "    pdfs = []\n",
    "    \n",
    "    for i in range(2):\n",
    "        pdf, _ = np.histogram(dts[i], bins=bins)\n",
    "        pdfs.append(pdf)\n",
    "    \n",
    "    cdfs = [np.cumsum(pdf) for pdf in pdfs]\n",
    "    \n",
    "    return cdfs, pdfs, bins\n",
    "\n",
    "# выбираем опцию, как считать DLL - либо на основе pdf, либо на основе cdf\n",
    "def get_dll_pdf(x, pdfs, cdfs, bins):\n",
    "    def get_probs_pdf(pdf, x):\n",
    "        indices = np.digitize(x, bins) - 1\n",
    "        wbin = (bins[indices + 1] - bins[indices]) / (np.max(bins) - np.min(bins))\n",
    "        prob = pdf[indices] / pdf.sum()\n",
    "        return prob #* wbin\n",
    "\n",
    "    probs = [get_probs_pdf(pdf, x) for pdf in pdfs]\n",
    "    DLL = np.log(probs[1]) - np.log(probs[0])\n",
    "    return DLL\n",
    "\n",
    "def get_dll_cdf(x, pdfs, cdfs, bins):\n",
    "    def get_probs_cdf(cdf, x):\n",
    "        indices = np.digitize(x, bins) - 1\n",
    "        wbin = (bins[indices + 1] - bins[0]) / (np.max(bins) - np.min(bins))\n",
    "        prob = cdf[indices] / cdf[-1]\n",
    "        return prob * wbin\n",
    "    probs = [get_probs_cdf(cdf, x) for cdf in cdfs]\n",
    "    DLL = np.log(probs[1]) - np.log(probs[0])\n",
    "    return DLL\n",
    "\n",
    "def add_dll(data, features):\n",
    "    data[err_cols[1]] = get_dll_pdf(data.loc[:, err_cols[0]], pdfs, cdfs, bins)\n",
    "    features += err_cols[1:2]\n",
    "    return data\n",
    "\n",
    "vm_cols = ['V', 'VT', 'M', 'MT']\n",
    "\n",
    "def add_velocity(data, features):\n",
    "    def get_layer_coords(data, cols, i):\n",
    "        return data[[cols[i], cols[i+4], cols[i+8]]].values\n",
    "    def get_elayer_coords(data, i):\n",
    "        exy = data.loc[:, [ex_cols[i], ey_cols[i]]].values\n",
    "        ez_ = np.tile(ez[i], exy.shape[0]).reshape((-1, 1))\n",
    "        return np.hstack((exy, ez_))\n",
    "    def dot(x, y):\n",
    "        return np.sum(x * y, axis=1, dtype=np.float32)\n",
    "    def norm(x):\n",
    "        return np.sqrt(dot(x, x))\n",
    "    def get_zero_point(data):\n",
    "        layers = [get_elayer_coords(data, i) for i in range(2)]\n",
    "        r = layers[1] - layers[0]\n",
    "        r = r / norm(r)[:, np.newaxis]\n",
    "        p = get_elayer_coords(data, 0)\n",
    "        alpha = - p[:, 2] / r[:, 2]\n",
    "        \n",
    "        xs = p[:, 0] + alpha * r[:, 0]\n",
    "        ys = p[:, 1] + alpha * r[:, 1]\n",
    "        zs = np.tile(0, len(xs))\n",
    "        return np.vstack((xs, ys, zs)).T\n",
    "    \n",
    "    # radius-vector r_i = p_i - p_0: S x N x 3; \n",
    "    r = np.array([get_layer_coords(data, xyz_cols, i) for i in range(4)]) - get_zero_point(data)\n",
    "    \n",
    "    # time: S x N\n",
    "    t = data.loc[:, t_cols].values.T\n",
    "    \n",
    "    # average velocity avg(r / t): N x 3    \n",
    "    v_avg = np.nanmean(r / t[:, :, np.newaxis], axis=0)\n",
    "    # average speed |v|: N\n",
    "    speed = norm(v_avg)\n",
    "    # transverse speed |v_xy|: N\n",
    "    speed_tr = norm(v_avg * np.array([1., 1., 0.]))\n",
    "    \n",
    "    # momentum: N\n",
    "    p = data.loc[:, mom_cols[0]].values\n",
    "    # transverse momentum: N\n",
    "    p_tr = data.loc[:, mom_cols[1]].values\n",
    "    \n",
    "    # mass: N\n",
    "    m = p / speed\n",
    "    # transverse mass: N\n",
    "    m_tr = p_tr / speed_tr\n",
    "\n",
    "    results = [speed, speed_tr, m, m_tr]\n",
    "    for col, res in zip(vm_cols, results):\n",
    "        data.loc[:, col] = res\n",
    "    features += vm_cols\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from common import x_cols, y_cols, z_cols, ex_cols, ey_cols\n",
    "\n",
    "da_cols = ['DAngle[%d]' % i for i in range(1, 4)]\n",
    "\n",
    "def add_coses(data, features):\n",
    "    def get_layer_coords(data, i):\n",
    "        return data[[x_cols[i], y_cols[i], z_cols[i]]].values\n",
    "  \n",
    "    def dot(x, y):\n",
    "        return np.sum(x * y, axis=1, dtype=np.float32)\n",
    "    \n",
    "    def norm(x):\n",
    "        return np.sqrt(dot(x, x))\n",
    "\n",
    "    def get_cosine_dist(L1, L2, L1_norm, L2_norm):\n",
    "        cosines = dot(L1, L2) / L1_norm / L2_norm\n",
    "        return np.clip(cosines, -1., 1.)\n",
    "    \n",
    "    layers = np.array([get_layer_coords(data, i) for i in range(4)])\n",
    "    layers[1:] -= layers[:3]\n",
    "    layers[0] = get_zero_point(data)\n",
    "    \n",
    "    for i in range(3):\n",
    "        cur_layer = layers[i]\n",
    "        next_layer = layers[i+1]\n",
    "        nan_mask = np.isnan(next_layer[:, 0])\n",
    "        next_layer[nan_mask, :] = cur_layer[nan_mask, :]\n",
    "        \n",
    "        cosines = get_cosine_dist(cur_layer, next_layer, norm(cur_layer), norm(next_layer))\n",
    "        degrees = to_degrees(cosines)\n",
    "        cosines[nan_mask] = np.NaN\n",
    "        degrees[nan_mask] = np.NaN\n",
    "        data[da_cols[i]] = degrees\n",
    "        \n",
    "    features += da_cols\n",
    "    return data\n",
    "\n",
    "def to_degrees(cosine):\n",
    "    return np.arccos(cosine) / np.pi * 180.\n",
    "\n",
    "def _to_degrees(cosines):\n",
    "    angles = cosines.copy()\n",
    "    isn_mask = ~np.isnan(cosines)\n",
    "    angles[isn_mask] = np.arccos(cosines[isn_mask]) / np.pi * 180.\n",
    "    return angles\n",
    "\n",
    "def get_zero_point(data):\n",
    "    layers = [data[[ex_cols[i], ey_cols[i], z_cols[i]]].values for i in range(2)]\n",
    "    d = layers[1] - layers[0]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03.1. Build distribution for DLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# готовим данные для распределения DLL\n",
    "\n",
    "# либо загружаем уже готовое\n",
    "# dll_filename = 'data/dll.pkl'\n",
    "# dll_train = pd.read_pickle(dll_filename)\n",
    "# display(dll_train.columns)\n",
    "\n",
    "# либо считаем заново\n",
    "dll_train, _ = read_train(xy_cols + dx_cols + dy_cols + exy_cols, 10000000)\n",
    "dll_train = add_mse(train, [])\n",
    "\n",
    "# опционально пересчитываем MatchedHits и заменяем ими координаты треков в dll_train, чтобы считать распределение на пересчитанных треках\n",
    "# dll_train = replace_hits(dll_train, [])\n",
    "\n",
    "# save DLL\n",
    "# display(dll_train.columns)\n",
    "# dll_train.to_pickle(dll_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# если распределение для DLL уже есть сохраненное, то загружаем\n",
    "# cdfs, pdfs, bins = np.load('data/train_cdfs.pkl.npy'), np.load('data/train_pdfs.pkl.npy'), np.load('data/train_bins.pkl.npy')\n",
    "\n",
    "# либо считаем на основе загруженного dll_train\n",
    "cdfs, pdfs, bins = create_distr(dll_train)\n",
    "# np.save('data/train_cdfs.pkl.npy', cdfs)\n",
    "# np.save('data/train_pdfs.pkl.npy', pdfs)\n",
    "# np.save('data/train_bins.pkl.npy', bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. Declare pipeline\n",
    "\n",
    "Here we declare pipeline helpers for fit, predict, save model, get stats and so on. Extracted as is from `pipeline.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn import model_selection as mdsel\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "import scoring\n",
    "from common import train_cols\n",
    "\n",
    "def split_classes(data):\n",
    "    return [data.loc[data.label == i, :] for i in range(2)]\n",
    "\n",
    "def count_classes(data):\n",
    "    cnt_0 = np.count_nonzero(data.label == 0)\n",
    "    return cnt_0, len(data.index) - cnt_0\n",
    "\n",
    "def sample(data, nrows):\n",
    "    return data.iloc[np.random.permutation(len(data.index))[:nrows], :]\n",
    "\n",
    "def fit(train, n_estimators, transformer_cls):\n",
    "    labels, weights = get_labels_weights(train.loc[:, train_cols])\n",
    "\n",
    "    # defined much later\n",
    "    transformer = transformer_cls().fit(train)\n",
    "    train_values = transformer.transform(train)\n",
    "    \n",
    "    estimator = xgb.XGBClassifier(n_estimators=n_estimators, n_jobs=3)\n",
    "    estimator.fit(train_values, labels, sample_weight=weights, eval_metric=scoring.rejection90_sklearn)\n",
    "    return transformer, estimator\n",
    "    \n",
    "def predict(fitted_state, test):\n",
    "    transformer, estimator = fitted_state\n",
    "\n",
    "    test_value = transformer.transform(test)\n",
    "    predictions = estimator.predict_proba(test_value)[:, 1]\n",
    "    return predictions\n",
    "\n",
    "def score(fitted_state, test):\n",
    "    labels, weights = get_labels_weights(test.loc[:, train_cols])\n",
    "    predictions = predict(fitted_state, test)\n",
    "    return scoring.rejection90(labels, predictions, sample_weight=weights)\n",
    "\n",
    "def fit_predict_save(train, test, filename, n_estimators, transformer_cls):\n",
    "    fitted_state = fit(train, n_estimators, transformer_cls)\n",
    "    predictions = predict(fitted_state, test)\n",
    "    \n",
    "    pd.DataFrame(data={\"prediction\": predictions}, index=test.index).to_csv(\n",
    "        filename, index_label='id'\n",
    "    )\n",
    "    save_model(fitted_state[1], fitted_state[0], filename)\n",
    "    \n",
    "def predict_private_save(test, model_fname, filename, transformer_cls):\n",
    "    fitted_state = (transformer_cls(), xgb.XGBClassifier())\n",
    "    fitted_state[1].load_model(fname=model_fname)\n",
    "    \n",
    "    predictions = predict(fitted_state, test)\n",
    "    \n",
    "    pd.DataFrame(data={\"prediction\": predictions}, index=test.index).to_csv(\n",
    "        filename, index_label='id'\n",
    "    )\n",
    "\n",
    "def fit_save_model(train, filename, n_estimators, transformer_cls):\n",
    "    transformer, model = fit(train, n_estimators, transformer_cls)\n",
    "    save_model(model, transformer, filename)\n",
    "\n",
    "def cv_step(train_subset, test_subset, n_estimators, transformer_cls):\n",
    "    fit_state = fit(train_subset, n_estimators, transformer_cls)\n",
    "        \n",
    "    labels, weights = get_labels_weights(test_subset[train_cols])\n",
    "    predictions = predict(fit_state, test_subset)\n",
    "\n",
    "    y_true = labels\n",
    "    l, r, _ = scoring.get_threshold_details(y_true, predictions, sample_weight=weights)\n",
    "    threshold = (l + r) / 2\n",
    "    y_pred = predictions >= threshold\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred)\n",
    "    rec = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, predictions)\n",
    "    scr = scoring.rejection90(y_true, predictions, sample_weight=weights)\n",
    "    \n",
    "    scores = [acc, prec, rec, f1, roc_auc, scr, threshold]\n",
    "    feature_importance = get_xgb_imp(fit_state[1], fit_state[0].features)\n",
    "    return scores, feature_importance\n",
    "    \n",
    "def cross_validate(train, n_estimators, n_splits, n_rows, transformer_cls):\n",
    "    train = sample(train, n_rows)\n",
    "    results = [\n",
    "        cv_step(train.iloc[train_indices, :], train.iloc[test_indices, :], n_estimators, transformer_cls)\n",
    "        for i, (train_indices, test_indices) in enumerate(mdsel.KFold(n_splits=max(n_splits, 3), shuffle=True).split(train, train.label))\n",
    "        if i < n_splits\n",
    "    ]\n",
    "    scores = [s for s, _ in results]\n",
    "    feature_importance = sum([fi for _, fi in results]) / n_splits\n",
    "        \n",
    "    descr = pd.DataFrame(scores, columns=['acc', 'prec', 'rec', 'f1', 'roc_auc', 'scr', 'th'])\n",
    "    feature_importance = feature_importance.sort_values(by='score', ascending=False)\n",
    "    return descr, feature_importance\n",
    "\n",
    "def get_labels_weights(data):\n",
    "    return data.label.values, data.weight.values\n",
    "\n",
    "def save_model(model, transformer, filename):\n",
    "    model.save_model(to_model_filename(filename))\n",
    "    with open(to_cols_filename(filename), 'w') as txt_file:\n",
    "        str_arr = map(str, [transformer.origin_features, transformer.new_features])\n",
    "        to_write = '\\n\\n'.join(str_arr)\n",
    "        txt_file.write(to_write)\n",
    "\n",
    "def to_model_filename(filename):\n",
    "    return filename.replace('out/', 'models/').replace('.csv', '.xgb')\n",
    "\n",
    "def to_cols_filename(filename):\n",
    "    return filename.replace('out/', 'models/').replace('.csv', '.txt').replace('.xgb', '.txt')\n",
    "\n",
    "def get_xgb_imp(model, feat_names):\n",
    "    imp_vals = model.get_booster().get_fscore()\n",
    "    scores = np.array([float(imp_vals.get('f'+str(i),0.)) for i in range(len(feat_names))])\n",
    "    scores /= scores.sum()\n",
    "    \n",
    "    score = pd.DataFrame(data=scores, index=feat_names, columns=['score'])\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05. Declare glue for transformers\n",
    "\n",
    "Here's the class, where we can set which feature transformers and features at all we gonna use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "def filter_unimportant_features(features):\n",
    "    if global_feature_importance is None:\n",
    "        return features\n",
    "    fscore = global_feature_importance\n",
    "    return [col for col in features if col not in fscore.index or fscore.loc[col, 'score'] > 0.00]\n",
    "    return features\n",
    "\n",
    "class DataTransformer(TransformerMixin):\n",
    "    def __init__(self, *featurizers):\n",
    "        self.featurizers = featurizers\n",
    "    \n",
    "    def fit(self, data, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        data = data.copy()\n",
    "#         features = [] + mom_cols + hit_type_cols + exy_cols + edxy_cols + hit_stats_cols + t_cols + ncl_cols + avg_cs_cols + dxyz_cols + xyz_cols\n",
    "\n",
    "        # final submission was w/ this set of origin columns\n",
    "        features = [\n",
    "            'P', 'PT', 'MatchedHit_TYPE[0]', 'MatchedHit_TYPE[1]', 'MatchedHit_TYPE[2]', \n",
    "            'Lextra_X[0]', 'Lextra_X[1]', 'Lextra_X[2]', 'Lextra_X[3]',\n",
    "            'Lextra_Y[0]', 'Lextra_Y[1]', 'Lextra_Y[2]', 'Lextra_Y[3]',\n",
    "            'Mextra_DX2[0]', 'Mextra_DX2[1]', 'Mextra_DX2[2]', 'Mextra_DX2[3]',\n",
    "            'Mextra_DY2[0]', 'Mextra_DY2[1]', 'Mextra_DY2[3]', 'FOI_hits_N',\n",
    "            'NShared', 'MatchedHit_T[0]', 'MatchedHit_T[2]',\n",
    "            'ncl[0]', 'ncl[1]', 'ncl[2]', 'ncl[3]',\n",
    "            'avg_cs[0]', 'avg_cs[1]', 'avg_cs[2]', 'avg_cs[3]',\n",
    "            'MatchedHit_DX[1]', 'MatchedHit_DX[2]', 'MatchedHit_DX[3]',\n",
    "            'MatchedHit_DY[0]', 'MatchedHit_DY[3]', 'MatchedHit_DZ[2]'\n",
    "        ]\n",
    "        \n",
    "        features = filter_unimportant_features(features)\n",
    "        self.origin_features = features.copy()\n",
    "\n",
    "        add_coses(data, features)\n",
    "        add_mse(data, features)\n",
    "        add_normed_err(data, features)\n",
    "        add_dll(data, features)\n",
    "        add_errs(data, features)\n",
    "        add_velocity(data, features)\n",
    "        \n",
    "        if not features:\n",
    "            raise('no features')\n",
    "    \n",
    "        features = filter_unimportant_features(features)\n",
    "#         self.new_features = features[len(self.origin_features):]\n",
    "        \n",
    "        # final submission was w/ this set of new features\n",
    "        self.new_features = [\n",
    "            'DAngle[1]', 'DAngle[2]', 'DAngle[3]', 'ErrMSE',\n",
    "            'NErr_X[0]', 'NErr_X[1]', 'NErr_X[2]', 'NErr_X[3]',\n",
    "            'NErr_Y[0]', 'NErr_Y[1]', 'NErr_Y[2]', 'NErr_Y[3]',\n",
    "            'DLL', 'Err_X[0]', 'Err_X[1]', 'Err_X[2]', 'Err_X[3]',\n",
    "            'Err_Y[0]', 'Err_Y[1]', 'Err_Y[3]',\n",
    "            'Err_Z[0]', 'Err_Z[1]', 'Err_Z[2]', 'Err_Z[3]',\n",
    "            'V', 'VT', 'M', 'MT'\n",
    "        ]\n",
    "        self.features = self.origin_features + self.new_features\n",
    "        return data[features].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06. Test all is ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>scr</th>\n",
       "      <th>th</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.781000</td>\n",
       "      <td>0.842410</td>\n",
       "      <td>0.889186</td>\n",
       "      <td>0.862179</td>\n",
       "      <td>0.736546</td>\n",
       "      <td>0.462138</td>\n",
       "      <td>0.305616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.006519</td>\n",
       "      <td>0.045320</td>\n",
       "      <td>0.069788</td>\n",
       "      <td>0.007524</td>\n",
       "      <td>0.023514</td>\n",
       "      <td>0.231075</td>\n",
       "      <td>0.070821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.773869</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.852459</td>\n",
       "      <td>0.710680</td>\n",
       "      <td>0.145398</td>\n",
       "      <td>0.215474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.837349</td>\n",
       "      <td>0.854305</td>\n",
       "      <td>0.859873</td>\n",
       "      <td>0.717491</td>\n",
       "      <td>0.434010</td>\n",
       "      <td>0.268212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.838509</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.735312</td>\n",
       "      <td>0.442666</td>\n",
       "      <td>0.315249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.785000</td>\n",
       "      <td>0.865772</td>\n",
       "      <td>0.896774</td>\n",
       "      <td>0.866044</td>\n",
       "      <td>0.752117</td>\n",
       "      <td>0.493119</td>\n",
       "      <td>0.322780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.872521</td>\n",
       "      <td>0.767131</td>\n",
       "      <td>0.795496</td>\n",
       "      <td>0.406364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            acc      prec       rec        f1   roc_auc       scr        th\n",
       "count  5.000000  5.000000  5.000000  5.000000  5.000000  5.000000  5.000000\n",
       "mean   0.781000  0.842410  0.889186  0.862179  0.736546  0.462138  0.305616\n",
       "std    0.006519  0.045320  0.069788  0.007524  0.023514  0.231075  0.070821\n",
       "min    0.775000  0.773869  0.812500  0.852459  0.710680  0.145398  0.215474\n",
       "25%    0.775000  0.837349  0.854305  0.859873  0.717491  0.434010  0.268212\n",
       "50%    0.780000  0.838509  0.882353  0.860000  0.735312  0.442666  0.315249\n",
       "75%    0.785000  0.865772  0.896774  0.866044  0.752117  0.493119  0.322780\n",
       "max    0.790000  0.896552  1.000000  0.872521  0.767131  0.795496  0.406364"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "global_feature_importance = None\n",
    "df_scores, feature_importance = cross_validate(train, n_estimators=10, n_splits=5, n_rows=1000, transformer_cls=DataTransformer)\n",
    "display(df_scores.describe())\n",
    "# display(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07. Train model\n",
    "\n",
    "Here we train model and save:\n",
    "\n",
    "- model to `models/<name>.xgb`\n",
    "- columns we used to `models/<name>.txt`. That's how we can restore which features were used :)\n",
    "\n",
    "On my notebook this model was trained for ~1 hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_t = global_feature_importance\n",
    "# global_feature_importance = None\n",
    "fit_save_model(train, \"models/15_reproduce.xgb\", n_estimators=400, transformer_cls=DataTransformer)\n",
    "global_feature_importance = _t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
