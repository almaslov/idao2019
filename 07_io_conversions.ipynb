{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABUkAAABECAYAAAC8urRAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAA4JJREFUeJzt2zFLVWEcx/H/DYmgSYoCc7Bw0KDFcyVoCaLNNxAS1FBRSw0NTYEUBA0NtRTVEoT0BtoqaAlCj0uQDlIOt6AonAKJ4PYG1Kmnp+7/8xnPs/wOHM7whafT7/cDAAAAACCrHbUHAAAAAADUJJICAAAAAKmJpAAAAABAaiIpAAAAAJCaSAoAAAAApDa03WHbtv2/NQQAAAAAoKSmaTqbPd82kkZEzM33/vyaf8Tc7GjcXn1ce0Yx18bPxcaNW7VnFPFy/GzMzI7E8/nPtacUMTM7EvH9We0Z5ew5FQ++bPpPGggX9/fj8NMXtWcU8/70yVhfOF97RjHD04/iZm8w3+/66KO4tLZSe0Yx98cm4sryWO0ZxdydXIsDDydqzyjm04WV6F5tas8oZvFOG0cPXq49o4i3H+/FaPd47RnF9BZfx6sz+2rPKObEk6/RbHRrzyim3bUY3d0D/G/50cabn4P5fsd2trEw/qH2jGKmVw9F826p9oxi2iNTMbQ+mN9mRMSv4TaaqeXaM4pplyajt3ek9owiRr9t3ZFctwcAAAAAUhNJAQAAAIDURFIAAAAAIDWRFAAAAABITSQFAAAAAFITSQEAAACA1ERSAAAAACA1kRQAAAAASE0kBQAAAABSE0kBAAAAgNREUgAAAAAgNZEUAAAAAEhNJAUAAAAAUhNJAQAAAIDURFIAAAAAIDWRFAAAAABITSQFAAAAAFITSQEAAACA1ERSAAAAACA1kRQAAAAASE0kBQAAAABSE0kBAAAAgNREUgAAAAAgNZEUAAAAAEhNJAUAAAAAUhNJAQAAAIDURFIAAAAAIDWRFAAAAABITSQFAAAAAFITSQEAAACA1ERSAAAAACA1kRQAAAAASE0kBQAAAABSE0kBAAAAgNREUgAAAAAgNZEUAAAAAEhNJAUAAAAAUhNJAQAAAIDURFIAAAAAIDWRFAAAAABITSQFAAAAAFITSQEAAACA1ERSAAAAACA1kRQAAAAASE0kBQAAAABSE0kBAAAAgNREUgAAAAAgNZEUAAAAAEhNJAUAAAAAUhNJAQAAAIDURFIAAAAAIDWRFAAAAABITSQFAAAAAFLr9Pv9LQ/btt36EAAAAADgP9I0TWez59tGUgAAAACAQee6PQAAAACQmkgKAAAAAKQmkgIAAAAAqYmkAAAAAEBqIikAAAAAkNpvuTVZgasyZKkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1728x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as sts\n",
    "import seaborn as sns\n",
    "import scoring\n",
    "from IPython.display import display\n",
    "\n",
    "sns.set()\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "color_palette = sns.color_palette('deep') + sns.color_palette('husl', 6) + sns.color_palette('bright') + sns.color_palette('pastel')\n",
    "\n",
    "%matplotlib inline\n",
    "sns.palplot(color_palette)\n",
    "\n",
    "def ndprint(a, precision=3):\n",
    "    with np.printoptions(precision=precision, suppress=True):\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STATIONS = 4\n",
    "\n",
    "# train cols\n",
    "unused_train_cols = ['particle_type', 'kinWeight', 'sWeight']\n",
    "train_cols = ['label', 'weight']\n",
    "ALL_TRAIN_COLS = train_cols + unused_train_cols\n",
    "\n",
    "# original cols\n",
    "x_cols = ['MatchedHit_X[%i]' % i for i in range(N_STATIONS)]\n",
    "y_cols = ['MatchedHit_Y[%i]' % i for i in range(N_STATIONS)]\n",
    "z_cols = ['MatchedHit_Z[%i]' % i for i in range(N_STATIONS)]\n",
    "xy_cols = x_cols + y_cols\n",
    "xyz_cols = x_cols + y_cols + z_cols\n",
    "dx_cols = ['MatchedHit_DX[%i]' % i for i in range(N_STATIONS)]\n",
    "dy_cols = ['MatchedHit_DY[%i]' % i for i in range(N_STATIONS)]\n",
    "dz_cols = ['MatchedHit_DZ[%i]' % i for i in range(N_STATIONS)]\n",
    "dxyz_cols = dx_cols + dy_cols + dz_cols\n",
    "\n",
    "ex_cols = ['Lextra_X[%i]' % i for i in range(N_STATIONS)]\n",
    "ey_cols = ['Lextra_Y[%i]' % i for i in range(N_STATIONS)]\n",
    "exy_cols = ex_cols + ey_cols\n",
    "edx_cols = ['Mextra_DX2[%i]' % i for i in range(N_STATIONS)]\n",
    "edy_cols = ['Mextra_DY2[%i]' % i for i in range(N_STATIONS)]\n",
    "edxy_cols = edx_cols + edy_cols\n",
    "\n",
    "t_cols = ['MatchedHit_T[%i]' % i for i in range(N_STATIONS)]\n",
    "dt_cols = ['MatchedHit_DT[%i]' % i for i in range(N_STATIONS)]\n",
    "\n",
    "hit_type_cols = ['MatchedHit_TYPE[%i]' % i for i in range(N_STATIONS)]\n",
    "mom_cols = ['P', 'PT']\n",
    "hit_stats_cols = ['FOI_hits_N', 'NShared', 'ndof']\n",
    "\n",
    "ncl_cols = ['ncl[%i]' % i for i in range(N_STATIONS)]\n",
    "avg_cs_cols = ['avg_cs[%i]' % i for i in range(N_STATIONS)]\n",
    "\n",
    "# foi cols\n",
    "foi_xyz_cols = [\"FOI_hits_X\", \"FOI_hits_Y\", \"FOI_hits_Z\"]\n",
    "foi_dxyz_cols = [\"FOI_hits_DX\", \"FOI_hits_DY\", \"FOI_hits_DZ\"]\n",
    "foi_ts_cols = [\"FOI_hits_T\", \"FOI_hits_DT\", \"FOI_hits_S\"]\n",
    "foi_cols = foi_xyz_cols + foi_dxyz_cols + foi_ts_cols\n",
    "\n",
    "# derivative cols\n",
    "pca_x_cols = ['PCA_X[%i]' % i for i in range(N_STATIONS)]\n",
    "pca_y_cols = ['PCA_Y[%i]' % i for i in range(N_STATIONS)]\n",
    "pca_z_cols = ['PCA_Z[%i]' % i for i in range(N_STATIONS)]\n",
    "pca_xyz_cols = pca_x_cols + pca_y_cols + pca_z_cols\n",
    "\n",
    "nerr_x_cols = ['NErr_X[%i]' % i for i in range(N_STATIONS)]\n",
    "nerr_y_cols = ['NErr_Y[%i]' % i for i in range(N_STATIONS)]\n",
    "nerr_xy_cols = nerr_x_cols + nerr_y_cols\n",
    "\n",
    "da_cols = ['DAngle[%d]' % i for i in range(1, 4)]\n",
    "is_muon_cols = ['IsMuonTight']\n",
    "prob_hit_detector_cols = ['ProbHit[%i]' % i for i in range(N_STATIONS)]\n",
    "err_cols = ['ErrMSE', 'Chi2Quantile']\n",
    "\n",
    "SIMPLE_FEATURE_COLS = xyz_cols + dxyz_cols + exy_cols + edxy_cols + t_cols + dt_cols + hit_type_cols + mom_cols + hit_stats_cols + ncl_cols + avg_cs_cols\n",
    "ARR_FEATURE_COLS = foi_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(SIMPLE_FEATURE_COLS + ARR_FEATURE_COLS + ALL_TRAIN_COLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert csv to pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(79, 0), (921, 0)]\n",
      "[(0, 1), (0, 1)]\n",
      "Wall time: 323 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from itertools import repeat\n",
    "\n",
    "def parse_float_array(line):\n",
    "    return np.fromstring(line[1:-1], sep=\" \", dtype=float_dtype)\n",
    "\n",
    "def parse_int_array(line):\n",
    "    return np.fromstring(line[1:-1], sep=\" \", dtype=int_dtype)\n",
    "\n",
    "def get_class_data(data, i):\n",
    "    return data.loc[data.label == i, :]\n",
    "\n",
    "filenames = ['data/train_part_1_v2.csv.gz', 'data/train_part_2_v2.csv.gz']\n",
    "filename_pattern = 'data/train_pub'\n",
    "usecols = SIMPLE_FEATURE_COLS + ARR_FEATURE_COLS + ALL_TRAIN_COLS\n",
    "groups = {\n",
    "    'sf': SIMPLE_FEATURE_COLS,\n",
    "    'af': ARR_FEATURE_COLS,\n",
    "    'tr': ALL_TRAIN_COLS\n",
    "}\n",
    "label_prefixes = ['L0', 'L1']\n",
    "write_chunksize = 1000\n",
    "\n",
    "na_values = ['-9999.0', '255']\n",
    "int_dtype = np.int32\n",
    "float_dtype = np.float32\n",
    "\n",
    "converters = dict(zip(ARR_FEATURE_COLS, repeat(parse_float_array)))\n",
    "converters[foi_ts_cols[-1]] = parse_int_array\n",
    "\n",
    "types = dict(zip(SIMPLE_FEATURE_COLS + ALL_TRAIN_COLS, repeat(float_dtype)))\n",
    "for col in unused_train_cols[:1] + train_cols[:1] + hit_stats_cols + ncl_cols + hit_type_cols:\n",
    "    types[col] = int_dtype\n",
    "    \n",
    "def generate_chunk_filename(label_prefix, group, chunk_ind):\n",
    "    return '{filepath}_{label}_{group}_{ind:03d}.pkl'.format(filepath=filename_pattern, label=label_prefix, group=group, ind=chunk_ind)\n",
    "    \n",
    "class ClsDataBuffer:\n",
    "    def __init__(self):\n",
    "        self.frames = []\n",
    "        self.chunk_counter = 0\n",
    "    \n",
    "    @property\n",
    "    def cur_len(self):\n",
    "        return sum([len(data.index) for data in self.frames])\n",
    "        \n",
    "    def set_frames(self, data):\n",
    "        self.frames = [data]\n",
    "        self.chunk_counter += 1\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return str((self.cur_len, self.chunk_counter))\n",
    "\n",
    "def write_chunk(chunk, label_prefix, chunk_ind):\n",
    "    for group_key, group_cols in groups.items():\n",
    "        filename = generate_chunk_filename(label_prefix, group_key, chunk_ind)\n",
    "        chunk.loc[:, group_cols].to_pickle(filename)\n",
    "    \n",
    "def write_chunks(i, chunksize, force=False):\n",
    "    c = classes[i]\n",
    "    while c.cur_len > chunksize or (force and c.cur_len > 0):\n",
    "        left_data = pd.concat(c.frames, axis=0, ignore_index=True) if len(c.frames) > 1 else c.frames[0]\n",
    "        chunk = left_data.iloc[:chunksize, :]\n",
    "        left_data = left_data.iloc[write_chunksize:, :]\n",
    "        \n",
    "        write_chunk(chunk, label_prefixes[i], classes[i].chunk_counter)\n",
    "        c.set_frames(left_data)\n",
    "        \n",
    "classes = [ClsDataBuffer() for i in range(2)]\n",
    "data_iterator = [\n",
    "    data\n",
    "    for filename in filenames[:1]\n",
    "    for data in pd.read_csv(filename, nrows=1000, chunksize=1000, na_values=na_values, na_filter=False, converters=converters, dtype=types, usecols=usecols)\n",
    "]\n",
    "\n",
    "for data in data_iterator:\n",
    "    for i in range(2):\n",
    "        classes[i].frames.append(get_class_data(data, i))\n",
    "        write_chunks(i, write_chunksize)\n",
    "    print(classes)\n",
    "            \n",
    "for i in range(2):\n",
    "    write_chunks(i, write_chunksize, force=True)\n",
    "\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 50)\n",
      "Wall time: 13.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_0 = pd.read_csv('data/train_part_1_v2.csv.gz', nrows=200000, na_values=['-9999.0', '255'], usecols=TRAINSET_COLS)\n",
    "train_1 = pd.read_csv('data/train_part_2_v2.csv.gz', nrows=200000, na_values=['-9999.0', '255'], usecols=TRAINSET_COLS)\n",
    "train = pd.concat([train_0, train_1], axis=0, ignore_index=True)\n",
    "\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30588 369412\n"
     ]
    }
   ],
   "source": [
    "def get_class(i):\n",
    "    return train.index[train.label == i]\n",
    "\n",
    "# Gets `n_rows` random samples from `data`\n",
    "def get_samples(data, n_rows):\n",
    "    indices = np.random.randint(len(data), size=n_rows)\n",
    "    indices = np.sort(indices)\n",
    "    return data.iloc[indices]\n",
    "\n",
    "# Gets `n_rows` random samples from `data` with specified class proportions.\n",
    "# If `prop_0` is None, it keeps `data`s natural proportions.\n",
    "def get_samples_w_proptions(data, n_rows, prop_0=None):\n",
    "    def get_class_samples(class_i, n_rows):\n",
    "        class_indices = np.random.randint(len(class_i), size=n_rows)\n",
    "        return class_i[class_indices]\n",
    "    \n",
    "    if prop_0 is None:\n",
    "        prop_0 = len(class_0) / (len(class_0) + len(class_1))\n",
    "    cnt_0 = int(n_rows * prop_0)\n",
    "    class_0_indices = get_class_samples(class_0, cnt_0)\n",
    "    class_1_indices = get_class_samples(class_1, n_rows - cnt_0)\n",
    "    indices = np.concatenate((class_0_indices, class_1_indices))\n",
    "    indices = np.sort(indices)\n",
    "    return data.loc[indices, :]\n",
    "\n",
    "# Gets `n_rows` head samples from `data` with specified class proportions.\n",
    "# If `prop_0` is None, it keeps `data`s natural proportions.\n",
    "# If `data` samples is not enough to fulfil proportions for any `class_i` then random sampling from the `class_i` is applied.\n",
    "def get_head_w_proportions(data, n_rows, prop_0=None):\n",
    "    def get_class_samples_head_smart(class_i, n_rows):\n",
    "        cls_len = len(class_i)\n",
    "        if cls_len > n_rows:\n",
    "            return class_i[:n_rows]\n",
    "        class_indices = np.concatenate((np.arange(cls_len),  np.random.randint(cls_len, size=n_rows-cls_len)))\n",
    "        return class_i[class_indices]\n",
    "    \n",
    "    if prop_0 is None:\n",
    "        prop_0 = len(class_0) / (len(class_0) + len(class_1))\n",
    "    cnt_0 = int(n_rows * prop_0)\n",
    "    class_0_indices = get_class_samples_head_smart(class_0, cnt_0)\n",
    "    class_1_indices = get_class_samples_head_smart(class_1, n_rows - cnt_0)\n",
    "    indices = np.concatenate((class_0_indices, class_1_indices))\n",
    "    indices = np.sort(indices)\n",
    "    return data.loc[indices, :]\n",
    "    \n",
    "print(np.count_nonzero(train.label == 0), np.count_nonzero(train.label == 1))\n",
    "\n",
    "class_0 = get_class(0)\n",
    "class_1 = get_class(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn import model_selection as mdsel\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "def fit(train):\n",
    "    target_train = train[train_cols]\n",
    "    \n",
    "    # defined much later\n",
    "    transformer = DataTransformer().fit(train)\n",
    "    train = transformer.transform(train)\n",
    "    \n",
    "    estimator = xgb.XGBClassifier(n_estimators=60, n_jobs=3)\n",
    "    estimator.fit(train.values, target_train.label.values, sample_weight=target_train.weight.values, eval_metric=scoring.rejection90_sklearn)\n",
    "    return transformer, estimator\n",
    "    \n",
    "def predict(fitted_state, test):\n",
    "    transformer, estimator = fitted_state\n",
    "    \n",
    "    test = transformer.transform(test)\n",
    "    predictions = estimator.predict_proba(test.values)[:, 1]\n",
    "    return predictions\n",
    "\n",
    "def score(fitted_state, test):\n",
    "    target_test = test.loc[:, train_cols]\n",
    "    predictions = predict(fitted_state, test)\n",
    "    return scoring.rejection90(target_test.label.values, predictions, sample_weight=target_test.weight.values)\n",
    "\n",
    "def fit_predict_save(train, test, filename):\n",
    "    fitted_state = fit(train)\n",
    "    predictions = predict(fitted_state, test)\n",
    "    \n",
    "    pd.DataFrame(data={\"prediction\": predictions}, index=test.index).to_csv(\n",
    "        filename, index_label=utils.ID_COLUMN\n",
    "    )\n",
    "    \n",
    "    model = fitted_state[1]\n",
    "    model_filename = filename.replace('out/', 'models/').replace('.csv', '.xgb')\n",
    "    model.save_model(model_filename)\n",
    "    \n",
    "def fit_save_model(train, filename):\n",
    "    if filename.endswith('.csv'):\n",
    "        filename = filename.replace('out/', 'models/').replace('.csv', '.xgb')\n",
    "        \n",
    "    _, model = fit(train)\n",
    "    model.save_model(filename)\n",
    "    \n",
    "def cross_validate(train, n_splits, n_rows):\n",
    "    train = get_head_w_proportions(train, n_rows, .5)\n",
    "    \n",
    "    splitter = mdsel.StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "    scores = []\n",
    "    for train_indices, test_indices in splitter.split(train, train.label):\n",
    "        train_subset = train.iloc[train_indices, :]\n",
    "        test_subset = train.iloc[test_indices, :]\n",
    "        \n",
    "        fit_state = fit(train_subset)\n",
    "        \n",
    "        target_test = test_subset[train_cols]\n",
    "        predictions = predict(fit_state, test_subset)\n",
    "        \n",
    "        y_true = target_test.label.values\n",
    "        l, r, ep = scoring.get_threshold_details(y_true, predictions, sample_weight=target_test.weight.values)\n",
    "        threshold = (l + r) / 2\n",
    "        y_pred = predictions >= threshold\n",
    "                \n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        prec = precision_score(y_true, y_pred)\n",
    "        rec = recall_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        roc_auc = roc_auc_score(y_true, predictions)\n",
    "        scr = scoring.rejection90(y_true, predictions, sample_weight=target_test.weight.values)\n",
    "        scores += [[acc, prec, rec, f1, roc_auc, scr, threshold, r - l]]\n",
    "\n",
    "    return pd.DataFrame(scores, columns=['acc', 'prec', 'rec', 'f1', 'roc_auc', 'scr', 'th', 'dTh'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transformer\n",
    "\n",
    "Это по сути основная часть. Класс, который отбирает нужные столбцы, возможно что-то модифицирует или добавляет. На выходе - входные данные для модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def fill_na(data):\n",
    "    mask = data.isna()\n",
    "    means = data.mean(skipna=True)\n",
    "    data.fillna(means, inplace=True)\n",
    "    return mask\n",
    "\n",
    "def restore_na(data, mask):\n",
    "    data.mask(mask, other=np.NaN, inplace=True)\n",
    "    \n",
    "def get_nth_detector_coords(i):\n",
    "    return [x_cols[i], y_cols[i], z_cols[i]]\n",
    "\n",
    "def get_nth_detector_coords_pca(i):\n",
    "    return [pca_x_cols[i], pca_y_cols[i], pca_z_cols[i]]\n",
    "    \n",
    "def pca_fit(data):\n",
    "    cols = get_nth_detector_coords(0)\n",
    "    data = data[cols].copy()\n",
    "    \n",
    "    fill_na(data)\n",
    "    pca_model = PCA(n_components=3)\n",
    "    pca_model.fit(data)\n",
    "    return pca_model\n",
    "\n",
    "def pca_transform(pca_model, data, features):\n",
    "    for i in range(4):\n",
    "        cols = get_nth_detector_coords(i)\n",
    "        new_cols = get_nth_detector_coords_pca(i)\n",
    "        data_detector = data.loc[:, cols]\n",
    "        \n",
    "        mask = fill_na(data_detector)\n",
    "        transformed_data = pca_model.transform(data_detector.values)\n",
    "        restore_na(data_detector, mask)\n",
    "        return\n",
    "        \n",
    "        for j in range(3):\n",
    "            data[new_cols[j]] = transformed_data[:, j]\n",
    "\n",
    "    features += pca_coord_cols\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cosine dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_coses(data, features):\n",
    "    def get_layer_coords(data, i):\n",
    "        return data[[x_cols[i], y_cols[i], z_cols[i]]].values\n",
    "\n",
    "    def dot(x, y):\n",
    "        return np.sum(x * y, axis=1)\n",
    "    \n",
    "    def norm(x):\n",
    "        return np.sqrt(dot(x, x))\n",
    "\n",
    "    def get_cosine_dist(L1, L2, L1_norm, L2_norm):\n",
    "        return dot(L1, L2) / L1_norm / L2_norm\n",
    "    \n",
    "    def get_angle(cosines):\n",
    "        return np.arccos(cosines, dtype=np.float32) / np.pi * 180\n",
    "    \n",
    "    layers = np.array([get_layer_coords(data, i) for i in range(4)])\n",
    "    layers[1:] -= layers[:3]\n",
    "    norms = list(map(norm, layers))\n",
    "    \n",
    "    for i in range(3):\n",
    "        cosines = get_cosine_dist(layers[i], layers[i+1], norms[i], norms[i+1])\n",
    "        angles = get_angle(cosines)\n",
    "        data[da_cols[i]] = angles\n",
    "        \n",
    "    features += da_cols        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### IsMuon && IsMuonTight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_is_muon_tight(data, features):\n",
    "    return add_is_muon(data, features, threshold=2)\n",
    "    \n",
    "def add_is_muon(data, features, threshold=1):\n",
    "    def lt(p):\n",
    "        return data.P < p\n",
    "    def gt(p):\n",
    "        return data.P >= p\n",
    "    def M(i):\n",
    "        return data[hit_type_cols[i]] >= threshold\n",
    "    \n",
    "    lt_6k_mask = lt(6000.) & M(0) & M(1)\n",
    "    lt_10k_gt_6k_mask = gt(6000.) & lt(10000.) & M(0) & M(1) & (M(2) | M(3))\n",
    "    gt_10k_mask = gt(10000.) & M(0) & M(1) & M(2) & M(3)\n",
    "    \n",
    "    data.loc[:, is_muon_cols[0]] = 1 * (lt_6k_mask | lt_10k_gt_6k_mask | gt_10k_mask)\n",
    "    features += is_muon_cols        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Probability hit detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_probability_hit_detector(data, features):\n",
    "    p = data[mom_cols[0]].values\n",
    "    \n",
    "    def prob(i):\n",
    "        alpha = (0.0260, 0.0021, 0.0015, 0.0008)\n",
    "        beta = (2040., 2387., 3320., 3903.)\n",
    "        t = (alpha[i] * (p - beta[i]))**(i+1)\n",
    "        return t / (1 + t)\n",
    "        \n",
    "    for i in range(4):\n",
    "        data.loc[:, prob_hit_detector_cols[i]] = prob(i)\n",
    "        \n",
    "    features += prob_hit_detector_cols\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_mse(data, features):\n",
    "    dxy = (data.loc[:, xy_cols].values - data.loc[:, exy_cols].values) / data.loc[:, dx_cols + dy_cols].values / 2.\n",
    "    D = np.mean(dxy**2, axis=1)\n",
    "    \n",
    "    data.loc[:, err_cols[0]] = D\n",
    "    features += [err_cols[0]]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_normed_err(data, features):\n",
    "    dxy = data.loc[:, xy_cols].values - data.loc[:, exy_cols].values\n",
    "    normed_errors = dxy / np.sqrt(data.loc[:, edxy_cols].values)\n",
    "    \n",
    "    for i in range(4):\n",
    "        data.loc[:, nerr_x_cols[i]] = normed_errors[:, i]\n",
    "        data.loc[:, nerr_y_cols[i]] = normed_errors[:, i + 4]\n",
    "    \n",
    "    features += nerr_xy_cols\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>scr</th>\n",
       "      <th>th</th>\n",
       "      <th>dTh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.756900</td>\n",
       "      <td>0.706487</td>\n",
       "      <td>0.879001</td>\n",
       "      <td>0.783350</td>\n",
       "      <td>0.797067</td>\n",
       "      <td>0.690523</td>\n",
       "      <td>0.066475</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000614</td>\n",
       "      <td>0.001772</td>\n",
       "      <td>0.004764</td>\n",
       "      <td>0.000989</td>\n",
       "      <td>0.005021</td>\n",
       "      <td>0.028296</td>\n",
       "      <td>0.001094</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.756376</td>\n",
       "      <td>0.704573</td>\n",
       "      <td>0.873725</td>\n",
       "      <td>0.782223</td>\n",
       "      <td>0.791623</td>\n",
       "      <td>0.665091</td>\n",
       "      <td>0.065471</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.756562</td>\n",
       "      <td>0.705695</td>\n",
       "      <td>0.877007</td>\n",
       "      <td>0.782989</td>\n",
       "      <td>0.794843</td>\n",
       "      <td>0.675283</td>\n",
       "      <td>0.065892</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.756749</td>\n",
       "      <td>0.706818</td>\n",
       "      <td>0.880288</td>\n",
       "      <td>0.783755</td>\n",
       "      <td>0.798062</td>\n",
       "      <td>0.685474</td>\n",
       "      <td>0.066313</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.757162</td>\n",
       "      <td>0.707444</td>\n",
       "      <td>0.881638</td>\n",
       "      <td>0.783914</td>\n",
       "      <td>0.799789</td>\n",
       "      <td>0.703239</td>\n",
       "      <td>0.066977</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.708070</td>\n",
       "      <td>0.882988</td>\n",
       "      <td>0.784073</td>\n",
       "      <td>0.801516</td>\n",
       "      <td>0.721003</td>\n",
       "      <td>0.067641</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            acc      prec       rec        f1   roc_auc       scr        th  \\\n",
       "count  3.000000  3.000000  3.000000  3.000000  3.000000  3.000000  3.000000   \n",
       "mean   0.756900  0.706487  0.879001  0.783350  0.797067  0.690523  0.066475   \n",
       "std    0.000614  0.001772  0.004764  0.000989  0.005021  0.028296  0.001094   \n",
       "min    0.756376  0.704573  0.873725  0.782223  0.791623  0.665091  0.065471   \n",
       "25%    0.756562  0.705695  0.877007  0.782989  0.794843  0.675283  0.065892   \n",
       "50%    0.756749  0.706818  0.880288  0.783755  0.798062  0.685474  0.066313   \n",
       "75%    0.757162  0.707444  0.881638  0.783914  0.799789  0.703239  0.066977   \n",
       "max    0.757576  0.708070  0.882988  0.784073  0.801516  0.721003  0.067641   \n",
       "\n",
       "       dTh  \n",
       "count  3.0  \n",
       "mean   0.0  \n",
       "std    0.0  \n",
       "min    0.0  \n",
       "25%    0.0  \n",
       "50%    0.0  \n",
       "75%    0.0  \n",
       "max    0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class DataTransformer(TransformerMixin):\n",
    "    def __init__(self, *featurizers):\n",
    "        self.featurizers = featurizers\n",
    "    \n",
    "    def fit(self, data, y=None):\n",
    "#         self.pca_model = pca_fit(data)\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        data = data.copy()\n",
    "        features = [] + xyz_cols + mom_cols + hit_type_cols + dxyz_cols + exy_cols + edxy_cols\n",
    "\n",
    "#         pca_transform(self.pca_model, data, features)\n",
    "#         add_is_muon(data, features)\n",
    "#         add_is_muon_tight(data, features)\n",
    "        add_probability_hit_detector(data, features)\n",
    "        add_coses(data, features)\n",
    "        add_mse(data, features)\n",
    "        add_normed_err(data, features)\n",
    "                \n",
    "        if features:\n",
    "            data = data.loc[:, features]\n",
    "        else:\n",
    "            data = data.drop(train_cols, axis=1)\n",
    "        return data\n",
    "\n",
    "df_scores = cross_validate(train, n_splits=3, n_rows=20000)\n",
    "display(df_scores.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>scr</th>\n",
       "      <th>th</th>\n",
       "      <th>dTh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.765430</td>\n",
       "      <td>0.716089</td>\n",
       "      <td>0.87966</td>\n",
       "      <td>0.789482</td>\n",
       "      <td>0.805171</td>\n",
       "      <td>0.746396</td>\n",
       "      <td>0.065736</td>\n",
       "      <td>0.000028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.003003</td>\n",
       "      <td>0.003847</td>\n",
       "      <td>0.00338</td>\n",
       "      <td>0.002117</td>\n",
       "      <td>0.003652</td>\n",
       "      <td>0.024239</td>\n",
       "      <td>0.001910</td>\n",
       "      <td>0.000063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.760100</td>\n",
       "      <td>0.709792</td>\n",
       "      <td>0.87680</td>\n",
       "      <td>0.785784</td>\n",
       "      <td>0.799652</td>\n",
       "      <td>0.722304</td>\n",
       "      <td>0.063544</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.766200</td>\n",
       "      <td>0.714990</td>\n",
       "      <td>0.87750</td>\n",
       "      <td>0.789803</td>\n",
       "      <td>0.804554</td>\n",
       "      <td>0.729295</td>\n",
       "      <td>0.064457</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.766650</td>\n",
       "      <td>0.718362</td>\n",
       "      <td>0.87870</td>\n",
       "      <td>0.790256</td>\n",
       "      <td>0.804688</td>\n",
       "      <td>0.743997</td>\n",
       "      <td>0.065446</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.767100</td>\n",
       "      <td>0.718512</td>\n",
       "      <td>0.88000</td>\n",
       "      <td>0.790482</td>\n",
       "      <td>0.808034</td>\n",
       "      <td>0.752049</td>\n",
       "      <td>0.066913</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.767100</td>\n",
       "      <td>0.718791</td>\n",
       "      <td>0.88530</td>\n",
       "      <td>0.791082</td>\n",
       "      <td>0.808925</td>\n",
       "      <td>0.784336</td>\n",
       "      <td>0.068320</td>\n",
       "      <td>0.000142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            acc      prec      rec        f1   roc_auc       scr        th  \\\n",
       "count  5.000000  5.000000  5.00000  5.000000  5.000000  5.000000  5.000000   \n",
       "mean   0.765430  0.716089  0.87966  0.789482  0.805171  0.746396  0.065736   \n",
       "std    0.003003  0.003847  0.00338  0.002117  0.003652  0.024239  0.001910   \n",
       "min    0.760100  0.709792  0.87680  0.785784  0.799652  0.722304  0.063544   \n",
       "25%    0.766200  0.714990  0.87750  0.789803  0.804554  0.729295  0.064457   \n",
       "50%    0.766650  0.718362  0.87870  0.790256  0.804688  0.743997  0.065446   \n",
       "75%    0.767100  0.718512  0.88000  0.790482  0.808034  0.752049  0.066913   \n",
       "max    0.767100  0.718791  0.88530  0.791082  0.808925  0.784336  0.068320   \n",
       "\n",
       "            dTh  \n",
       "count  5.000000  \n",
       "mean   0.000028  \n",
       "std    0.000063  \n",
       "min    0.000000  \n",
       "25%    0.000000  \n",
       "50%    0.000000  \n",
       "75%    0.000000  \n",
       "max    0.000142  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 53.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_scores = cross_validate(train, n_splits=5, n_rows=100000)\n",
    "display(df_scores.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/test_public_v2.csv.gz', na_values=['-9999.0', '255'], usecols=TESTSET_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fit_predict_save(get_head_w_proportions(train, 1000000, None), test, \"out/06_x_dx_ex_edx_mom_hit_phit_da_mse_nerr_orig_1000k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fit_predict_save(get_head_w_proportions(train, 1300000, .3), test, \"out/06_x_dx_ex_edx_mom_hit_phit_da_mse_nerr_30_1300k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fit_predict_save(get_head_w_proportions(train, 1700000, .5), test, \"out/06_x_dx_ex_edx_mom_hit_phit_da_mse_nerr_50_1700k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fit_predict_save(get_samples_w_proptions(train, 100000, .5), test, \"out/04_prop_80_20_100_800.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fit_predict_save(get_samples(train, 100000), test, \"out/03_baseline_head_100.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fit_save_model(get_head_w_proportions(train, 100000, .5), \"models/06_x_dx_ex_edx_mom_hit_da_mse_50_100k.xgb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame([[1.0, np.NaN], [np.NaN, np.NaN], [2.0, 3.1]], columns=['a', 'b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_backup = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\tf\\lib\\site-packages\\numpy\\lib\\arraysetops.py:522: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = pd.read_csv('data/train_part_1_v2.csv.gz', na_values=['-9999.0', '255'], index_col=utils.ID_COLUMN)\n",
    "# train = pd.read_csv('data/train_part_2_v2.csv.gz', nrows=10000, na_values=['-9999.0', '255'])\n",
    "# train = pd.concat([train_0, train_1], axis=0, ignore_index=True)\n",
    "\n",
    "label0 = train.loc[train.label==0, :]\n",
    "label0.to_csv('data/train_pub_L0_p1.csv.gz', compression='gzip')\n",
    "\n",
    "label1 = train.loc[train.label==1, :]\n",
    "label1.to_csv('data/train_pub_L1_p1.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "label0.to_pickle('data/train_pub_L0_p1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 48.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "label1.to_pickle('data/train_pub_L1_p1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((210403, 79), (2512449, 79))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label0.shape, label1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train = pd.read_csv('data/train_part_2_v2.csv.gz', na_values=['-9999.0', '255'], index_col=utils.ID_COLUMN)\n",
    "# train = pd.read_csv('data/train_part_2_v2.csv.gz', nrows=10000, na_values=['-9999.0', '255'])\n",
    "# train = pd.concat([train_0, train_1], axis=0, ignore_index=True)\n",
    "\n",
    "label0 = train.loc[train.label==0, :]\n",
    "label0.to_csv('data/train_pub_L0_p2.csv.gz', compression='gzip')\n",
    "\n",
    "label1 = train.loc[train.label==1, :]\n",
    "label1.to_csv('data/train_pub_L1_p2.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(736, 79) (736, 79)\n",
      "Wall time: 702 ms\n"
     ]
    }
   ],
   "source": [
    "# label0_ = pd.read_csv('data/train_pub_L0_p1.csv', nrows=10000, na_values=['-9999.0', '255'], index_col=utils.ID_COLUMN)\n",
    "\n",
    "# print(label0.shape, label0_.shape)\n",
    "\n",
    "# for c in label0.columns:\n",
    "#     lc0 = label0.loc[:, c]\n",
    "#     if not np.issubdtype(lc0.dtype, np.number):\n",
    "#         continue\n",
    "#     lc0_ = label0_.loc[:, c]\n",
    "#     mask = (lc0 - lc0_).abs() > 1e-10\n",
    "#     if mask.sum() > 0 :\n",
    "#         display(lc0[mask] - lc0_[mask])\n",
    "        \n",
    "\n",
    "# for c in label0.columns:\n",
    "#     lc0 = label0.loc[:, c]\n",
    "#     if np.issubdtype(lc0.dtype, np.number):\n",
    "#         continue\n",
    "#     lc0_ = label0_.loc[:, c]\n",
    "#     mask = (lc0 != lc0_) & (lc0 == lc0) & (lc0_ == lc0_)\n",
    "#     for i in range(mask.sum()):\n",
    "#         s0 = ''.join(lc0.iloc[mask].iloc[i].split())\n",
    "#         s1 = ''.join(lc0_.iloc[mask].iloc[i].split())\n",
    "#         if s0 == s1:\n",
    "#             continue\n",
    "        \n",
    "#         print(lc0.iloc[0])\n",
    "#         print(lc0_.iloc[0])\n",
    "#         for i,s in enumerate(difflib.ndiff(lc0.iloc[0], lc0_.iloc[0])):\n",
    "#             if s[0]==' ': continue\n",
    "#             elif s[0]=='-':\n",
    "#                 print(u'Delete \"{}\" `{}` from position {}'.format(s[-1], ord(s[-1]),i))\n",
    "#             elif s[0]=='+':\n",
    "#                 print(u'Add \"{}\" `{}` to position {}'.format(s[-1],ord(s[-1]),i))    \n",
    "#         print()  \n",
    "\n",
    "# display(label0[mask].head(10))\n",
    "# display(label0_[mask].head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
