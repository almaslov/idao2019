{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABUkAAABECAYAAAC8urRAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAA4JJREFUeJzt2zFLVWEcx/H/DYmgSYoCc7Bw0KDFcyVoCaLNNxAS1FBRSw0NTYEUBA0NtRTVEoT0BtoqaAlCj0uQDlIOt6AonAKJ4PYG1Kmnp+7/8xnPs/wOHM7whafT7/cDAAAAACCrHbUHAAAAAADUJJICAAAAAKmJpAAAAABAaiIpAAAAAJCaSAoAAAAApDa03WHbtv2/NQQAAAAAoKSmaTqbPd82kkZEzM33/vyaf8Tc7GjcXn1ce0Yx18bPxcaNW7VnFPFy/GzMzI7E8/nPtacUMTM7EvH9We0Z5ew5FQ++bPpPGggX9/fj8NMXtWcU8/70yVhfOF97RjHD04/iZm8w3+/66KO4tLZSe0Yx98cm4sryWO0ZxdydXIsDDydqzyjm04WV6F5tas8oZvFOG0cPXq49o4i3H+/FaPd47RnF9BZfx6sz+2rPKObEk6/RbHRrzyim3bUY3d0D/G/50cabn4P5fsd2trEw/qH2jGKmVw9F826p9oxi2iNTMbQ+mN9mRMSv4TaaqeXaM4pplyajt3ek9owiRr9t3ZFctwcAAAAAUhNJAQAAAIDURFIAAAAAIDWRFAAAAABITSQFAAAAAFITSQEAAACA1ERSAAAAACA1kRQAAAAASE0kBQAAAABSE0kBAAAAgNREUgAAAAAgNZEUAAAAAEhNJAUAAAAAUhNJAQAAAIDURFIAAAAAIDWRFAAAAABITSQFAAAAAFITSQEAAACA1ERSAAAAACA1kRQAAAAASE0kBQAAAABSE0kBAAAAgNREUgAAAAAgNZEUAAAAAEhNJAUAAAAAUhNJAQAAAIDURFIAAAAAIDWRFAAAAABITSQFAAAAAFITSQEAAACA1ERSAAAAACA1kRQAAAAASE0kBQAAAABSE0kBAAAAgNREUgAAAAAgNZEUAAAAAEhNJAUAAAAAUhNJAQAAAIDURFIAAAAAIDWRFAAAAABITSQFAAAAAFITSQEAAACA1ERSAAAAACA1kRQAAAAASE0kBQAAAABSE0kBAAAAgNREUgAAAAAgNZEUAAAAAEhNJAUAAAAAUhNJAQAAAIDURFIAAAAAIDWRFAAAAABITSQFAAAAAFLr9Pv9LQ/btt36EAAAAADgP9I0TWez59tGUgAAAACAQee6PQAAAACQmkgKAAAAAKQmkgIAAAAAqYmkAAAAAEBqIikAAAAAkNpvuTVZgasyZKkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1728x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as sts\n",
    "import seaborn as sns\n",
    "import scoring\n",
    "from IPython.display import display\n",
    "\n",
    "sns.set()\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "color_palette = sns.color_palette('deep') + sns.color_palette('husl', 6) + sns.color_palette('bright') + sns.color_palette('pastel')\n",
    "\n",
    "%matplotlib inline\n",
    "sns.palplot(color_palette)\n",
    "\n",
    "def ndprint(a, precision=3):\n",
    "    with np.printoptions(precision=precision, suppress=True):\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STATIONS = 4\n",
    "\n",
    "# train cols\n",
    "unused_train_cols = ['particle_type', 'kinWeight', 'sWeight']\n",
    "train_cols = ['label', 'weight']\n",
    "ALL_TRAIN_COLS = train_cols + unused_train_cols\n",
    "\n",
    "# original cols\n",
    "x_cols = ['MatchedHit_X[%i]' % i for i in range(N_STATIONS)]\n",
    "y_cols = ['MatchedHit_Y[%i]' % i for i in range(N_STATIONS)]\n",
    "z_cols = ['MatchedHit_Z[%i]' % i for i in range(N_STATIONS)]\n",
    "xy_cols = x_cols + y_cols\n",
    "xyz_cols = x_cols + y_cols + z_cols\n",
    "dx_cols = ['MatchedHit_DX[%i]' % i for i in range(N_STATIONS)]\n",
    "dy_cols = ['MatchedHit_DY[%i]' % i for i in range(N_STATIONS)]\n",
    "dz_cols = ['MatchedHit_DZ[%i]' % i for i in range(N_STATIONS)]\n",
    "dxyz_cols = dx_cols + dy_cols + dz_cols\n",
    "\n",
    "ex_cols = ['Lextra_X[%i]' % i for i in range(N_STATIONS)]\n",
    "ey_cols = ['Lextra_Y[%i]' % i for i in range(N_STATIONS)]\n",
    "exy_cols = ex_cols + ey_cols\n",
    "edx_cols = ['Mextra_DX2[%i]' % i for i in range(N_STATIONS)]\n",
    "edy_cols = ['Mextra_DY2[%i]' % i for i in range(N_STATIONS)]\n",
    "edxy_cols = edx_cols + edy_cols\n",
    "\n",
    "t_cols = ['MatchedHit_T[%i]' % i for i in range(N_STATIONS)]\n",
    "dt_cols = ['MatchedHit_DT[%i]' % i for i in range(N_STATIONS)]\n",
    "\n",
    "hit_type_cols = ['MatchedHit_TYPE[%i]' % i for i in range(N_STATIONS)]\n",
    "mom_cols = ['P', 'PT']\n",
    "hit_stats_cols = ['FOI_hits_N', 'NShared', 'ndof']\n",
    "\n",
    "ncl_cols = ['ncl[%i]' % i for i in range(N_STATIONS)]\n",
    "avg_cs_cols = ['avg_cs[%i]' % i for i in range(N_STATIONS)]\n",
    "\n",
    "# foi cols\n",
    "foi_xyz_cols = [\"FOI_hits_X\", \"FOI_hits_Y\", \"FOI_hits_Z\"]\n",
    "foi_dxyz_cols = [\"FOI_hits_DX\", \"FOI_hits_DY\", \"FOI_hits_DZ\"]\n",
    "foi_ts_cols = [\"FOI_hits_T\", \"FOI_hits_DT\", \"FOI_hits_S\"]\n",
    "foi_cols = foi_xyz_cols + foi_dxyz_cols + foi_ts_cols\n",
    "\n",
    "# derivative cols\n",
    "pca_x_cols = ['PCA_X[%i]' % i for i in range(N_STATIONS)]\n",
    "pca_y_cols = ['PCA_Y[%i]' % i for i in range(N_STATIONS)]\n",
    "pca_z_cols = ['PCA_Z[%i]' % i for i in range(N_STATIONS)]\n",
    "pca_xyz_cols = pca_x_cols + pca_y_cols + pca_z_cols\n",
    "\n",
    "nerr_x_cols = ['NErr_X[%i]' % i for i in range(N_STATIONS)]\n",
    "nerr_y_cols = ['NErr_Y[%i]' % i for i in range(N_STATIONS)]\n",
    "nerr_xy_cols = nerr_x_cols + nerr_y_cols\n",
    "\n",
    "da_cols = ['DAngle[%d]' % i for i in range(1, 4)]\n",
    "is_muon_cols = ['IsMuonTight']\n",
    "prob_hit_detector_cols = ['ProbHit[%i]' % i for i in range(N_STATIONS)]\n",
    "err_cols = ['ErrMSE', 'Chi2Quantile']\n",
    "\n",
    "SIMPLE_FEATURE_COLS = xyz_cols + dxyz_cols + exy_cols + edxy_cols + t_cols + dt_cols + hit_type_cols + mom_cols + hit_stats_cols + ncl_cols + avg_cs_cols\n",
    "ARR_FEATURE_COLS = foi_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(SIMPLE_FEATURE_COLS + ARR_FEATURE_COLS + ALL_TRAIN_COLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert csv to pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetMetaData:\n",
    "    def __init__(self, origin_csv_filenames, chunk_filenames_pattern, origin_col_set):\n",
    "        self.origin_csv_filenames = origin_csv_filenames\n",
    "        self.chunk_filenames_pattern = chunk_filenames_pattern\n",
    "        self.origin_col_set = origin_col_set\n",
    "        self.is_test = 'test_' in chunk_filenames_pattern\n",
    "\n",
    "meta_train = DatasetMetaData(\n",
    "    origin_csv_filenames=['data/train_part_1_v2.csv.gz', 'data/train_part_2_v2.csv.gz'],\n",
    "    chunk_filenames_pattern='data/train_{label}_{group}_{ind:03d}.pkl',\n",
    "    origin_col_set=SIMPLE_FEATURE_COLS + ARR_FEATURE_COLS + ALL_TRAIN_COLS\n",
    ")\n",
    "meta_pub_test = DatasetMetaData(\n",
    "    origin_csv_filenames=['data/test_public_v2.csv.gz'],\n",
    "    chunk_filenames_pattern='data/test_pub_{group}_{ind:03d}.pkl',\n",
    "    origin_col_set=SIMPLE_FEATURE_COLS + ARR_FEATURE_COLS\n",
    ")\n",
    "meta_pvt_test = DatasetMetaData(\n",
    "    origin_csv_filenames=[],\n",
    "    chunk_filenames_pattern='data/test_pvt_{group}_{ind:03d}.pkl',\n",
    "    origin_col_set=SIMPLE_FEATURE_COLS + ARR_FEATURE_COLS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import repeat\n",
    "\n",
    "class CsvDataReader:\n",
    "    int_dtype = np.int32\n",
    "    float_dtype = np.float32\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.na_values = ['-9999.0', '255']\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_read_stream(filenames, usecols, chunk_size=25000):\n",
    "        return CsvDataReader()._get_read_stream(filenames, usecols, chunk_size)\n",
    "    \n",
    "    def _get_read_stream(self, filenames, usecols, chunk_size):\n",
    "        for filename in filenames:\n",
    "            data_generator = pd.read_csv(\n",
    "                filename, usecols=usecols, chunksize=chunk_size, #nrows=400000,\n",
    "                na_values=self.na_values, na_filter=False, \n",
    "                converters=self._get_converters(), dtype=self._get_types()\n",
    "            )\n",
    "            for data in data_generator:\n",
    "                yield data\n",
    "\n",
    "    def _get_converters(self):\n",
    "        def parse_float_array(line):\n",
    "            return np.fromstring(line[1:-1], sep=\" \", dtype=self.float_dtype)\n",
    "\n",
    "        def parse_int_array(line):\n",
    "            return np.fromstring(line[1:-1], sep=\" \", dtype=self.int_dtype)\n",
    "    \n",
    "        converters = dict(zip(ARR_FEATURE_COLS, repeat(parse_float_array)))\n",
    "        converters[foi_ts_cols[-1]] = parse_int_array\n",
    "        return\n",
    "    \n",
    "    def _get_types(self):\n",
    "        types = dict(zip(SIMPLE_FEATURE_COLS + ALL_TRAIN_COLS, repeat(self.float_dtype)))\n",
    "        for col in unused_train_cols[:1] + train_cols[:1] + hit_stats_cols + ncl_cols + hit_type_cols:\n",
    "            types[col] = self.int_dtype\n",
    "        return types;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read from pickle chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBuffer:\n",
    "    def __init__(self):\n",
    "        self._frames = []\n",
    "    \n",
    "    def append(self, frame):\n",
    "        self._frames.append(frame)\n",
    "    \n",
    "    def cut(self, nrows):\n",
    "        nrows = min(nrows, self.nrows)\n",
    "        merged = self._merge_frames()\n",
    "        head = merged.iloc[:nrows, :]\n",
    "        tail = merged.iloc[nrows:, :]\n",
    "        \n",
    "        self._frames = [tail]\n",
    "        return head, nrows\n",
    "    \n",
    "    def _merge_frames(self):\n",
    "        if len(self._frames) > 1:\n",
    "            merged = pd.concat(self._frames, axis=0, ignore_index=True)\n",
    "            self._frames = [merged]\n",
    "        return self._frames[0]\n",
    "    \n",
    "    @property\n",
    "    def nrows(self):\n",
    "        return sum([len(frame.index) for frame in self._frames])\n",
    "    \n",
    "    @property\n",
    "    def is_empty(self):\n",
    "        return self.nrows == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTank:\n",
    "    def __init__(self, max_volume, callback_on_full):\n",
    "        self._max_volume = max_volume\n",
    "        self._buffer = DataBuffer()\n",
    "        self._on_full = callback_on_full\n",
    "    \n",
    "    def add(self, frame):\n",
    "        self._buffer.append(frame)\n",
    "        flushed = 0\n",
    "        while self._is_full():\n",
    "            flushed += self.flush()\n",
    "        return flushed\n",
    "        \n",
    "    def flush(self):\n",
    "        if self._buffer.is_empty:\n",
    "            return 0\n",
    "        flushed_data, flushed_vol = self._buffer.cut(self._max_volume)\n",
    "        self._on_full(flushed_data)\n",
    "        return flushed_vol\n",
    "    \n",
    "    def _is_full(self):\n",
    "        return self._buffer.nrows >= self._max_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDatasetHelper:\n",
    "    def __init__(self, filename_pattern):\n",
    "        self._filename_pattern = filename_pattern\n",
    "\n",
    "    def filter_frame(self, frame):\n",
    "        return frame\n",
    "        \n",
    "    def get_col_groups(self):\n",
    "        return col_groups[:-1]\n",
    "        \n",
    "    def generate_chunk_filename(self, group_key, chunk_ind):\n",
    "        return self._filename_pattern.format(group=group_key, ind=chunk_ind)\n",
    "    \n",
    "\n",
    "class TrainDatasetHelper:\n",
    "    def __init__(self, filename_pattern, label, label_key):\n",
    "        self._filename_pattern = filename_pattern\n",
    "        self._label = label\n",
    "        self._label_key = label_key\n",
    "        \n",
    "    def filter_frame(self, frame):\n",
    "        return frame.loc[frame.label == self._label, :]\n",
    "        \n",
    "    def get_col_groups(self):\n",
    "        return col_groups\n",
    "        \n",
    "    def generate_chunk_filename(self, group_key, chunk_ind):\n",
    "        return self._filename_pattern.format(label=self._label_key, group=group_key, ind=chunk_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PickleDataWriter:\n",
    "    def __init__(self, helper, chunk_size):\n",
    "        self._helper = helper\n",
    "        self._data_tank = DataTank(max_volume=chunk_size, callback_on_full=self._flush_chunk)\n",
    "        self._chunk_index = 0\n",
    "    \n",
    "    def store(self, frame):\n",
    "        filtered_frame = self._helper.filter_frame(frame)\n",
    "        return self._data_tank.add(filtered_frame)\n",
    "        \n",
    "    def flush(self):\n",
    "        return self._data_tank.flush()\n",
    "    \n",
    "    def _flush_chunk(self, chunk):\n",
    "        self._store_chunk(chunk, self._chunk_index)\n",
    "        self._chunk_index += 1\n",
    "        \n",
    "    def _store_chunk(self, chunk, chunk_index):\n",
    "        for group_key, col_group in self._helper.get_col_groups():\n",
    "            filename = self._helper.generate_chunk_filename(group_key, chunk_index)\n",
    "            chunk.loc[:, col_group].to_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "class PickleDataReader:\n",
    "    def __init__(self, helper):\n",
    "        self._helper = helper\n",
    "        self._result = None\n",
    "        \n",
    "    def read(self, nrows, cols):\n",
    "        data_tank = DataTank(nrows, self._set_read_result)\n",
    "        for frame in self._read_chunks(cols):\n",
    "            if data_tank.add(frame) > 0:\n",
    "                return self._result\n",
    "\n",
    "        data_tank.flush()\n",
    "        return self._result\n",
    "        \n",
    "    def _read_chunks(self, cols):\n",
    "        chunk_index = 0\n",
    "        while True:\n",
    "            frame = self._read_chunk(chunk_index, cols)\n",
    "            if frame is None:\n",
    "                break\n",
    "            \n",
    "            yield frame\n",
    "            chunk_index += 1\n",
    "            \n",
    "    def _set_read_result(self, data):\n",
    "        self._result = data\n",
    "    \n",
    "    def _read_chunk(self, chunk_index, cols):\n",
    "        chunk_parts = []\n",
    "        for group_key, col_group in self._helper.get_col_groups():\n",
    "            cols_ = self._intersect_cols(cols, set(col_group))\n",
    "            if not cols_:\n",
    "                continue\n",
    "            \n",
    "            filename = self._helper.generate_chunk_filename(group_key, chunk_index)\n",
    "            if not os.path.exists(filename):\n",
    "                return None\n",
    "            \n",
    "            chunk_part = pd.read_pickle(filename).loc[:, cols_]\n",
    "            chunk_parts.append(chunk_part)\n",
    "            \n",
    "        return pd.concat(chunk_parts, axis=1, sort=False)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _intersect_cols(cols, col_subset):\n",
    "        return [col for col in cols if col in col_subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetConverter:\n",
    "    def __init__(self):\n",
    "        self._stored = 0\n",
    "                \n",
    "    @staticmethod\n",
    "    def convert(data_set_meta: DatasetMetaData, chunk_size=50000):\n",
    "        dataframes_stream = CsvDataReader.get_read_stream(data_set_meta.origin_csv_filenames, data_set_meta.origin_col_set)\n",
    "        \n",
    "        filename_pattern = data_set_meta.chunk_filenames_pattern\n",
    "        if data_set_meta.is_test:\n",
    "            writers = [PickleDataWriter(TestDatasetHelper(filename_pattern), chunk_size)]\n",
    "        else: \n",
    "            writers = [PickleDataWriter(TrainDatasetHelper(filename_pattern, i, label_prefixes[i]), chunk_size) for i in range(2)]\n",
    "            \n",
    "        DatasetConverter()._store_chunkified(dataframes_stream, writers)\n",
    "        \n",
    "    def _store_chunkified(self, dataframes_stream, writers):\n",
    "        for data in dataframes_stream:\n",
    "            for writer in writers:\n",
    "                self._print_stored(writer.store(data))\n",
    "\n",
    "        for writer in writers:\n",
    "            self._print_stored(writer.flush())\n",
    "            \n",
    "    def _print_stored(self, stored):\n",
    "        if stored == 0:\n",
    "            return\n",
    "        self._stored += stored\n",
    "        if self._stored % 200000 == 0:\n",
    "            print('Stored: {0}M'.format(self._stored / 1000000.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetReader:   \n",
    "    @staticmethod\n",
    "    def read_dataset(data_set_meta: DatasetMetaData, cols, nrows=None, prop_0=.5):\n",
    "        nrows = nrows if nrows is not None else 100000000\n",
    "        filename_pattern = data_set_meta.chunk_filenames_pattern\n",
    "        if data_set_meta.is_test:\n",
    "            readers = [PickleDataReader(TestDatasetHelper(filename_pattern))]\n",
    "            proportions = [nrows]\n",
    "        else: \n",
    "            readers = [PickleDataReader(TrainDatasetHelper(filename_pattern, i, label_prefixes[i])) for i in range(2)]\n",
    "            nrows0 = int(nrows * prop_0)\n",
    "            proportions = [nrows0, nrows - nrows0]\n",
    "            \n",
    "        return DatasetReader()._read_dataset(readers, cols, proportions)\n",
    "            \n",
    "    def _read_dataset(self, readers, cols, proportions):\n",
    "        data_parts = []\n",
    "        delta = 0\n",
    "        for reader, nrows in zip(readers, proportions):\n",
    "            data_part = reader.read(nrows + delta, cols)\n",
    "            data_parts.append(data_part)\n",
    "            delta = nrows - len(data_part.index)\n",
    "            \n",
    "        data = pd.concat(data_parts, axis=0, ignore_index=True)\n",
    "        return data\n",
    "    \n",
    "def convert_train():\n",
    "    DatasetConverter.convert(meta_train)\n",
    "\n",
    "def convert_pub_test():\n",
    "    DatasetConverter.convert(meta_pub_test)\n",
    "    \n",
    "def convert_pvt_test():\n",
    "    DatasetConverter.convert(meta_pvt_test)\n",
    "    \n",
    "def read_train(cols, rows):\n",
    "    return DatasetReader.read_dataset(meta_train, cols + train_cols, rows)\n",
    "\n",
    "def read_pub_test(cols, rows):\n",
    "    return DatasetReader.read_dataset(meta_pub_test, cols, rows)\n",
    "\n",
    "def read_pvt_test(cols, rows):\n",
    "    return DatasetReader.read_dataset(meta_pvt_test, cols, rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored: 0.2M\n",
      "Stored: 0.4M\n",
      "Stored: 0.6M\n",
      "Stored: 0.8M\n",
      "Stored: 1.0M\n",
      "Stored: 1.2M\n",
      "Stored: 1.4M\n",
      "Stored: 1.6M\n",
      "Stored: 1.8M\n",
      "Stored: 2.0M\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "convert_train()\n",
    "# convert_pub_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000000, 20)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = read_train(xyz_cols + exy_cols + train_cols, 2000000)\n",
    "display(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MatchedHit_X[0]</th>\n",
       "      <th>MatchedHit_X[1]</th>\n",
       "      <th>MatchedHit_X[2]</th>\n",
       "      <th>MatchedHit_X[3]</th>\n",
       "      <th>MatchedHit_Y[0]</th>\n",
       "      <th>MatchedHit_Y[1]</th>\n",
       "      <th>MatchedHit_Y[2]</th>\n",
       "      <th>MatchedHit_Y[3]</th>\n",
       "      <th>MatchedHit_Z[0]</th>\n",
       "      <th>MatchedHit_Z[1]</th>\n",
       "      <th>MatchedHit_Z[2]</th>\n",
       "      <th>MatchedHit_Z[3]</th>\n",
       "      <th>Lextra_X[0]</th>\n",
       "      <th>Lextra_X[1]</th>\n",
       "      <th>Lextra_X[2]</th>\n",
       "      <th>Lextra_X[3]</th>\n",
       "      <th>Lextra_Y[0]</th>\n",
       "      <th>Lextra_Y[1]</th>\n",
       "      <th>Lextra_Y[2]</th>\n",
       "      <th>Lextra_Y[3]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-953.080017</td>\n",
       "      <td>-1028.400024</td>\n",
       "      <td>-1072.170044</td>\n",
       "      <td>-1145.569946</td>\n",
       "      <td>2164.736572</td>\n",
       "      <td>2332.150391</td>\n",
       "      <td>2500.885254</td>\n",
       "      <td>2671.054932</td>\n",
       "      <td>15410.896484</td>\n",
       "      <td>16615.699219</td>\n",
       "      <td>17809.107422</td>\n",
       "      <td>19017.820312</td>\n",
       "      <td>-979.726379</td>\n",
       "      <td>-1081.261353</td>\n",
       "      <td>-1182.796387</td>\n",
       "      <td>-1284.331299</td>\n",
       "      <td>2143.538086</td>\n",
       "      <td>2311.782959</td>\n",
       "      <td>2480.027832</td>\n",
       "      <td>2648.272705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1935.099976</td>\n",
       "      <td>2140.560059</td>\n",
       "      <td>2310.600098</td>\n",
       "      <td>2467.820068</td>\n",
       "      <td>-1465.656616</td>\n",
       "      <td>-1580.002197</td>\n",
       "      <td>-1694.378418</td>\n",
       "      <td>-1809.069702</td>\n",
       "      <td>15399.722656</td>\n",
       "      <td>16606.611328</td>\n",
       "      <td>17799.798828</td>\n",
       "      <td>19008.287109</td>\n",
       "      <td>1870.011353</td>\n",
       "      <td>2059.895996</td>\n",
       "      <td>2249.780518</td>\n",
       "      <td>2439.665039</td>\n",
       "      <td>-1465.227051</td>\n",
       "      <td>-1580.495728</td>\n",
       "      <td>-1695.764404</td>\n",
       "      <td>-1811.033203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-62.421665</td>\n",
       "      <td>-178.475006</td>\n",
       "      <td>-464.503326</td>\n",
       "      <td>-1051.069946</td>\n",
       "      <td>-430.946503</td>\n",
       "      <td>-579.326843</td>\n",
       "      <td>-570.592285</td>\n",
       "      <td>-1062.020752</td>\n",
       "      <td>15401.548828</td>\n",
       "      <td>16404.212891</td>\n",
       "      <td>17714.044922</td>\n",
       "      <td>19004.376953</td>\n",
       "      <td>-59.783524</td>\n",
       "      <td>-157.381760</td>\n",
       "      <td>-254.979996</td>\n",
       "      <td>-352.578217</td>\n",
       "      <td>-485.069519</td>\n",
       "      <td>-522.643494</td>\n",
       "      <td>-560.217529</td>\n",
       "      <td>-597.791504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-181.130005</td>\n",
       "      <td>-123.625000</td>\n",
       "      <td>-23.803333</td>\n",
       "      <td>151.320007</td>\n",
       "      <td>-336.147125</td>\n",
       "      <td>-363.382050</td>\n",
       "      <td>-352.796204</td>\n",
       "      <td>-373.926514</td>\n",
       "      <td>15401.890625</td>\n",
       "      <td>16605.992188</td>\n",
       "      <td>17798.830078</td>\n",
       "      <td>18929.453125</td>\n",
       "      <td>-247.451950</td>\n",
       "      <td>-221.716705</td>\n",
       "      <td>-195.981445</td>\n",
       "      <td>-170.246201</td>\n",
       "      <td>-303.306122</td>\n",
       "      <td>-327.093506</td>\n",
       "      <td>-350.880890</td>\n",
       "      <td>-374.668274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-255.330002</td>\n",
       "      <td>63.401669</td>\n",
       "      <td>109.533333</td>\n",
       "      <td>23.453333</td>\n",
       "      <td>420.254272</td>\n",
       "      <td>354.039093</td>\n",
       "      <td>306.612427</td>\n",
       "      <td>405.169739</td>\n",
       "      <td>15119.612305</td>\n",
       "      <td>16412.574219</td>\n",
       "      <td>17606.003906</td>\n",
       "      <td>18815.257812</td>\n",
       "      <td>169.635284</td>\n",
       "      <td>143.966980</td>\n",
       "      <td>118.298660</td>\n",
       "      <td>92.630348</td>\n",
       "      <td>493.114594</td>\n",
       "      <td>531.561035</td>\n",
       "      <td>570.007507</td>\n",
       "      <td>608.453918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MatchedHit_X[0]  MatchedHit_X[1]  MatchedHit_X[2]  MatchedHit_X[3]  \\\n",
       "0      -953.080017     -1028.400024     -1072.170044     -1145.569946   \n",
       "1      1935.099976      2140.560059      2310.600098      2467.820068   \n",
       "2       -62.421665      -178.475006      -464.503326     -1051.069946   \n",
       "3      -181.130005      -123.625000       -23.803333       151.320007   \n",
       "4      -255.330002        63.401669       109.533333        23.453333   \n",
       "\n",
       "   MatchedHit_Y[0]  MatchedHit_Y[1]  MatchedHit_Y[2]  MatchedHit_Y[3]  \\\n",
       "0      2164.736572      2332.150391      2500.885254      2671.054932   \n",
       "1     -1465.656616     -1580.002197     -1694.378418     -1809.069702   \n",
       "2      -430.946503      -579.326843      -570.592285     -1062.020752   \n",
       "3      -336.147125      -363.382050      -352.796204      -373.926514   \n",
       "4       420.254272       354.039093       306.612427       405.169739   \n",
       "\n",
       "   MatchedHit_Z[0]  MatchedHit_Z[1]  MatchedHit_Z[2]  MatchedHit_Z[3]  \\\n",
       "0     15410.896484     16615.699219     17809.107422     19017.820312   \n",
       "1     15399.722656     16606.611328     17799.798828     19008.287109   \n",
       "2     15401.548828     16404.212891     17714.044922     19004.376953   \n",
       "3     15401.890625     16605.992188     17798.830078     18929.453125   \n",
       "4     15119.612305     16412.574219     17606.003906     18815.257812   \n",
       "\n",
       "   Lextra_X[0]  Lextra_X[1]  Lextra_X[2]  Lextra_X[3]  Lextra_Y[0]  \\\n",
       "0  -979.726379 -1081.261353 -1182.796387 -1284.331299  2143.538086   \n",
       "1  1870.011353  2059.895996  2249.780518  2439.665039 -1465.227051   \n",
       "2   -59.783524  -157.381760  -254.979996  -352.578217  -485.069519   \n",
       "3  -247.451950  -221.716705  -195.981445  -170.246201  -303.306122   \n",
       "4   169.635284   143.966980   118.298660    92.630348   493.114594   \n",
       "\n",
       "   Lextra_Y[1]  Lextra_Y[2]  Lextra_Y[3]  \n",
       "0  2311.782959  2480.027832  2648.272705  \n",
       "1 -1580.495728 -1695.764404 -1811.033203  \n",
       "2  -522.643494  -560.217529  -597.791504  \n",
       "3  -327.093506  -350.880890  -374.668274  \n",
       "4   531.561035   570.007507   608.453918  "
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-152-2ec0c6730a08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\envs\\tf\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   4374\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4375\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4376\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4378\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'label'"
     ]
    }
   ],
   "source": [
    "train.loc[train.label == 0, :].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30588 369412\n"
     ]
    }
   ],
   "source": [
    "def get_class(i):\n",
    "    return train.index[train.label == i]\n",
    "\n",
    "# Gets `n_rows` random samples from `data`\n",
    "def get_samples(data, n_rows):\n",
    "    indices = np.random.randint(len(data), size=n_rows)\n",
    "    indices = np.sort(indices)\n",
    "    return data.iloc[indices]\n",
    "\n",
    "# Gets `n_rows` random samples from `data` with specified class proportions.\n",
    "# If `prop_0` is None, it keeps `data`s natural proportions.\n",
    "def get_samples_w_proptions(data, n_rows, prop_0=None):\n",
    "    def get_class_samples(class_i, n_rows):\n",
    "        class_indices = np.random.randint(len(class_i), size=n_rows)\n",
    "        return class_i[class_indices]\n",
    "    \n",
    "    if prop_0 is None:\n",
    "        prop_0 = len(class_0) / (len(class_0) + len(class_1))\n",
    "    cnt_0 = int(n_rows * prop_0)\n",
    "    class_0_indices = get_class_samples(class_0, cnt_0)\n",
    "    class_1_indices = get_class_samples(class_1, n_rows - cnt_0)\n",
    "    indices = np.concatenate((class_0_indices, class_1_indices))\n",
    "    indices = np.sort(indices)\n",
    "    return data.loc[indices, :]\n",
    "\n",
    "# Gets `n_rows` head samples from `data` with specified class proportions.\n",
    "# If `prop_0` is None, it keeps `data`s natural proportions.\n",
    "# If `data` samples is not enough to fulfil proportions for any `class_i` then random sampling from the `class_i` is applied.\n",
    "def get_head_w_proportions(data, n_rows, prop_0=None):\n",
    "    def get_class_samples_head_smart(class_i, n_rows):\n",
    "        cls_len = len(class_i)\n",
    "        if cls_len > n_rows:\n",
    "            return class_i[:n_rows]\n",
    "        class_indices = np.concatenate((np.arange(cls_len),  np.random.randint(cls_len, size=n_rows-cls_len)))\n",
    "        return class_i[class_indices]\n",
    "    \n",
    "    if prop_0 is None:\n",
    "        prop_0 = len(class_0) / (len(class_0) + len(class_1))\n",
    "    cnt_0 = int(n_rows * prop_0)\n",
    "    class_0_indices = get_class_samples_head_smart(class_0, cnt_0)\n",
    "    class_1_indices = get_class_samples_head_smart(class_1, n_rows - cnt_0)\n",
    "    indices = np.concatenate((class_0_indices, class_1_indices))\n",
    "    indices = np.sort(indices)\n",
    "    return data.loc[indices, :]\n",
    "    \n",
    "print(np.count_nonzero(train.label == 0), np.count_nonzero(train.label == 1))\n",
    "\n",
    "class_0 = get_class(0)\n",
    "class_1 = get_class(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn import model_selection as mdsel\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "def fit(train):\n",
    "    target_train = train[train_cols]\n",
    "    \n",
    "    # defined much later\n",
    "    transformer = DataTransformer().fit(train)\n",
    "    train = transformer.transform(train)\n",
    "    \n",
    "    estimator = xgb.XGBClassifier(n_estimators=60, n_jobs=3)\n",
    "    estimator.fit(train.values, target_train.label.values, sample_weight=target_train.weight.values, eval_metric=scoring.rejection90_sklearn)\n",
    "    return transformer, estimator\n",
    "    \n",
    "def predict(fitted_state, test):\n",
    "    transformer, estimator = fitted_state\n",
    "    \n",
    "    test = transformer.transform(test)\n",
    "    predictions = estimator.predict_proba(test.values)[:, 1]\n",
    "    return predictions\n",
    "\n",
    "def score(fitted_state, test):\n",
    "    target_test = test.loc[:, train_cols]\n",
    "    predictions = predict(fitted_state, test)\n",
    "    return scoring.rejection90(target_test.label.values, predictions, sample_weight=target_test.weight.values)\n",
    "\n",
    "def fit_predict_save(train, test, filename):\n",
    "    fitted_state = fit(train)\n",
    "    predictions = predict(fitted_state, test)\n",
    "    \n",
    "    pd.DataFrame(data={\"prediction\": predictions}, index=test.index).to_csv(\n",
    "        filename, index_label=utils.ID_COLUMN\n",
    "    )\n",
    "    \n",
    "    model = fitted_state[1]\n",
    "    model_filename = filename.replace('out/', 'models/').replace('.csv', '.xgb')\n",
    "    model.save_model(model_filename)\n",
    "    \n",
    "def fit_save_model(train, filename):\n",
    "    if filename.endswith('.csv'):\n",
    "        filename = filename.replace('out/', 'models/').replace('.csv', '.xgb')\n",
    "        \n",
    "    _, model = fit(train)\n",
    "    model.save_model(filename)\n",
    "    \n",
    "def cross_validate(train, n_splits, n_rows):\n",
    "    train = get_head_w_proportions(train, n_rows, .5)\n",
    "    \n",
    "    splitter = mdsel.StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "    scores = []\n",
    "    for train_indices, test_indices in splitter.split(train, train.label):\n",
    "        train_subset = train.iloc[train_indices, :]\n",
    "        test_subset = train.iloc[test_indices, :]\n",
    "        \n",
    "        fit_state = fit(train_subset)\n",
    "        \n",
    "        target_test = test_subset[train_cols]\n",
    "        predictions = predict(fit_state, test_subset)\n",
    "        \n",
    "        y_true = target_test.label.values\n",
    "        l, r, ep = scoring.get_threshold_details(y_true, predictions, sample_weight=target_test.weight.values)\n",
    "        threshold = (l + r) / 2\n",
    "        y_pred = predictions >= threshold\n",
    "                \n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        prec = precision_score(y_true, y_pred)\n",
    "        rec = recall_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        roc_auc = roc_auc_score(y_true, predictions)\n",
    "        scr = scoring.rejection90(y_true, predictions, sample_weight=target_test.weight.values)\n",
    "        scores += [[acc, prec, rec, f1, roc_auc, scr, threshold, r - l]]\n",
    "\n",
    "    return pd.DataFrame(scores, columns=['acc', 'prec', 'rec', 'f1', 'roc_auc', 'scr', 'th', 'dTh'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transformer\n",
    "\n",
    "Это по сути основная часть. Класс, который отбирает нужные столбцы, возможно что-то модифицирует или добавляет. На выходе - входные данные для модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def fill_na(data):\n",
    "    mask = data.isna()\n",
    "    means = data.mean(skipna=True)\n",
    "    data.fillna(means, inplace=True)\n",
    "    return mask\n",
    "\n",
    "def restore_na(data, mask):\n",
    "    data.mask(mask, other=np.NaN, inplace=True)\n",
    "    \n",
    "def get_nth_detector_coords(i):\n",
    "    return [x_cols[i], y_cols[i], z_cols[i]]\n",
    "\n",
    "def get_nth_detector_coords_pca(i):\n",
    "    return [pca_x_cols[i], pca_y_cols[i], pca_z_cols[i]]\n",
    "    \n",
    "def pca_fit(data):\n",
    "    cols = get_nth_detector_coords(0)\n",
    "    data = data[cols].copy()\n",
    "    \n",
    "    fill_na(data)\n",
    "    pca_model = PCA(n_components=3)\n",
    "    pca_model.fit(data)\n",
    "    return pca_model\n",
    "\n",
    "def pca_transform(pca_model, data, features):\n",
    "    for i in range(4):\n",
    "        cols = get_nth_detector_coords(i)\n",
    "        new_cols = get_nth_detector_coords_pca(i)\n",
    "        data_detector = data.loc[:, cols]\n",
    "        \n",
    "        mask = fill_na(data_detector)\n",
    "        transformed_data = pca_model.transform(data_detector.values)\n",
    "        restore_na(data_detector, mask)\n",
    "        return\n",
    "        \n",
    "        for j in range(3):\n",
    "            data[new_cols[j]] = transformed_data[:, j]\n",
    "\n",
    "    features += pca_coord_cols\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cosine dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_coses(data, features):\n",
    "    def get_layer_coords(data, i):\n",
    "        return data[[x_cols[i], y_cols[i], z_cols[i]]].values\n",
    "\n",
    "    def dot(x, y):\n",
    "        return np.sum(x * y, axis=1)\n",
    "    \n",
    "    def norm(x):\n",
    "        return np.sqrt(dot(x, x))\n",
    "\n",
    "    def get_cosine_dist(L1, L2, L1_norm, L2_norm):\n",
    "        return dot(L1, L2) / L1_norm / L2_norm\n",
    "    \n",
    "    def get_angle(cosines):\n",
    "        return np.arccos(cosines, dtype=np.float32) / np.pi * 180\n",
    "    \n",
    "    layers = np.array([get_layer_coords(data, i) for i in range(4)])\n",
    "    layers[1:] -= layers[:3]\n",
    "    norms = list(map(norm, layers))\n",
    "    \n",
    "    for i in range(3):\n",
    "        cosines = get_cosine_dist(layers[i], layers[i+1], norms[i], norms[i+1])\n",
    "        angles = get_angle(cosines)\n",
    "        data[da_cols[i]] = angles\n",
    "        \n",
    "    features += da_cols        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### IsMuon && IsMuonTight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_is_muon_tight(data, features):\n",
    "    return add_is_muon(data, features, threshold=2)\n",
    "    \n",
    "def add_is_muon(data, features, threshold=1):\n",
    "    def lt(p):\n",
    "        return data.P < p\n",
    "    def gt(p):\n",
    "        return data.P >= p\n",
    "    def M(i):\n",
    "        return data[hit_type_cols[i]] >= threshold\n",
    "    \n",
    "    lt_6k_mask = lt(6000.) & M(0) & M(1)\n",
    "    lt_10k_gt_6k_mask = gt(6000.) & lt(10000.) & M(0) & M(1) & (M(2) | M(3))\n",
    "    gt_10k_mask = gt(10000.) & M(0) & M(1) & M(2) & M(3)\n",
    "    \n",
    "    data.loc[:, is_muon_cols[0]] = 1 * (lt_6k_mask | lt_10k_gt_6k_mask | gt_10k_mask)\n",
    "    features += is_muon_cols        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Probability hit detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_probability_hit_detector(data, features):\n",
    "    p = data[mom_cols[0]].values\n",
    "    \n",
    "    def prob(i):\n",
    "        alpha = (0.0260, 0.0021, 0.0015, 0.0008)\n",
    "        beta = (2040., 2387., 3320., 3903.)\n",
    "        t = (alpha[i] * (p - beta[i]))**(i+1)\n",
    "        return t / (1 + t)\n",
    "        \n",
    "    for i in range(4):\n",
    "        data.loc[:, prob_hit_detector_cols[i]] = prob(i)\n",
    "        \n",
    "    features += prob_hit_detector_cols\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_mse(data, features):\n",
    "    dxy = (data.loc[:, xy_cols].values - data.loc[:, exy_cols].values) / data.loc[:, dx_cols + dy_cols].values / 2.\n",
    "    D = np.mean(dxy**2, axis=1)\n",
    "    \n",
    "    data.loc[:, err_cols[0]] = D\n",
    "    features += [err_cols[0]]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_normed_err(data, features):\n",
    "    dxy = data.loc[:, xy_cols].values - data.loc[:, exy_cols].values\n",
    "    normed_errors = dxy / np.sqrt(data.loc[:, edxy_cols].values)\n",
    "    \n",
    "    for i in range(4):\n",
    "        data.loc[:, nerr_x_cols[i]] = normed_errors[:, i]\n",
    "        data.loc[:, nerr_y_cols[i]] = normed_errors[:, i + 4]\n",
    "    \n",
    "    features += nerr_xy_cols\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>scr</th>\n",
       "      <th>th</th>\n",
       "      <th>dTh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.756900</td>\n",
       "      <td>0.706487</td>\n",
       "      <td>0.879001</td>\n",
       "      <td>0.783350</td>\n",
       "      <td>0.797067</td>\n",
       "      <td>0.690523</td>\n",
       "      <td>0.066475</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000614</td>\n",
       "      <td>0.001772</td>\n",
       "      <td>0.004764</td>\n",
       "      <td>0.000989</td>\n",
       "      <td>0.005021</td>\n",
       "      <td>0.028296</td>\n",
       "      <td>0.001094</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.756376</td>\n",
       "      <td>0.704573</td>\n",
       "      <td>0.873725</td>\n",
       "      <td>0.782223</td>\n",
       "      <td>0.791623</td>\n",
       "      <td>0.665091</td>\n",
       "      <td>0.065471</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.756562</td>\n",
       "      <td>0.705695</td>\n",
       "      <td>0.877007</td>\n",
       "      <td>0.782989</td>\n",
       "      <td>0.794843</td>\n",
       "      <td>0.675283</td>\n",
       "      <td>0.065892</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.756749</td>\n",
       "      <td>0.706818</td>\n",
       "      <td>0.880288</td>\n",
       "      <td>0.783755</td>\n",
       "      <td>0.798062</td>\n",
       "      <td>0.685474</td>\n",
       "      <td>0.066313</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.757162</td>\n",
       "      <td>0.707444</td>\n",
       "      <td>0.881638</td>\n",
       "      <td>0.783914</td>\n",
       "      <td>0.799789</td>\n",
       "      <td>0.703239</td>\n",
       "      <td>0.066977</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.708070</td>\n",
       "      <td>0.882988</td>\n",
       "      <td>0.784073</td>\n",
       "      <td>0.801516</td>\n",
       "      <td>0.721003</td>\n",
       "      <td>0.067641</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            acc      prec       rec        f1   roc_auc       scr        th  \\\n",
       "count  3.000000  3.000000  3.000000  3.000000  3.000000  3.000000  3.000000   \n",
       "mean   0.756900  0.706487  0.879001  0.783350  0.797067  0.690523  0.066475   \n",
       "std    0.000614  0.001772  0.004764  0.000989  0.005021  0.028296  0.001094   \n",
       "min    0.756376  0.704573  0.873725  0.782223  0.791623  0.665091  0.065471   \n",
       "25%    0.756562  0.705695  0.877007  0.782989  0.794843  0.675283  0.065892   \n",
       "50%    0.756749  0.706818  0.880288  0.783755  0.798062  0.685474  0.066313   \n",
       "75%    0.757162  0.707444  0.881638  0.783914  0.799789  0.703239  0.066977   \n",
       "max    0.757576  0.708070  0.882988  0.784073  0.801516  0.721003  0.067641   \n",
       "\n",
       "       dTh  \n",
       "count  3.0  \n",
       "mean   0.0  \n",
       "std    0.0  \n",
       "min    0.0  \n",
       "25%    0.0  \n",
       "50%    0.0  \n",
       "75%    0.0  \n",
       "max    0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class DataTransformer(TransformerMixin):\n",
    "    def __init__(self, *featurizers):\n",
    "        self.featurizers = featurizers\n",
    "    \n",
    "    def fit(self, data, y=None):\n",
    "#         self.pca_model = pca_fit(data)\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        data = data.copy()\n",
    "        features = [] + xyz_cols + mom_cols + hit_type_cols + dxyz_cols + exy_cols + edxy_cols\n",
    "\n",
    "#         pca_transform(self.pca_model, data, features)\n",
    "#         add_is_muon(data, features)\n",
    "#         add_is_muon_tight(data, features)\n",
    "        add_probability_hit_detector(data, features)\n",
    "        add_coses(data, features)\n",
    "        add_mse(data, features)\n",
    "        add_normed_err(data, features)\n",
    "                \n",
    "        if features:\n",
    "            data = data.loc[:, features]\n",
    "        else:\n",
    "            data = data.drop(train_cols, axis=1)\n",
    "        return data\n",
    "\n",
    "df_scores = cross_validate(train, n_splits=3, n_rows=20000)\n",
    "display(df_scores.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>scr</th>\n",
       "      <th>th</th>\n",
       "      <th>dTh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.765430</td>\n",
       "      <td>0.716089</td>\n",
       "      <td>0.87966</td>\n",
       "      <td>0.789482</td>\n",
       "      <td>0.805171</td>\n",
       "      <td>0.746396</td>\n",
       "      <td>0.065736</td>\n",
       "      <td>0.000028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.003003</td>\n",
       "      <td>0.003847</td>\n",
       "      <td>0.00338</td>\n",
       "      <td>0.002117</td>\n",
       "      <td>0.003652</td>\n",
       "      <td>0.024239</td>\n",
       "      <td>0.001910</td>\n",
       "      <td>0.000063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.760100</td>\n",
       "      <td>0.709792</td>\n",
       "      <td>0.87680</td>\n",
       "      <td>0.785784</td>\n",
       "      <td>0.799652</td>\n",
       "      <td>0.722304</td>\n",
       "      <td>0.063544</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.766200</td>\n",
       "      <td>0.714990</td>\n",
       "      <td>0.87750</td>\n",
       "      <td>0.789803</td>\n",
       "      <td>0.804554</td>\n",
       "      <td>0.729295</td>\n",
       "      <td>0.064457</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.766650</td>\n",
       "      <td>0.718362</td>\n",
       "      <td>0.87870</td>\n",
       "      <td>0.790256</td>\n",
       "      <td>0.804688</td>\n",
       "      <td>0.743997</td>\n",
       "      <td>0.065446</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.767100</td>\n",
       "      <td>0.718512</td>\n",
       "      <td>0.88000</td>\n",
       "      <td>0.790482</td>\n",
       "      <td>0.808034</td>\n",
       "      <td>0.752049</td>\n",
       "      <td>0.066913</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.767100</td>\n",
       "      <td>0.718791</td>\n",
       "      <td>0.88530</td>\n",
       "      <td>0.791082</td>\n",
       "      <td>0.808925</td>\n",
       "      <td>0.784336</td>\n",
       "      <td>0.068320</td>\n",
       "      <td>0.000142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            acc      prec      rec        f1   roc_auc       scr        th  \\\n",
       "count  5.000000  5.000000  5.00000  5.000000  5.000000  5.000000  5.000000   \n",
       "mean   0.765430  0.716089  0.87966  0.789482  0.805171  0.746396  0.065736   \n",
       "std    0.003003  0.003847  0.00338  0.002117  0.003652  0.024239  0.001910   \n",
       "min    0.760100  0.709792  0.87680  0.785784  0.799652  0.722304  0.063544   \n",
       "25%    0.766200  0.714990  0.87750  0.789803  0.804554  0.729295  0.064457   \n",
       "50%    0.766650  0.718362  0.87870  0.790256  0.804688  0.743997  0.065446   \n",
       "75%    0.767100  0.718512  0.88000  0.790482  0.808034  0.752049  0.066913   \n",
       "max    0.767100  0.718791  0.88530  0.791082  0.808925  0.784336  0.068320   \n",
       "\n",
       "            dTh  \n",
       "count  5.000000  \n",
       "mean   0.000028  \n",
       "std    0.000063  \n",
       "min    0.000000  \n",
       "25%    0.000000  \n",
       "50%    0.000000  \n",
       "75%    0.000000  \n",
       "max    0.000142  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 53.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_scores = cross_validate(train, n_splits=5, n_rows=100000)\n",
    "display(df_scores.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/test_public_v2.csv.gz', na_values=['-9999.0', '255'], usecols=TESTSET_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fit_predict_save(get_head_w_proportions(train, 1000000, None), test, \"out/06_x_dx_ex_edx_mom_hit_phit_da_mse_nerr_orig_1000k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fit_predict_save(get_head_w_proportions(train, 1300000, .3), test, \"out/06_x_dx_ex_edx_mom_hit_phit_da_mse_nerr_30_1300k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fit_predict_save(get_head_w_proportions(train, 1700000, .5), test, \"out/06_x_dx_ex_edx_mom_hit_phit_da_mse_nerr_50_1700k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fit_predict_save(get_samples_w_proptions(train, 100000, .5), test, \"out/04_prop_80_20_100_800.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fit_predict_save(get_samples(train, 100000), test, \"out/03_baseline_head_100.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fit_save_model(get_head_w_proportions(train, 100000, .5), \"models/06_x_dx_ex_edx_mom_hit_da_mse_50_100k.xgb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame([[1.0, np.NaN], [np.NaN, np.NaN], [2.0, 3.1]], columns=['a', 'b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_backup = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\tf\\lib\\site-packages\\numpy\\lib\\arraysetops.py:522: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = pd.read_csv('data/train_part_1_v2.csv.gz', na_values=['-9999.0', '255'], index_col=utils.ID_COLUMN)\n",
    "# train = pd.read_csv('data/train_part_2_v2.csv.gz', nrows=10000, na_values=['-9999.0', '255'])\n",
    "# train = pd.concat([train_0, train_1], axis=0, ignore_index=True)\n",
    "\n",
    "label0 = train.loc[train.label==0, :]\n",
    "label0.to_csv('data/train_pub_L0_p1.csv.gz', compression='gzip')\n",
    "\n",
    "label1 = train.loc[train.label==1, :]\n",
    "label1.to_csv('data/train_pub_L1_p1.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "label0.to_pickle('data/train_pub_L0_p1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 48.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "label1.to_pickle('data/train_pub_L1_p1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((210403, 79), (2512449, 79))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label0.shape, label1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train = pd.read_csv('data/train_part_2_v2.csv.gz', na_values=['-9999.0', '255'], index_col=utils.ID_COLUMN)\n",
    "# train = pd.read_csv('data/train_part_2_v2.csv.gz', nrows=10000, na_values=['-9999.0', '255'])\n",
    "# train = pd.concat([train_0, train_1], axis=0, ignore_index=True)\n",
    "\n",
    "label0 = train.loc[train.label==0, :]\n",
    "label0.to_csv('data/train_pub_L0_p2.csv.gz', compression='gzip')\n",
    "\n",
    "label1 = train.loc[train.label==1, :]\n",
    "label1.to_csv('data/train_pub_L1_p2.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(736, 79) (736, 79)\n",
      "Wall time: 702 ms\n"
     ]
    }
   ],
   "source": [
    "# label0_ = pd.read_csv('data/train_pub_L0_p1.csv', nrows=10000, na_values=['-9999.0', '255'], index_col=utils.ID_COLUMN)\n",
    "\n",
    "# print(label0.shape, label0_.shape)\n",
    "\n",
    "# for c in label0.columns:\n",
    "#     lc0 = label0.loc[:, c]\n",
    "#     if not np.issubdtype(lc0.dtype, np.number):\n",
    "#         continue\n",
    "#     lc0_ = label0_.loc[:, c]\n",
    "#     mask = (lc0 - lc0_).abs() > 1e-10\n",
    "#     if mask.sum() > 0 :\n",
    "#         display(lc0[mask] - lc0_[mask])\n",
    "        \n",
    "\n",
    "# for c in label0.columns:\n",
    "#     lc0 = label0.loc[:, c]\n",
    "#     if np.issubdtype(lc0.dtype, np.number):\n",
    "#         continue\n",
    "#     lc0_ = label0_.loc[:, c]\n",
    "#     mask = (lc0 != lc0_) & (lc0 == lc0) & (lc0_ == lc0_)\n",
    "#     for i in range(mask.sum()):\n",
    "#         s0 = ''.join(lc0.iloc[mask].iloc[i].split())\n",
    "#         s1 = ''.join(lc0_.iloc[mask].iloc[i].split())\n",
    "#         if s0 == s1:\n",
    "#             continue\n",
    "        \n",
    "#         print(lc0.iloc[0])\n",
    "#         print(lc0_.iloc[0])\n",
    "#         for i,s in enumerate(difflib.ndiff(lc0.iloc[0], lc0_.iloc[0])):\n",
    "#             if s[0]==' ': continue\n",
    "#             elif s[0]=='-':\n",
    "#                 print(u'Delete \"{}\" `{}` from position {}'.format(s[-1], ord(s[-1]),i))\n",
    "#             elif s[0]=='+':\n",
    "#                 print(u'Add \"{}\" `{}` to position {}'.format(s[-1],ord(s[-1]),i))    \n",
    "#         print()  \n",
    "\n",
    "# display(label0[mask].head(10))\n",
    "# display(label0_[mask].head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
