{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABUkAAABECAYAAAC8urRAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAA4JJREFUeJzt2zFLVWEcx/H/DYmgSYoCc7Bw0KDFcyVoCaLNNxAS1FBRSw0NTYEUBA0NtRTVEoT0BtoqaAlCj0uQDlIOt6AonAKJ4PYG1Kmnp+7/8xnPs/wOHM7whafT7/cDAAAAACCrHbUHAAAAAADUJJICAAAAAKmJpAAAAABAaiIpAAAAAJCaSAoAAAAApDa03WHbtv2/NQQAAAAAoKSmaTqbPd82kkZEzM33/vyaf8Tc7GjcXn1ce0Yx18bPxcaNW7VnFPFy/GzMzI7E8/nPtacUMTM7EvH9We0Z5ew5FQ++bPpPGggX9/fj8NMXtWcU8/70yVhfOF97RjHD04/iZm8w3+/66KO4tLZSe0Yx98cm4sryWO0ZxdydXIsDDydqzyjm04WV6F5tas8oZvFOG0cPXq49o4i3H+/FaPd47RnF9BZfx6sz+2rPKObEk6/RbHRrzyim3bUY3d0D/G/50cabn4P5fsd2trEw/qH2jGKmVw9F826p9oxi2iNTMbQ+mN9mRMSv4TaaqeXaM4pplyajt3ek9owiRr9t3ZFctwcAAAAAUhNJAQAAAIDURFIAAAAAIDWRFAAAAABITSQFAAAAAFITSQEAAACA1ERSAAAAACA1kRQAAAAASE0kBQAAAABSE0kBAAAAgNREUgAAAAAgNZEUAAAAAEhNJAUAAAAAUhNJAQAAAIDURFIAAAAAIDWRFAAAAABITSQFAAAAAFITSQEAAACA1ERSAAAAACA1kRQAAAAASE0kBQAAAABSE0kBAAAAgNREUgAAAAAgNZEUAAAAAEhNJAUAAAAAUhNJAQAAAIDURFIAAAAAIDWRFAAAAABITSQFAAAAAFITSQEAAACA1ERSAAAAACA1kRQAAAAASE0kBQAAAABSE0kBAAAAgNREUgAAAAAgNZEUAAAAAEhNJAUAAAAAUhNJAQAAAIDURFIAAAAAIDWRFAAAAABITSQFAAAAAFITSQEAAACA1ERSAAAAACA1kRQAAAAASE0kBQAAAABSE0kBAAAAgNREUgAAAAAgNZEUAAAAAEhNJAUAAAAAUhNJAQAAAIDURFIAAAAAIDWRFAAAAABITSQFAAAAAFLr9Pv9LQ/btt36EAAAAADgP9I0TWez59tGUgAAAACAQee6PQAAAACQmkgKAAAAAKQmkgIAAAAAqYmkAAAAAEBqIikAAAAAkNpvuTVZgasyZKkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1728x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as sts\n",
    "import seaborn as sns\n",
    "import scoring\n",
    "from IPython.display import display\n",
    "\n",
    "sns.set()\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "color_palette = sns.color_palette('deep') + sns.color_palette('husl', 6) + sns.color_palette('bright') + sns.color_palette('pastel')\n",
    "\n",
    "%matplotlib inline\n",
    "sns.palplot(color_palette)\n",
    "\n",
    "def ndprint(a, precision=3):\n",
    "    with np.printoptions(precision=precision, suppress=True):\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STATIONS = 4\n",
    "\n",
    "# train cols\n",
    "unused_train_cols = ['particle_type', 'kinWeight', 'sWeight']\n",
    "train_cols = ['label', 'weight']\n",
    "ALL_TRAIN_COLS = train_cols + unused_train_cols\n",
    "\n",
    "# original cols\n",
    "x_cols = ['MatchedHit_X[%i]' % i for i in range(N_STATIONS)]\n",
    "y_cols = ['MatchedHit_Y[%i]' % i for i in range(N_STATIONS)]\n",
    "z_cols = ['MatchedHit_Z[%i]' % i for i in range(N_STATIONS)]\n",
    "xy_cols = x_cols + y_cols\n",
    "xyz_cols = x_cols + y_cols + z_cols\n",
    "dx_cols = ['MatchedHit_DX[%i]' % i for i in range(N_STATIONS)]\n",
    "dy_cols = ['MatchedHit_DY[%i]' % i for i in range(N_STATIONS)]\n",
    "dz_cols = ['MatchedHit_DZ[%i]' % i for i in range(N_STATIONS)]\n",
    "dxyz_cols = dx_cols + dy_cols + dz_cols\n",
    "\n",
    "ex_cols = ['Lextra_X[%i]' % i for i in range(N_STATIONS)]\n",
    "ey_cols = ['Lextra_Y[%i]' % i for i in range(N_STATIONS)]\n",
    "exy_cols = ex_cols + ey_cols\n",
    "edx_cols = ['Mextra_DX2[%i]' % i for i in range(N_STATIONS)]\n",
    "edy_cols = ['Mextra_DY2[%i]' % i for i in range(N_STATIONS)]\n",
    "edxy_cols = edx_cols + edy_cols\n",
    "\n",
    "t_cols = ['MatchedHit_T[%i]' % i for i in range(N_STATIONS)]\n",
    "dt_cols = ['MatchedHit_DT[%i]' % i for i in range(N_STATIONS)]\n",
    "\n",
    "hit_type_cols = ['MatchedHit_TYPE[%i]' % i for i in range(N_STATIONS)]\n",
    "mom_cols = ['P', 'PT']\n",
    "hit_stats_cols = ['FOI_hits_N', 'NShared', 'ndof']\n",
    "\n",
    "ncl_cols = ['ncl[%i]' % i for i in range(N_STATIONS)]\n",
    "avg_cs_cols = ['avg_cs[%i]' % i for i in range(N_STATIONS)]\n",
    "\n",
    "# foi cols\n",
    "foi_xyz_cols = [\"FOI_hits_X\", \"FOI_hits_Y\", \"FOI_hits_Z\"]\n",
    "foi_dxyz_cols = [\"FOI_hits_DX\", \"FOI_hits_DY\", \"FOI_hits_DZ\"]\n",
    "foi_ts_cols = [\"FOI_hits_T\", \"FOI_hits_DT\", \"FOI_hits_S\"]\n",
    "foi_cols = foi_xyz_cols + foi_dxyz_cols + foi_ts_cols\n",
    "\n",
    "# derivative cols\n",
    "pca_x_cols = ['PCA_X[%i]' % i for i in range(N_STATIONS)]\n",
    "pca_y_cols = ['PCA_Y[%i]' % i for i in range(N_STATIONS)]\n",
    "pca_z_cols = ['PCA_Z[%i]' % i for i in range(N_STATIONS)]\n",
    "pca_xyz_cols = pca_x_cols + pca_y_cols + pca_z_cols\n",
    "\n",
    "nerr_x_cols = ['NErr_X[%i]' % i for i in range(N_STATIONS)]\n",
    "nerr_y_cols = ['NErr_Y[%i]' % i for i in range(N_STATIONS)]\n",
    "nerr_xy_cols = nerr_x_cols + nerr_y_cols\n",
    "\n",
    "da_cols = ['DAngle[%d]' % i for i in range(1, 4)]\n",
    "is_muon_cols = ['IsMuonTight']\n",
    "prob_hit_detector_cols = ['ProbHit[%i]' % i for i in range(N_STATIONS)]\n",
    "err_cols = ['ErrMSE', 'Chi2Quantile']\n",
    "\n",
    "SIMPLE_FEATURE_COLS = xyz_cols + dxyz_cols + exy_cols + edxy_cols + t_cols + dt_cols + hit_type_cols + mom_cols + hit_stats_cols + ncl_cols + avg_cs_cols\n",
    "ARR_FEATURE_COLS = foi_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(SIMPLE_FEATURE_COLS + ARR_FEATURE_COLS + ALL_TRAIN_COLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert csv to pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetMetaData:\n",
    "    def __init__(self, origin_csv_filenames, chunk_filenames_pattern, origin_col_set):\n",
    "        self.origin_csv_filenames = origin_csv_filenames\n",
    "        self.chunk_filenames_pattern = chunk_filenames_pattern\n",
    "        self.origin_col_set = origin_col_set\n",
    "        self.is_test = 'test_' in chunk_filenames_pattern\n",
    "\n",
    "meta_train = DatasetMetaData(\n",
    "    origin_csv_filenames=['data/train_part_1_v2.csv.gz', 'data/train_part_2_v2.csv.gz'],\n",
    "    chunk_filenames_pattern='data/train_{label}_{group}_{ind:03d}.pkl',\n",
    "    origin_col_set=SIMPLE_FEATURE_COLS + ARR_FEATURE_COLS + ALL_TRAIN_COLS\n",
    ")\n",
    "meta_pub_test = DatasetMetaData(\n",
    "    origin_csv_filenames=['data/test_public_v2.csv.gz'],\n",
    "    chunk_filenames_pattern='data/test_pub_{group}_{ind:03d}.pkl',\n",
    "    origin_col_set=SIMPLE_FEATURE_COLS + ARR_FEATURE_COLS\n",
    ")\n",
    "meta_pvt_test = DatasetMetaData(\n",
    "    origin_csv_filenames=[],\n",
    "    chunk_filenames_pattern='data/test_pvt_{group}_{ind:03d}.pkl',\n",
    "    origin_col_set=SIMPLE_FEATURE_COLS + ARR_FEATURE_COLS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import repeat\n",
    "\n",
    "class CsvDataReader:\n",
    "    def __init__(self):\n",
    "        self.na_values = ['-9999.0', '255']\n",
    "        self.int_dtype = np.int32\n",
    "        self.float_dtype = np.float32\n",
    "    \n",
    "    def get_data(self, filenames, usecols, chunk_size=25000):        \n",
    "        for filename in filenames:\n",
    "            data_generator = pd.read_csv(\n",
    "                filename, usecols=usecols, chunksize=chunk_size, #nrows=400000,\n",
    "                na_values=self.na_values, na_filter=False, \n",
    "                converters=self._get_converters(), dtype=self._get_types()\n",
    "            )\n",
    "            for data in data_generator:\n",
    "                yield data\n",
    "\n",
    "    def _get_converters(self):\n",
    "        def parse_float_array(line):\n",
    "            return np.fromstring(line[1:-1], sep=\" \", dtype=self.float_dtype)\n",
    "\n",
    "        def parse_int_array(line):\n",
    "            return np.fromstring(line[1:-1], sep=\" \", dtype=self.int_dtype)\n",
    "    \n",
    "        converters = dict(zip(ARR_FEATURE_COLS, repeat(parse_float_array)))\n",
    "        converters[foi_ts_cols[-1]] = parse_int_array\n",
    "        return\n",
    "    \n",
    "    def _get_types(self):\n",
    "        types = dict(zip(SIMPLE_FEATURE_COLS + ALL_TRAIN_COLS, repeat(self.float_dtype)))\n",
    "        for col in unused_train_cols[:1] + train_cols[:1] + hit_stats_cols + ncl_cols + hit_type_cols:\n",
    "            types[col] = self.int_dtype\n",
    "        return types;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBuffer:\n",
    "    def __init__(self):\n",
    "        self._frames = []\n",
    "        self._chunk_counter = 0\n",
    "    \n",
    "    def slice_and_append(self, frame):\n",
    "        self._frames.append(frame)\n",
    "    \n",
    "    def store_chunk(self, chunk_size, filename_pattern):\n",
    "        chunk, chunk_ind = self._cut_chunk(chunk_size)\n",
    "        for group_key, col_group in self._col_groups:\n",
    "            filename = self._generate_chunk_filename(filename_pattern, group_key, chunk_ind)\n",
    "            chunk.loc[:, col_group].to_pickle(filename)\n",
    "    \n",
    "    def _cut_chunk(self, chunk_size):\n",
    "        data = pd.concat(self._frames, axis=0, ignore_index=True) if len(self._frames) > 1 else self._frames[0]\n",
    "        chunk = data.iloc[:chunk_size, :]\n",
    "        tail = data.iloc[chunk_size:, :]\n",
    "        chunk_ind = self._chunk_counter\n",
    "        \n",
    "        self._frames = [tail]\n",
    "        self._chunk_counter += 1\n",
    "        return chunk, chunk_ind\n",
    "    \n",
    "    @property\n",
    "    def nrows(self):\n",
    "        return sum([len(frame.index) for frame in self._frames])\n",
    "    \n",
    "    @property\n",
    "    def _col_groups(self):\n",
    "        return col_groups[:-1]\n",
    "    \n",
    "    def _generate_chunk_filename(self, filename_pattern, group_key, chunk_ind):\n",
    "        return filename_pattern.format(group=group_key, ind=chunk_ind)\n",
    "    \n",
    "class ClassDataBuffer(DataBuffer):\n",
    "    def __init__(self, label, label_key):\n",
    "        super(ClassDataBuffer, self).__init__()\n",
    "        self._label = label\n",
    "        self._label_key = label_key\n",
    "        \n",
    "    def slice_and_append(self, frame):\n",
    "        self._frames.append(frame.loc[frame.label == self._label, :])\n",
    "    \n",
    "    @property\n",
    "    def _col_groups(self):\n",
    "        return col_groups\n",
    "    \n",
    "    def _generate_chunk_filename(self, filename_pattern, group_key, chunk_ind):\n",
    "        return filename_pattern.format(label=self._label_key, group=group_key, ind=chunk_ind)\n",
    "    \n",
    "class PickleDataWriter:\n",
    "    def __init__(self, filename_pattern, chunk_size=50000):\n",
    "        self._filename_pattern = filename_pattern\n",
    "        self._chunk_size = chunk_size\n",
    "        self._is_test = 'test' in filename_pattern\n",
    "        self._stored = 0\n",
    "    \n",
    "    def store_chunkified(self, dataframes_stream):\n",
    "        buffers = [DataBuffer()] if self._is_test else [ClassDataBuffer(i, label_prefixes[i]) for i in range(2)]\n",
    "        for data in dataframes_stream:\n",
    "            for buffer in buffers:\n",
    "                buffer.slice_and_append(data)\n",
    "                self._try_offload_chunks(buffer)\n",
    "\n",
    "        for buffer in buffers:\n",
    "            self._try_offload_chunks(buffer, force=True)\n",
    "\n",
    "    def _try_offload_chunks(self, buffer, force=False):\n",
    "        while buffer.nrows > self._chunk_size or (force and buffer.nrows > 0):\n",
    "            buffer.store_chunk(self._chunk_size, self._filename_pattern)    \n",
    "            self._inc_stored()\n",
    "            \n",
    "    def _inc_stored(self):\n",
    "        self._stored += self._chunk_size\n",
    "        if self._stored % 200000 == 0:\n",
    "            print('Stored: {0}M'.format(self._stored / 1000000.))\n",
    "        \n",
    "col_groups = list(zip(['sf', 'af', 'tr'], [SIMPLE_FEATURE_COLS, ARR_FEATURE_COLS, ALL_TRAIN_COLS]))\n",
    "label_prefixes = ['L0', 'L1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def convert_dataset(data_set_meta: DatasetMetaData):\n",
    "    dataframes_stream = CsvDataReader().get_data(data_set_meta.origin_csv_filenames, data_set_meta.origin_col_set)\n",
    "    PickleDataWriter(data_set_meta.chunk_filenames_pattern).store_chunkified(dataframes_stream)\n",
    "    \n",
    "convert_dataset(meta_pvt_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read from pickle chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBuffer:\n",
    "    def __init__(self):\n",
    "        self._frames = []\n",
    "    \n",
    "    def append(self, frame):\n",
    "        self._frames.append(frame)\n",
    "    \n",
    "    def cut(self, nrows):\n",
    "        nrows = min(nrows, self.nrows)\n",
    "        merged = self._merge_frames()\n",
    "        head = merged.iloc[:nrows, :]\n",
    "        tail = merged.iloc[nrows:, :]\n",
    "        \n",
    "        self._frames = [tail]\n",
    "        return head, nrows\n",
    "    \n",
    "    def _merge_frames(self):\n",
    "        if len(self._frames) > 1:\n",
    "            merged = pd.concat(self._frames, axis=0, ignore_index=True)\n",
    "            self._frames = [merged]\n",
    "        return self._frames[0]\n",
    "    \n",
    "    @property\n",
    "    def nrows(self):\n",
    "        return sum([len(frame.index) for frame in self._frames])\n",
    "    \n",
    "    @property\n",
    "    def is_empty(self):\n",
    "        return self.nrows == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTank:\n",
    "    def __init__(self, max_volume, callback_on_full):\n",
    "        self._max_volume = max_volume\n",
    "        self._buffer = DataBuffer()\n",
    "        self._on_full = callback_on_full\n",
    "    \n",
    "    def add(self, frame):\n",
    "        self._buffer.append(frame)\n",
    "        flushed = 0\n",
    "        while self._is_full():\n",
    "            flushed += self.flush(self)\n",
    "        return flushed\n",
    "        \n",
    "    def flush(self):\n",
    "        if self._buffer.is_empty:\n",
    "            return 0\n",
    "        flushed_data, flushed_vol = self._buffer.cut(self._max_volume)\n",
    "        self._on_full(flushed_data)\n",
    "        return flushed_vol\n",
    "    \n",
    "    def _is_full(self):\n",
    "        return self._buffer.nrows >= self._max_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDatasetHelper:\n",
    "    def __init__(self, filename_pattern):\n",
    "        self._filename_pattern = filename_pattern\n",
    "\n",
    "    def filter_frame(frame):\n",
    "        return frame\n",
    "        \n",
    "    def get_col_groups(self):\n",
    "        return col_groups[:-1]\n",
    "        \n",
    "    def generate_chunk_filename(self, group_key, chunk_ind):\n",
    "        return self._filename_pattern.format(group=group_key, ind=chunk_ind)\n",
    "    \n",
    "\n",
    "class TrainDatasetHelper:\n",
    "    def __init__(self, filename_pattern, label, label_key):\n",
    "        self._filename_pattern = filename_pattern\n",
    "        self._label = label\n",
    "        self._label_key = label_key\n",
    "        \n",
    "    def filter_frame(frame):\n",
    "        return frame.loc[frame.label == self._label, :]\n",
    "        \n",
    "    def get_col_groups(self):\n",
    "        return col_groups[:-1]\n",
    "        \n",
    "    def generate_chunk_filename(self, group_key, chunk_ind):\n",
    "        return self._filename_pattern.format(label=self._label_key, group=group_key, ind=chunk_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PickleDataWriter:\n",
    "    def __init__(self, filename_pattern, helper, chunk_size):\n",
    "        self._filename_pattern = filename_pattern\n",
    "        self._helper = helper\n",
    "        self._data_tank = DataTank(max_volume=chunk_size, callback_on_full=self._flush_chunk)\n",
    "        self._chunk_index = 0\n",
    "    \n",
    "    def store(self, frame):\n",
    "        filtered_frame = self._helper.filter_frame(frame)\n",
    "        return self._data_tank.add(filtered_frame)\n",
    "        \n",
    "    def flush(self):\n",
    "        return self._data_tank.flush()\n",
    "    \n",
    "    def _flush_chunk(self, chunk):\n",
    "        self._store_chunk(chunk, self._chunk_index)\n",
    "        self._chunk_index += 1\n",
    "        \n",
    "    def _store_chunk(self, chunk, chunk_index):\n",
    "        for group_key, col_group in self._helper.get_col_groups():\n",
    "            filename = self._helper.generate_chunk_filename(group_key, chunk_index)\n",
    "            chunk.loc[:, col_group].to_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-59-af170f6c1043>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-59-af170f6c1043>\"\u001b[1;36m, line \u001b[1;32m13\u001b[0m\n\u001b[1;33m    return self._flush_chunk() if not self._buffer.is_empty\u001b[0m\n\u001b[1;37m                                                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "\n",
    "class PickleDataReader:\n",
    "    def __init__(self, filename_pattern, helper):\n",
    "        self._filename_pattern = filename_pattern\n",
    "        self._helper = helper\n",
    "        self._result = None\n",
    "        \n",
    "    def read(self, nrows, cols):\n",
    "        data_tank = DataTank(nrows, self._set_read_result)\n",
    "        for frame in self._read_chunks(cols):\n",
    "            if data_tank.add(frame) > 0:\n",
    "                return self._result\n",
    "\n",
    "        data_tank.flush()\n",
    "        return self._result\n",
    "        \n",
    "    def _read_chunks(self, cols):\n",
    "        chunk_index = 0\n",
    "        while True:\n",
    "            frame = self._read_chunk(chunk_index, cols)\n",
    "            if frame is None:\n",
    "                break\n",
    "            \n",
    "            yield frame\n",
    "            chunk_index += 1\n",
    "            \n",
    "    def _set_read_result(self, data):\n",
    "        self._result = data\n",
    "    \n",
    "    def _read_chunk(self, chunk_index, cols):\n",
    "        chunk_parts = []\n",
    "        for group_key, col_group in self._helper.get_col_groups():\n",
    "            cols_ = list(set(col_group) & set(cols))\n",
    "            if not cols_:\n",
    "                continue\n",
    "            \n",
    "            filename = self._helper.generate_chunk_filename(group_key, chunk_index)\n",
    "            if not os.path.exists(filename):\n",
    "                return None\n",
    "            \n",
    "            chunk_part = pd.read_pickle(filename).loc[:, cols_]\n",
    "            chunk_parts.append(chunk_part)\n",
    "            \n",
    "        return pd.concat(chunk_parts, axis=0)\n",
    "    \n",
    "class PickleDataContainer:\n",
    "    def __init__(self, filename_pattern, chunk_size=50000):\n",
    "        self._filename_pattern = filename_pattern\n",
    "        self._chunk_size = chunk_size\n",
    "        self._is_test = 'test' in filename_pattern\n",
    "        self._stored = 0\n",
    "    \n",
    "    def store_chunkified(self, dataframes_stream):\n",
    "        containers = [PickleDataContainer()] if self._is_test else [ClassDataBuffer(i, label_prefixes[i]) for i in range(2)]\n",
    "        for data in dataframes_stream:\n",
    "            for buffer in buffers:\n",
    "                buffer.slice_and_append(data)\n",
    "                self._try_offload_chunks(buffer)\n",
    "\n",
    "        for buffer in buffers:\n",
    "            self._try_offload_chunks(buffer, force=True)\n",
    "\n",
    "    def _try_offload_chunks(self, buffer, force=False):\n",
    "        while buffer.nrows > self._chunk_size or (force and buffer.nrows > 0):\n",
    "            buffer.store_chunk(self._chunk_size, self._filename_pattern)    \n",
    "            self._inc_stored()\n",
    "            \n",
    "    def _inc_stored(self):\n",
    "        self._stored += self._chunk_size\n",
    "        if self._stored % 200000 == 0:\n",
    "            print('Stored: {0}M'.format(self._stored / 1000000.))\n",
    "    \n",
    "# def read_dataset(data_set_meta: DatasetMetaData, nrows, cols, class_proportions):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-59-af170f6c1043>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-59-af170f6c1043>\"\u001b[1;36m, line \u001b[1;32m13\u001b[0m\n\u001b[1;33m    return self._flush_chunk() if not self._buffer.is_empty\u001b[0m\n\u001b[1;37m                                                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class PickleDataReader:\n",
    "    def __init__(self, filename_pattern):\n",
    "        self._filename_pattern = filename_pattern\n",
    "        self._buffer = DataBuffer()\n",
    "        self._chunk_index = 0\n",
    "    \n",
    "    def read(self, nrows):\n",
    "        while not self._contains(nrows):\n",
    "            frame = self._read_chunk(self._chunk_index)\n",
    "            if frame is None:\n",
    "                break\n",
    "\n",
    "            self._buffer.append(frame)\n",
    "            self._chunk_index += 1\n",
    "        \n",
    "        return self._buffer.cut(nrows) if not self._buffer.is_empty\n",
    "    \n",
    "    def _try_offload_chunks(self):\n",
    "        offloaded = 0\n",
    "        while self._contains(self._chunk_size):\n",
    "            chunk = self._buffer.cut(self._chunk_size)\n",
    "            self._store_chunk(chunk, self._chunk_index)\n",
    "            \n",
    "            self._chunk_index += 1\n",
    "            offloaded += self._chunk_size\n",
    "        return offloaded\n",
    "            \n",
    "    def _contains(self, nrows):\n",
    "        return self._buffer.nrows >= nrows\n",
    "    \n",
    "    def _store_chunk(self, chunk, chunk_index):\n",
    "        for group_key, col_group in self._col_groups:\n",
    "            filename = self._generate_chunk_filename(group_key, chunk_index)\n",
    "            chunk.loc[:, col_group].to_pickle(filename)\n",
    "    \n",
    "    @property\n",
    "    def _col_groups(self):\n",
    "        return col_groups[:-1]\n",
    "    \n",
    "    def _generate_chunk_filename(self, group_key, chunk_ind):\n",
    "        return self._filename_pattern.format(group=group_key, ind=chunk_ind)\n",
    "\n",
    "class ClassDataBuffer(DataBuffer):\n",
    "    def __init__(self, label, label_key):\n",
    "        super(ClassDataBuffer, self).__init__()\n",
    "        self._label = label\n",
    "        self._label_key = label_key\n",
    "        \n",
    "    def slice_and_append(self, frame):\n",
    "        self._frames.append(frame.loc[frame.label == self._label, :])\n",
    "    \n",
    "    @property\n",
    "    def _col_groups(self):\n",
    "        return col_groups\n",
    "    \n",
    "    def _generate_chunk_filename(self, filename_pattern, group_key, chunk_ind):\n",
    "        return filename_pattern.format(label=self._label_key, group=group_key, ind=chunk_ind)\n",
    "    \n",
    "class PickleDataContainer:\n",
    "    def __init__(self, filename_pattern, chunk_size=50000):\n",
    "        self._filename_pattern = filename_pattern\n",
    "        self._chunk_size = chunk_size\n",
    "        self._is_test = 'test' in filename_pattern\n",
    "        self._stored = 0\n",
    "    \n",
    "    def store_chunkified(self, dataframes_stream):\n",
    "        containers = [PickleDataContainer()] if self._is_test else [ClassDataBuffer(i, label_prefixes[i]) for i in range(2)]\n",
    "        for data in dataframes_stream:\n",
    "            for buffer in buffers:\n",
    "                buffer.slice_and_append(data)\n",
    "                self._try_offload_chunks(buffer)\n",
    "\n",
    "        for buffer in buffers:\n",
    "            self._try_offload_chunks(buffer, force=True)\n",
    "\n",
    "    def _try_offload_chunks(self, buffer, force=False):\n",
    "        while buffer.nrows > self._chunk_size or (force and buffer.nrows > 0):\n",
    "            buffer.store_chunk(self._chunk_size, self._filename_pattern)    \n",
    "            self._inc_stored()\n",
    "            \n",
    "    def _inc_stored(self):\n",
    "        self._stored += self._chunk_size\n",
    "        if self._stored % 200000 == 0:\n",
    "            print('Stored: {0}M'.format(self._stored / 1000000.))\n",
    "    \n",
    "# def read_dataset(data_set_meta: DatasetMetaData, nrows, cols, class_proportions):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-59-af170f6c1043>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-59-af170f6c1043>\"\u001b[1;36m, line \u001b[1;32m13\u001b[0m\n\u001b[1;33m    return self._flush_chunk() if not self._buffer.is_empty\u001b[0m\n\u001b[1;37m                                                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class PickleDataWriter:\n",
    "    def __init__(self, chunk_size, filename_pattern):\n",
    "        self._chunk_size = chunk_size\n",
    "        self._filename_pattern = filename_pattern\n",
    "        self._buffer = DataBuffer()\n",
    "        self._chunk_index = 0\n",
    "    \n",
    "    def store(self, frame):\n",
    "        self._buffer.append(frame)\n",
    "        return self._flush_chunks()\n",
    "        \n",
    "    def flush(self):\n",
    "        return self._flush_chunk() if not self._buffer.is_empty\n",
    "    \n",
    "    def _flush_chunks(self, force=False):\n",
    "        flushed_nrows = 0\n",
    "        while self._contains(self._chunk_size):\n",
    "            flushed_nrows += self._flush_chunk()\n",
    "        return flushed_nrows\n",
    "    \n",
    "    def _flush_chunk(self):\n",
    "        chunk = self._buffer.cut(self._chunk_size)\n",
    "        self._store_chunk(chunk, self._chunk_index)\n",
    "        self._chunk_index += 1\n",
    "        return len(chunk.index)\n",
    "        \n",
    "    def _store_chunk(self, chunk, chunk_index):\n",
    "        for group_key, col_group in self._col_groups:\n",
    "            filename = self._generate_chunk_filename(group_key, chunk_index)\n",
    "            chunk.loc[:, col_group].to_pickle(filename)\n",
    "    \n",
    "    def _contains(self, nrows):\n",
    "        return self._buffer.nrows >= nrows\n",
    "    \n",
    "    @property\n",
    "    def _col_groups(self):\n",
    "        return col_groups[:-1]\n",
    "    \n",
    "    def _generate_chunk_filename(self, group_key, chunk_ind):\n",
    "        return self._filename_pattern.format(group=group_key, ind=chunk_ind)\n",
    "    \n",
    "class PickleDataReader:\n",
    "    def __init__(self, filename_pattern):\n",
    "        self._filename_pattern = filename_pattern\n",
    "        self._buffer = DataBuffer()\n",
    "        self._chunk_index = 0\n",
    "    \n",
    "    def read(self, nrows):\n",
    "        while not self._contains(nrows):\n",
    "            frame = self._read_chunk(self._chunk_index)\n",
    "            if frame is None:\n",
    "                break\n",
    "\n",
    "            self._buffer.append(frame)\n",
    "            self._chunk_index += 1\n",
    "        \n",
    "        return self._buffer.cut(nrows) if not self._buffer.is_empty\n",
    "    \n",
    "    def _try_offload_chunks(self):\n",
    "        offloaded = 0\n",
    "        while self._contains(self._chunk_size):\n",
    "            chunk = self._buffer.cut(self._chunk_size)\n",
    "            self._store_chunk(chunk, self._chunk_index)\n",
    "            \n",
    "            self._chunk_index += 1\n",
    "            offloaded += self._chunk_size\n",
    "        return offloaded\n",
    "            \n",
    "    def _contains(self, nrows):\n",
    "        return self._buffer.nrows >= nrows\n",
    "    \n",
    "    def _store_chunk(self, chunk, chunk_index):\n",
    "        for group_key, col_group in self._col_groups:\n",
    "            filename = self._generate_chunk_filename(group_key, chunk_index)\n",
    "            chunk.loc[:, col_group].to_pickle(filename)\n",
    "    \n",
    "    @property\n",
    "    def _col_groups(self):\n",
    "        return col_groups[:-1]\n",
    "    \n",
    "    def _generate_chunk_filename(self, group_key, chunk_ind):\n",
    "        return self._filename_pattern.format(group=group_key, ind=chunk_ind)\n",
    "\n",
    "class ClassDataBuffer(DataBuffer):\n",
    "    def __init__(self, label, label_key):\n",
    "        super(ClassDataBuffer, self).__init__()\n",
    "        self._label = label\n",
    "        self._label_key = label_key\n",
    "        \n",
    "    def slice_and_append(self, frame):\n",
    "        self._frames.append(frame.loc[frame.label == self._label, :])\n",
    "    \n",
    "    @property\n",
    "    def _col_groups(self):\n",
    "        return col_groups\n",
    "    \n",
    "    def _generate_chunk_filename(self, filename_pattern, group_key, chunk_ind):\n",
    "        return filename_pattern.format(label=self._label_key, group=group_key, ind=chunk_ind)\n",
    "    \n",
    "class PickleDataContainer:\n",
    "    def __init__(self, filename_pattern, chunk_size=50000):\n",
    "        self._filename_pattern = filename_pattern\n",
    "        self._chunk_size = chunk_size\n",
    "        self._is_test = 'test' in filename_pattern\n",
    "        self._stored = 0\n",
    "    \n",
    "    def store_chunkified(self, dataframes_stream):\n",
    "        containers = [PickleDataContainer()] if self._is_test else [ClassDataBuffer(i, label_prefixes[i]) for i in range(2)]\n",
    "        for data in dataframes_stream:\n",
    "            for buffer in buffers:\n",
    "                buffer.slice_and_append(data)\n",
    "                self._try_offload_chunks(buffer)\n",
    "\n",
    "        for buffer in buffers:\n",
    "            self._try_offload_chunks(buffer, force=True)\n",
    "\n",
    "    def _try_offload_chunks(self, buffer, force=False):\n",
    "        while buffer.nrows > self._chunk_size or (force and buffer.nrows > 0):\n",
    "            buffer.store_chunk(self._chunk_size, self._filename_pattern)    \n",
    "            self._inc_stored()\n",
    "            \n",
    "    def _inc_stored(self):\n",
    "        self._stored += self._chunk_size\n",
    "        if self._stored % 200000 == 0:\n",
    "            print('Stored: {0}M'.format(self._stored / 1000000.))\n",
    "    \n",
    "# def read_dataset(data_set_meta: DatasetMetaData, nrows, cols, class_proportions):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_train_ds(nrows, cols, class_proportions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 50)\n",
      "Wall time: 13.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_0 = pd.read_csv('data/train_part_1_v2.csv.gz', nrows=200000, na_values=['-9999.0', '255'], usecols=TRAINSET_COLS)\n",
    "train_1 = pd.read_csv('data/train_part_2_v2.csv.gz', nrows=200000, na_values=['-9999.0', '255'], usecols=TRAINSET_COLS)\n",
    "train = pd.concat([train_0, train_1], axis=0, ignore_index=True)\n",
    "\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30588 369412\n"
     ]
    }
   ],
   "source": [
    "def get_class(i):\n",
    "    return train.index[train.label == i]\n",
    "\n",
    "# Gets `n_rows` random samples from `data`\n",
    "def get_samples(data, n_rows):\n",
    "    indices = np.random.randint(len(data), size=n_rows)\n",
    "    indices = np.sort(indices)\n",
    "    return data.iloc[indices]\n",
    "\n",
    "# Gets `n_rows` random samples from `data` with specified class proportions.\n",
    "# If `prop_0` is None, it keeps `data`s natural proportions.\n",
    "def get_samples_w_proptions(data, n_rows, prop_0=None):\n",
    "    def get_class_samples(class_i, n_rows):\n",
    "        class_indices = np.random.randint(len(class_i), size=n_rows)\n",
    "        return class_i[class_indices]\n",
    "    \n",
    "    if prop_0 is None:\n",
    "        prop_0 = len(class_0) / (len(class_0) + len(class_1))\n",
    "    cnt_0 = int(n_rows * prop_0)\n",
    "    class_0_indices = get_class_samples(class_0, cnt_0)\n",
    "    class_1_indices = get_class_samples(class_1, n_rows - cnt_0)\n",
    "    indices = np.concatenate((class_0_indices, class_1_indices))\n",
    "    indices = np.sort(indices)\n",
    "    return data.loc[indices, :]\n",
    "\n",
    "# Gets `n_rows` head samples from `data` with specified class proportions.\n",
    "# If `prop_0` is None, it keeps `data`s natural proportions.\n",
    "# If `data` samples is not enough to fulfil proportions for any `class_i` then random sampling from the `class_i` is applied.\n",
    "def get_head_w_proportions(data, n_rows, prop_0=None):\n",
    "    def get_class_samples_head_smart(class_i, n_rows):\n",
    "        cls_len = len(class_i)\n",
    "        if cls_len > n_rows:\n",
    "            return class_i[:n_rows]\n",
    "        class_indices = np.concatenate((np.arange(cls_len),  np.random.randint(cls_len, size=n_rows-cls_len)))\n",
    "        return class_i[class_indices]\n",
    "    \n",
    "    if prop_0 is None:\n",
    "        prop_0 = len(class_0) / (len(class_0) + len(class_1))\n",
    "    cnt_0 = int(n_rows * prop_0)\n",
    "    class_0_indices = get_class_samples_head_smart(class_0, cnt_0)\n",
    "    class_1_indices = get_class_samples_head_smart(class_1, n_rows - cnt_0)\n",
    "    indices = np.concatenate((class_0_indices, class_1_indices))\n",
    "    indices = np.sort(indices)\n",
    "    return data.loc[indices, :]\n",
    "    \n",
    "print(np.count_nonzero(train.label == 0), np.count_nonzero(train.label == 1))\n",
    "\n",
    "class_0 = get_class(0)\n",
    "class_1 = get_class(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn import model_selection as mdsel\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "def fit(train):\n",
    "    target_train = train[train_cols]\n",
    "    \n",
    "    # defined much later\n",
    "    transformer = DataTransformer().fit(train)\n",
    "    train = transformer.transform(train)\n",
    "    \n",
    "    estimator = xgb.XGBClassifier(n_estimators=60, n_jobs=3)\n",
    "    estimator.fit(train.values, target_train.label.values, sample_weight=target_train.weight.values, eval_metric=scoring.rejection90_sklearn)\n",
    "    return transformer, estimator\n",
    "    \n",
    "def predict(fitted_state, test):\n",
    "    transformer, estimator = fitted_state\n",
    "    \n",
    "    test = transformer.transform(test)\n",
    "    predictions = estimator.predict_proba(test.values)[:, 1]\n",
    "    return predictions\n",
    "\n",
    "def score(fitted_state, test):\n",
    "    target_test = test.loc[:, train_cols]\n",
    "    predictions = predict(fitted_state, test)\n",
    "    return scoring.rejection90(target_test.label.values, predictions, sample_weight=target_test.weight.values)\n",
    "\n",
    "def fit_predict_save(train, test, filename):\n",
    "    fitted_state = fit(train)\n",
    "    predictions = predict(fitted_state, test)\n",
    "    \n",
    "    pd.DataFrame(data={\"prediction\": predictions}, index=test.index).to_csv(\n",
    "        filename, index_label=utils.ID_COLUMN\n",
    "    )\n",
    "    \n",
    "    model = fitted_state[1]\n",
    "    model_filename = filename.replace('out/', 'models/').replace('.csv', '.xgb')\n",
    "    model.save_model(model_filename)\n",
    "    \n",
    "def fit_save_model(train, filename):\n",
    "    if filename.endswith('.csv'):\n",
    "        filename = filename.replace('out/', 'models/').replace('.csv', '.xgb')\n",
    "        \n",
    "    _, model = fit(train)\n",
    "    model.save_model(filename)\n",
    "    \n",
    "def cross_validate(train, n_splits, n_rows):\n",
    "    train = get_head_w_proportions(train, n_rows, .5)\n",
    "    \n",
    "    splitter = mdsel.StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "    scores = []\n",
    "    for train_indices, test_indices in splitter.split(train, train.label):\n",
    "        train_subset = train.iloc[train_indices, :]\n",
    "        test_subset = train.iloc[test_indices, :]\n",
    "        \n",
    "        fit_state = fit(train_subset)\n",
    "        \n",
    "        target_test = test_subset[train_cols]\n",
    "        predictions = predict(fit_state, test_subset)\n",
    "        \n",
    "        y_true = target_test.label.values\n",
    "        l, r, ep = scoring.get_threshold_details(y_true, predictions, sample_weight=target_test.weight.values)\n",
    "        threshold = (l + r) / 2\n",
    "        y_pred = predictions >= threshold\n",
    "                \n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        prec = precision_score(y_true, y_pred)\n",
    "        rec = recall_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        roc_auc = roc_auc_score(y_true, predictions)\n",
    "        scr = scoring.rejection90(y_true, predictions, sample_weight=target_test.weight.values)\n",
    "        scores += [[acc, prec, rec, f1, roc_auc, scr, threshold, r - l]]\n",
    "\n",
    "    return pd.DataFrame(scores, columns=['acc', 'prec', 'rec', 'f1', 'roc_auc', 'scr', 'th', 'dTh'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transformer\n",
    "\n",
    "Это по сути основная часть. Класс, который отбирает нужные столбцы, возможно что-то модифицирует или добавляет. На выходе - входные данные для модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def fill_na(data):\n",
    "    mask = data.isna()\n",
    "    means = data.mean(skipna=True)\n",
    "    data.fillna(means, inplace=True)\n",
    "    return mask\n",
    "\n",
    "def restore_na(data, mask):\n",
    "    data.mask(mask, other=np.NaN, inplace=True)\n",
    "    \n",
    "def get_nth_detector_coords(i):\n",
    "    return [x_cols[i], y_cols[i], z_cols[i]]\n",
    "\n",
    "def get_nth_detector_coords_pca(i):\n",
    "    return [pca_x_cols[i], pca_y_cols[i], pca_z_cols[i]]\n",
    "    \n",
    "def pca_fit(data):\n",
    "    cols = get_nth_detector_coords(0)\n",
    "    data = data[cols].copy()\n",
    "    \n",
    "    fill_na(data)\n",
    "    pca_model = PCA(n_components=3)\n",
    "    pca_model.fit(data)\n",
    "    return pca_model\n",
    "\n",
    "def pca_transform(pca_model, data, features):\n",
    "    for i in range(4):\n",
    "        cols = get_nth_detector_coords(i)\n",
    "        new_cols = get_nth_detector_coords_pca(i)\n",
    "        data_detector = data.loc[:, cols]\n",
    "        \n",
    "        mask = fill_na(data_detector)\n",
    "        transformed_data = pca_model.transform(data_detector.values)\n",
    "        restore_na(data_detector, mask)\n",
    "        return\n",
    "        \n",
    "        for j in range(3):\n",
    "            data[new_cols[j]] = transformed_data[:, j]\n",
    "\n",
    "    features += pca_coord_cols\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cosine dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_coses(data, features):\n",
    "    def get_layer_coords(data, i):\n",
    "        return data[[x_cols[i], y_cols[i], z_cols[i]]].values\n",
    "\n",
    "    def dot(x, y):\n",
    "        return np.sum(x * y, axis=1)\n",
    "    \n",
    "    def norm(x):\n",
    "        return np.sqrt(dot(x, x))\n",
    "\n",
    "    def get_cosine_dist(L1, L2, L1_norm, L2_norm):\n",
    "        return dot(L1, L2) / L1_norm / L2_norm\n",
    "    \n",
    "    def get_angle(cosines):\n",
    "        return np.arccos(cosines, dtype=np.float32) / np.pi * 180\n",
    "    \n",
    "    layers = np.array([get_layer_coords(data, i) for i in range(4)])\n",
    "    layers[1:] -= layers[:3]\n",
    "    norms = list(map(norm, layers))\n",
    "    \n",
    "    for i in range(3):\n",
    "        cosines = get_cosine_dist(layers[i], layers[i+1], norms[i], norms[i+1])\n",
    "        angles = get_angle(cosines)\n",
    "        data[da_cols[i]] = angles\n",
    "        \n",
    "    features += da_cols        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### IsMuon && IsMuonTight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_is_muon_tight(data, features):\n",
    "    return add_is_muon(data, features, threshold=2)\n",
    "    \n",
    "def add_is_muon(data, features, threshold=1):\n",
    "    def lt(p):\n",
    "        return data.P < p\n",
    "    def gt(p):\n",
    "        return data.P >= p\n",
    "    def M(i):\n",
    "        return data[hit_type_cols[i]] >= threshold\n",
    "    \n",
    "    lt_6k_mask = lt(6000.) & M(0) & M(1)\n",
    "    lt_10k_gt_6k_mask = gt(6000.) & lt(10000.) & M(0) & M(1) & (M(2) | M(3))\n",
    "    gt_10k_mask = gt(10000.) & M(0) & M(1) & M(2) & M(3)\n",
    "    \n",
    "    data.loc[:, is_muon_cols[0]] = 1 * (lt_6k_mask | lt_10k_gt_6k_mask | gt_10k_mask)\n",
    "    features += is_muon_cols        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Probability hit detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_probability_hit_detector(data, features):\n",
    "    p = data[mom_cols[0]].values\n",
    "    \n",
    "    def prob(i):\n",
    "        alpha = (0.0260, 0.0021, 0.0015, 0.0008)\n",
    "        beta = (2040., 2387., 3320., 3903.)\n",
    "        t = (alpha[i] * (p - beta[i]))**(i+1)\n",
    "        return t / (1 + t)\n",
    "        \n",
    "    for i in range(4):\n",
    "        data.loc[:, prob_hit_detector_cols[i]] = prob(i)\n",
    "        \n",
    "    features += prob_hit_detector_cols\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_mse(data, features):\n",
    "    dxy = (data.loc[:, xy_cols].values - data.loc[:, exy_cols].values) / data.loc[:, dx_cols + dy_cols].values / 2.\n",
    "    D = np.mean(dxy**2, axis=1)\n",
    "    \n",
    "    data.loc[:, err_cols[0]] = D\n",
    "    features += [err_cols[0]]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_normed_err(data, features):\n",
    "    dxy = data.loc[:, xy_cols].values - data.loc[:, exy_cols].values\n",
    "    normed_errors = dxy / np.sqrt(data.loc[:, edxy_cols].values)\n",
    "    \n",
    "    for i in range(4):\n",
    "        data.loc[:, nerr_x_cols[i]] = normed_errors[:, i]\n",
    "        data.loc[:, nerr_y_cols[i]] = normed_errors[:, i + 4]\n",
    "    \n",
    "    features += nerr_xy_cols\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>scr</th>\n",
       "      <th>th</th>\n",
       "      <th>dTh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.756900</td>\n",
       "      <td>0.706487</td>\n",
       "      <td>0.879001</td>\n",
       "      <td>0.783350</td>\n",
       "      <td>0.797067</td>\n",
       "      <td>0.690523</td>\n",
       "      <td>0.066475</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000614</td>\n",
       "      <td>0.001772</td>\n",
       "      <td>0.004764</td>\n",
       "      <td>0.000989</td>\n",
       "      <td>0.005021</td>\n",
       "      <td>0.028296</td>\n",
       "      <td>0.001094</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.756376</td>\n",
       "      <td>0.704573</td>\n",
       "      <td>0.873725</td>\n",
       "      <td>0.782223</td>\n",
       "      <td>0.791623</td>\n",
       "      <td>0.665091</td>\n",
       "      <td>0.065471</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.756562</td>\n",
       "      <td>0.705695</td>\n",
       "      <td>0.877007</td>\n",
       "      <td>0.782989</td>\n",
       "      <td>0.794843</td>\n",
       "      <td>0.675283</td>\n",
       "      <td>0.065892</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.756749</td>\n",
       "      <td>0.706818</td>\n",
       "      <td>0.880288</td>\n",
       "      <td>0.783755</td>\n",
       "      <td>0.798062</td>\n",
       "      <td>0.685474</td>\n",
       "      <td>0.066313</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.757162</td>\n",
       "      <td>0.707444</td>\n",
       "      <td>0.881638</td>\n",
       "      <td>0.783914</td>\n",
       "      <td>0.799789</td>\n",
       "      <td>0.703239</td>\n",
       "      <td>0.066977</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.708070</td>\n",
       "      <td>0.882988</td>\n",
       "      <td>0.784073</td>\n",
       "      <td>0.801516</td>\n",
       "      <td>0.721003</td>\n",
       "      <td>0.067641</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            acc      prec       rec        f1   roc_auc       scr        th  \\\n",
       "count  3.000000  3.000000  3.000000  3.000000  3.000000  3.000000  3.000000   \n",
       "mean   0.756900  0.706487  0.879001  0.783350  0.797067  0.690523  0.066475   \n",
       "std    0.000614  0.001772  0.004764  0.000989  0.005021  0.028296  0.001094   \n",
       "min    0.756376  0.704573  0.873725  0.782223  0.791623  0.665091  0.065471   \n",
       "25%    0.756562  0.705695  0.877007  0.782989  0.794843  0.675283  0.065892   \n",
       "50%    0.756749  0.706818  0.880288  0.783755  0.798062  0.685474  0.066313   \n",
       "75%    0.757162  0.707444  0.881638  0.783914  0.799789  0.703239  0.066977   \n",
       "max    0.757576  0.708070  0.882988  0.784073  0.801516  0.721003  0.067641   \n",
       "\n",
       "       dTh  \n",
       "count  3.0  \n",
       "mean   0.0  \n",
       "std    0.0  \n",
       "min    0.0  \n",
       "25%    0.0  \n",
       "50%    0.0  \n",
       "75%    0.0  \n",
       "max    0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class DataTransformer(TransformerMixin):\n",
    "    def __init__(self, *featurizers):\n",
    "        self.featurizers = featurizers\n",
    "    \n",
    "    def fit(self, data, y=None):\n",
    "#         self.pca_model = pca_fit(data)\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        data = data.copy()\n",
    "        features = [] + xyz_cols + mom_cols + hit_type_cols + dxyz_cols + exy_cols + edxy_cols\n",
    "\n",
    "#         pca_transform(self.pca_model, data, features)\n",
    "#         add_is_muon(data, features)\n",
    "#         add_is_muon_tight(data, features)\n",
    "        add_probability_hit_detector(data, features)\n",
    "        add_coses(data, features)\n",
    "        add_mse(data, features)\n",
    "        add_normed_err(data, features)\n",
    "                \n",
    "        if features:\n",
    "            data = data.loc[:, features]\n",
    "        else:\n",
    "            data = data.drop(train_cols, axis=1)\n",
    "        return data\n",
    "\n",
    "df_scores = cross_validate(train, n_splits=3, n_rows=20000)\n",
    "display(df_scores.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>scr</th>\n",
       "      <th>th</th>\n",
       "      <th>dTh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.765430</td>\n",
       "      <td>0.716089</td>\n",
       "      <td>0.87966</td>\n",
       "      <td>0.789482</td>\n",
       "      <td>0.805171</td>\n",
       "      <td>0.746396</td>\n",
       "      <td>0.065736</td>\n",
       "      <td>0.000028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.003003</td>\n",
       "      <td>0.003847</td>\n",
       "      <td>0.00338</td>\n",
       "      <td>0.002117</td>\n",
       "      <td>0.003652</td>\n",
       "      <td>0.024239</td>\n",
       "      <td>0.001910</td>\n",
       "      <td>0.000063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.760100</td>\n",
       "      <td>0.709792</td>\n",
       "      <td>0.87680</td>\n",
       "      <td>0.785784</td>\n",
       "      <td>0.799652</td>\n",
       "      <td>0.722304</td>\n",
       "      <td>0.063544</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.766200</td>\n",
       "      <td>0.714990</td>\n",
       "      <td>0.87750</td>\n",
       "      <td>0.789803</td>\n",
       "      <td>0.804554</td>\n",
       "      <td>0.729295</td>\n",
       "      <td>0.064457</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.766650</td>\n",
       "      <td>0.718362</td>\n",
       "      <td>0.87870</td>\n",
       "      <td>0.790256</td>\n",
       "      <td>0.804688</td>\n",
       "      <td>0.743997</td>\n",
       "      <td>0.065446</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.767100</td>\n",
       "      <td>0.718512</td>\n",
       "      <td>0.88000</td>\n",
       "      <td>0.790482</td>\n",
       "      <td>0.808034</td>\n",
       "      <td>0.752049</td>\n",
       "      <td>0.066913</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.767100</td>\n",
       "      <td>0.718791</td>\n",
       "      <td>0.88530</td>\n",
       "      <td>0.791082</td>\n",
       "      <td>0.808925</td>\n",
       "      <td>0.784336</td>\n",
       "      <td>0.068320</td>\n",
       "      <td>0.000142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            acc      prec      rec        f1   roc_auc       scr        th  \\\n",
       "count  5.000000  5.000000  5.00000  5.000000  5.000000  5.000000  5.000000   \n",
       "mean   0.765430  0.716089  0.87966  0.789482  0.805171  0.746396  0.065736   \n",
       "std    0.003003  0.003847  0.00338  0.002117  0.003652  0.024239  0.001910   \n",
       "min    0.760100  0.709792  0.87680  0.785784  0.799652  0.722304  0.063544   \n",
       "25%    0.766200  0.714990  0.87750  0.789803  0.804554  0.729295  0.064457   \n",
       "50%    0.766650  0.718362  0.87870  0.790256  0.804688  0.743997  0.065446   \n",
       "75%    0.767100  0.718512  0.88000  0.790482  0.808034  0.752049  0.066913   \n",
       "max    0.767100  0.718791  0.88530  0.791082  0.808925  0.784336  0.068320   \n",
       "\n",
       "            dTh  \n",
       "count  5.000000  \n",
       "mean   0.000028  \n",
       "std    0.000063  \n",
       "min    0.000000  \n",
       "25%    0.000000  \n",
       "50%    0.000000  \n",
       "75%    0.000000  \n",
       "max    0.000142  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 53.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_scores = cross_validate(train, n_splits=5, n_rows=100000)\n",
    "display(df_scores.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/test_public_v2.csv.gz', na_values=['-9999.0', '255'], usecols=TESTSET_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fit_predict_save(get_head_w_proportions(train, 1000000, None), test, \"out/06_x_dx_ex_edx_mom_hit_phit_da_mse_nerr_orig_1000k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fit_predict_save(get_head_w_proportions(train, 1300000, .3), test, \"out/06_x_dx_ex_edx_mom_hit_phit_da_mse_nerr_30_1300k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fit_predict_save(get_head_w_proportions(train, 1700000, .5), test, \"out/06_x_dx_ex_edx_mom_hit_phit_da_mse_nerr_50_1700k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fit_predict_save(get_samples_w_proptions(train, 100000, .5), test, \"out/04_prop_80_20_100_800.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fit_predict_save(get_samples(train, 100000), test, \"out/03_baseline_head_100.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fit_save_model(get_head_w_proportions(train, 100000, .5), \"models/06_x_dx_ex_edx_mom_hit_da_mse_50_100k.xgb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame([[1.0, np.NaN], [np.NaN, np.NaN], [2.0, 3.1]], columns=['a', 'b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_backup = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\tf\\lib\\site-packages\\numpy\\lib\\arraysetops.py:522: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = pd.read_csv('data/train_part_1_v2.csv.gz', na_values=['-9999.0', '255'], index_col=utils.ID_COLUMN)\n",
    "# train = pd.read_csv('data/train_part_2_v2.csv.gz', nrows=10000, na_values=['-9999.0', '255'])\n",
    "# train = pd.concat([train_0, train_1], axis=0, ignore_index=True)\n",
    "\n",
    "label0 = train.loc[train.label==0, :]\n",
    "label0.to_csv('data/train_pub_L0_p1.csv.gz', compression='gzip')\n",
    "\n",
    "label1 = train.loc[train.label==1, :]\n",
    "label1.to_csv('data/train_pub_L1_p1.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "label0.to_pickle('data/train_pub_L0_p1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 48.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "label1.to_pickle('data/train_pub_L1_p1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((210403, 79), (2512449, 79))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label0.shape, label1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train = pd.read_csv('data/train_part_2_v2.csv.gz', na_values=['-9999.0', '255'], index_col=utils.ID_COLUMN)\n",
    "# train = pd.read_csv('data/train_part_2_v2.csv.gz', nrows=10000, na_values=['-9999.0', '255'])\n",
    "# train = pd.concat([train_0, train_1], axis=0, ignore_index=True)\n",
    "\n",
    "label0 = train.loc[train.label==0, :]\n",
    "label0.to_csv('data/train_pub_L0_p2.csv.gz', compression='gzip')\n",
    "\n",
    "label1 = train.loc[train.label==1, :]\n",
    "label1.to_csv('data/train_pub_L1_p2.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(736, 79) (736, 79)\n",
      "Wall time: 702 ms\n"
     ]
    }
   ],
   "source": [
    "# label0_ = pd.read_csv('data/train_pub_L0_p1.csv', nrows=10000, na_values=['-9999.0', '255'], index_col=utils.ID_COLUMN)\n",
    "\n",
    "# print(label0.shape, label0_.shape)\n",
    "\n",
    "# for c in label0.columns:\n",
    "#     lc0 = label0.loc[:, c]\n",
    "#     if not np.issubdtype(lc0.dtype, np.number):\n",
    "#         continue\n",
    "#     lc0_ = label0_.loc[:, c]\n",
    "#     mask = (lc0 - lc0_).abs() > 1e-10\n",
    "#     if mask.sum() > 0 :\n",
    "#         display(lc0[mask] - lc0_[mask])\n",
    "        \n",
    "\n",
    "# for c in label0.columns:\n",
    "#     lc0 = label0.loc[:, c]\n",
    "#     if np.issubdtype(lc0.dtype, np.number):\n",
    "#         continue\n",
    "#     lc0_ = label0_.loc[:, c]\n",
    "#     mask = (lc0 != lc0_) & (lc0 == lc0) & (lc0_ == lc0_)\n",
    "#     for i in range(mask.sum()):\n",
    "#         s0 = ''.join(lc0.iloc[mask].iloc[i].split())\n",
    "#         s1 = ''.join(lc0_.iloc[mask].iloc[i].split())\n",
    "#         if s0 == s1:\n",
    "#             continue\n",
    "        \n",
    "#         print(lc0.iloc[0])\n",
    "#         print(lc0_.iloc[0])\n",
    "#         for i,s in enumerate(difflib.ndiff(lc0.iloc[0], lc0_.iloc[0])):\n",
    "#             if s[0]==' ': continue\n",
    "#             elif s[0]=='-':\n",
    "#                 print(u'Delete \"{}\" `{}` from position {}'.format(s[-1], ord(s[-1]),i))\n",
    "#             elif s[0]=='+':\n",
    "#                 print(u'Add \"{}\" `{}` to position {}'.format(s[-1],ord(s[-1]),i))    \n",
    "#         print()  \n",
    "\n",
    "# display(label0[mask].head(10))\n",
    "# display(label0_[mask].head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
