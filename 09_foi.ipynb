{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABUkAAABECAYAAAC8urRAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAA4JJREFUeJzt2zFLVWEcx/H/DYmgSYoCc7Bw0KDFcyVoCaLNNxAS1FBRSw0NTYEUBA0NtRTVEoT0BtoqaAlCj0uQDlIOt6AonAKJ4PYG1Kmnp+7/8xnPs/wOHM7whafT7/cDAAAAACCrHbUHAAAAAADUJJICAAAAAKmJpAAAAABAaiIpAAAAAJCaSAoAAAAApDa03WHbtv2/NQQAAAAAoKSmaTqbPd82kkZEzM33/vyaf8Tc7GjcXn1ce0Yx18bPxcaNW7VnFPFy/GzMzI7E8/nPtacUMTM7EvH9We0Z5ew5FQ++bPpPGggX9/fj8NMXtWcU8/70yVhfOF97RjHD04/iZm8w3+/66KO4tLZSe0Yx98cm4sryWO0ZxdydXIsDDydqzyjm04WV6F5tas8oZvFOG0cPXq49o4i3H+/FaPd47RnF9BZfx6sz+2rPKObEk6/RbHRrzyim3bUY3d0D/G/50cabn4P5fsd2trEw/qH2jGKmVw9F826p9oxi2iNTMbQ+mN9mRMSv4TaaqeXaM4pplyajt3ek9owiRr9t3ZFctwcAAAAAUhNJAQAAAIDURFIAAAAAIDWRFAAAAABITSQFAAAAAFITSQEAAACA1ERSAAAAACA1kRQAAAAASE0kBQAAAABSE0kBAAAAgNREUgAAAAAgNZEUAAAAAEhNJAUAAAAAUhNJAQAAAIDURFIAAAAAIDWRFAAAAABITSQFAAAAAFITSQEAAACA1ERSAAAAACA1kRQAAAAASE0kBQAAAABSE0kBAAAAgNREUgAAAAAgNZEUAAAAAEhNJAUAAAAAUhNJAQAAAIDURFIAAAAAIDWRFAAAAABITSQFAAAAAFITSQEAAACA1ERSAAAAACA1kRQAAAAASE0kBQAAAABSE0kBAAAAgNREUgAAAAAgNZEUAAAAAEhNJAUAAAAAUhNJAQAAAIDURFIAAAAAIDWRFAAAAABITSQFAAAAAFITSQEAAACA1ERSAAAAACA1kRQAAAAASE0kBQAAAABSE0kBAAAAgNREUgAAAAAgNZEUAAAAAEhNJAUAAAAAUhNJAQAAAIDURFIAAAAAIDWRFAAAAABITSQFAAAAAFLr9Pv9LQ/btt36EAAAAADgP9I0TWez59tGUgAAAACAQee6PQAAAACQmkgKAAAAAKQmkgIAAAAAqYmkAAAAAEBqIikAAAAAkNpvuTVZgasyZKkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1728x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as sts\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from common import *\n",
    "from io_tools import (\n",
    "    read_train, read_pub_test, read_pvt_test,\n",
    "    convert_train, convert_pub_test, convert_pvt_test,\n",
    ")\n",
    "from pipeline import (\n",
    "    split_classes, count_classes, sample,\n",
    "    cross_validate, fit_predict_save, fit_save_model\n",
    ")\n",
    "from transformers.pca import pca_fit, pca_transform\n",
    "from transformers.cosine import add_coses, to_degrees\n",
    "from transformers.momentum import add_is_muon, add_is_muon_tight, add_probability_hit_detector\n",
    "from transformers.err import add_mse, add_normed_err, err_cols\n",
    "\n",
    "sns.set()\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "color_palette = sns.color_palette('deep') + sns.color_palette('husl', 6) + sns.color_palette('bright') + sns.color_palette('pastel')\n",
    "\n",
    "%matplotlib inline\n",
    "sns.palplot(color_palette)\n",
    "\n",
    "def ndprint(a, precision=3):\n",
    "    with np.printoptions(precision=precision, suppress=True):\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 63)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(421218, 578782)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "used_cols = xyz_cols + mom_cols + hit_type_cols + dxyz_cols + exy_cols + edxy_cols + hit_stats_cols + t_cols + ncl_cols + avg_cs_cols\n",
    "global_feature_importance = None\n",
    "train, train_foi = read_train(used_cols, 1000000)\n",
    "display(train.shape, count_classes(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transformer\n",
    "\n",
    "Это по сути основная часть. Класс, который отбирает нужные столбцы, возможно что-то модифицирует или добавляет. На выходе - входные данные для модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dll_train, _ = read_train(xy_cols + dx_cols + dy_cols + exy_cols, 10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dll_train = add_mse(dll_train, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233\n"
     ]
    }
   ],
   "source": [
    "def create_pdfs(data):\n",
    "    dts = [dt.loc[:, err_cols[0]] for dt in split_classes(data)]\n",
    "    min_len = min(map(len, dts))\n",
    "    nbins = int(round(np.sqrt(min_len) / np.pi))\n",
    "    nbins = 230\n",
    "    \n",
    "    l, r = np.min(data[err_cols[0]]) - 1e-5, np.max(data[err_cols[0]]) + 1e-5\n",
    "    k = 9\n",
    "    m = (r - l) / k\n",
    "    m = 20\n",
    "    bins = np.concatenate((\n",
    "        np.arange(l, 1, .02),\n",
    "        np.arange(1, 3, .04),\n",
    "        np.arange(3, 10, .1),\n",
    "        np.arange(10, 16, .4),\n",
    "        np.arange(16, 34, 1.),\n",
    "        np.arange(34, 66, 2),\n",
    "        np.arange(66, 120, 5.),\n",
    "        np.linspace(120, r, 3),\n",
    "    ))\n",
    "    nbins = len(bins)\n",
    "    print(nbins)\n",
    "#     bins = np.concatenate((\n",
    "#         np.linspace(l, m, (k-1) * nbins // k, endpoint=False), \n",
    "#         np.linspace(m, r, nbins // k)\n",
    "#     ))\n",
    "    pdfs = []\n",
    "    \n",
    "    for i in range(2):\n",
    "        pdf, _ = np.histogram(dts[i], bins=bins)\n",
    "        pdfs.append(pdf)\n",
    "                       \n",
    "    return pdfs, bins\n",
    "                       \n",
    "pdfs, bins = create_pdfs(dll_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1617, 5303, 7394, 8508, 8308, 8093, 7779, 7113, 6767, 6152, 5532,\n",
       "        4973, 4504, 4202, 3838, 3555, 3310, 3099, 2746, 2699, 2521, 2441,\n",
       "        2238, 2269, 2050, 2110, 1840, 1840, 1770, 1733, 1597, 1590, 1563,\n",
       "        1516, 1415, 1378, 1311, 1356, 1199, 1288, 1129, 1155, 1153, 1078,\n",
       "        1117, 1068, 1058,  989,  984,  892, 1868, 1747, 1667, 1638, 1580,\n",
       "        1521, 1551, 1439, 1343, 1429, 1258, 1221, 1202, 1179, 1177, 1160,\n",
       "        1110, 1086, 1068, 1009,  970, 1033,  950,  943,  955,  929,  934,\n",
       "         894,  821,  868,  852,  834,  823,  767,  781,  717,  750,  720,\n",
       "         761,  696,  698,  728,  726,  685,  673,  673,  615,  581,  617,\n",
       "         661, 1545, 1516, 1405, 1398, 1386, 1314, 1306, 1291, 1210, 1211,\n",
       "        1175, 1186, 1087, 1099, 1089, 1058, 1099,  990, 1042, 1019, 1033,\n",
       "        1007,  951,  912,  930,  909,  916,  867,  894,  877,  884,  872,\n",
       "         883,  833,  860,  908,  845,  839,  850,  883,  853,  850,  858,\n",
       "         771,  780,  768,  844,  838,  779,  794,  817,  756,  784,  786,\n",
       "         744,  757,  817,  795,  755,  727,  725,  755,  736,  773,  723,\n",
       "         706,  756,  740,  715,  764, 2802, 2743, 2786, 2684, 2582, 2648,\n",
       "        2599, 2597, 2490, 2336, 2372, 2433, 2336, 2331, 2290, 5528, 5207,\n",
       "        5032, 4866, 4659, 4398, 4264, 3946, 3827, 3532, 3476, 3301, 3121,\n",
       "        2970, 2931, 2767, 2655, 2477, 4621, 4275, 3808, 3374, 3110, 2855,\n",
       "        2556, 2342, 2048, 1796, 1657, 1541, 1389, 1157, 1082,  977, 1970,\n",
       "        1557, 1159,  858,  675,  443,  335,  248,  214,  141,   80,  254,\n",
       "          17], dtype=int64),\n",
       " array([ 56952, 180666, 245985, 281076, 277342, 266211, 252409, 237309,\n",
       "        216042, 193494, 171383, 151215, 134795, 119490, 107454,  97186,\n",
       "         87833,  80144,  73356,  67180,  62304,  57651,  53569,  49609,\n",
       "         46324,  43070,  40778,  38335,  35972,  34169,  32195,  30542,\n",
       "         28748,  27500,  26226,  24663,  23493,  22573,  21181,  20386,\n",
       "         19970,  18925,  18187,  17411,  16645,  16165,  15368,  14640,\n",
       "         14391,  13123,  26061,  24175,  22895,  21204,  20093,  18882,\n",
       "         17830,  16765,  15813,  15096,  13951,  13492,  12730,  12283,\n",
       "         11335,  10882,  10450,   9970,   9485,   9180,   8762,   8356,\n",
       "          7869,   7700,   7365,   7081,   6742,   6522,   6326,   6284,\n",
       "          5806,   5574,   5435,   5255,   5061,   4971,   4750,   4704,\n",
       "          4507,   4264,   4247,   3990,   4080,   3931,   3748,   3619,\n",
       "          3575,   3469,   3351,   3301,   7871,   7533,   6869,   6561,\n",
       "          6208,   5832,   5527,   5274,   5022,   4710,   4629,   4276,\n",
       "          4175,   3937,   3692,   3630,   3496,   3395,   3156,   3145,\n",
       "          3017,   2880,   2851,   2782,   2657,   2566,   2508,   2432,\n",
       "          2374,   2337,   2213,   2142,   2123,   1995,   1944,   1855,\n",
       "          1873,   1809,   1796,   1714,   1701,   1608,   1629,   1606,\n",
       "          1611,   1623,   1507,   1523,   1488,   1386,   1467,   1379,\n",
       "          1337,   1369,   1296,   1308,   1252,   1235,   1241,   1214,\n",
       "          1162,   1210,   1139,   1137,   1197,   1116,   1090,   1112,\n",
       "          1052,   1053,   4028,   3883,   3795,   3510,   3469,   3249,\n",
       "          3161,   3195,   3014,   2974,   2761,   2848,   2789,   2577,\n",
       "          2331,   5821,   5774,   5446,   5028,   4903,   4607,   4300,\n",
       "          4196,   4069,   3845,   3729,   3499,   3354,   3318,   3202,\n",
       "          2941,   2910,   2795,   5310,   4869,   4599,   4220,   3925,\n",
       "          3673,   3307,   3021,   2826,   2552,   2429,   2171,   1949,\n",
       "          1800,   1747,   1508,   3286,   2496,   1875,   1529,   1122,\n",
       "           873,    627,    462,    354,    276,    148,    493,     13],\n",
       "       dtype=int64)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>scr</th>\n",
       "      <th>th</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.776299</td>\n",
       "      <td>0.769564</td>\n",
       "      <td>0.878120</td>\n",
       "      <td>0.820216</td>\n",
       "      <td>0.813297</td>\n",
       "      <td>0.676602</td>\n",
       "      <td>0.096984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.012190</td>\n",
       "      <td>0.014392</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.007968</td>\n",
       "      <td>0.011291</td>\n",
       "      <td>0.078658</td>\n",
       "      <td>0.008253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.763876</td>\n",
       "      <td>0.754993</td>\n",
       "      <td>0.877646</td>\n",
       "      <td>0.812127</td>\n",
       "      <td>0.805925</td>\n",
       "      <td>0.594847</td>\n",
       "      <td>0.088325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.770327</td>\n",
       "      <td>0.762460</td>\n",
       "      <td>0.877873</td>\n",
       "      <td>0.816295</td>\n",
       "      <td>0.806798</td>\n",
       "      <td>0.639029</td>\n",
       "      <td>0.093096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.776778</td>\n",
       "      <td>0.769928</td>\n",
       "      <td>0.878099</td>\n",
       "      <td>0.820463</td>\n",
       "      <td>0.807672</td>\n",
       "      <td>0.683211</td>\n",
       "      <td>0.097867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.782510</td>\n",
       "      <td>0.776849</td>\n",
       "      <td>0.878357</td>\n",
       "      <td>0.824260</td>\n",
       "      <td>0.816983</td>\n",
       "      <td>0.717479</td>\n",
       "      <td>0.101313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.788242</td>\n",
       "      <td>0.783771</td>\n",
       "      <td>0.878616</td>\n",
       "      <td>0.828057</td>\n",
       "      <td>0.826295</td>\n",
       "      <td>0.751746</td>\n",
       "      <td>0.104759</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            acc      prec       rec        f1   roc_auc       scr        th\n",
       "count  3.000000  3.000000  3.000000  3.000000  3.000000  3.000000  3.000000\n",
       "mean   0.776299  0.769564  0.878120  0.820216  0.813297  0.676602  0.096984\n",
       "std    0.012190  0.014392  0.000485  0.007968  0.011291  0.078658  0.008253\n",
       "min    0.763876  0.754993  0.877646  0.812127  0.805925  0.594847  0.088325\n",
       "25%    0.770327  0.762460  0.877873  0.816295  0.806798  0.639029  0.093096\n",
       "50%    0.776778  0.769928  0.878099  0.820463  0.807672  0.683211  0.097867\n",
       "75%    0.782510  0.776849  0.878357  0.824260  0.816983  0.717479  0.101313\n",
       "max    0.788242  0.783771  0.878616  0.828057  0.826295  0.751746  0.104759"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def add_dll(data, features):\n",
    "    def get_probs(pdf, x):\n",
    "        indices = np.digitize(x, bins) - 1\n",
    "        wbin = (bins[indices + 1] - bins[indices]) / (np.max(bins) - np.min(bins))\n",
    "        prob = pdf[indices] / pdf.sum()\n",
    "        return prob #* wbin\n",
    "\n",
    "    def get_dll(x):\n",
    "        probs = [get_probs(pdf, x) for pdf in pdfs]\n",
    "        DLL = np.log(probs[1]) - np.log(probs[0])\n",
    "        return DLL\n",
    "\n",
    "    data[err_cols[1]] = get_dll(data.loc[:, err_cols[0]])\n",
    "    features += err_cols[1:2]\n",
    "    return data\n",
    "\n",
    "df_scores, feature_importance = cross_validate(train, n_estimators=60, n_splits=3, n_rows=10000, transformer_cls=DataTransformer)\n",
    "display(df_scores.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>scr</th>\n",
       "      <th>th</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.780200</td>\n",
       "      <td>0.769615</td>\n",
       "      <td>0.883301</td>\n",
       "      <td>0.822542</td>\n",
       "      <td>0.815059</td>\n",
       "      <td>0.658164</td>\n",
       "      <td>0.094295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.001347</td>\n",
       "      <td>0.002696</td>\n",
       "      <td>0.003479</td>\n",
       "      <td>0.000807</td>\n",
       "      <td>0.007415</td>\n",
       "      <td>0.047332</td>\n",
       "      <td>0.009170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.778644</td>\n",
       "      <td>0.767012</td>\n",
       "      <td>0.879292</td>\n",
       "      <td>0.821825</td>\n",
       "      <td>0.806830</td>\n",
       "      <td>0.606934</td>\n",
       "      <td>0.085063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.779811</td>\n",
       "      <td>0.768226</td>\n",
       "      <td>0.882184</td>\n",
       "      <td>0.822105</td>\n",
       "      <td>0.811976</td>\n",
       "      <td>0.637111</td>\n",
       "      <td>0.089741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.780978</td>\n",
       "      <td>0.769439</td>\n",
       "      <td>0.885075</td>\n",
       "      <td>0.822384</td>\n",
       "      <td>0.817122</td>\n",
       "      <td>0.667288</td>\n",
       "      <td>0.094420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.780978</td>\n",
       "      <td>0.770917</td>\n",
       "      <td>0.885306</td>\n",
       "      <td>0.822900</td>\n",
       "      <td>0.819173</td>\n",
       "      <td>0.683779</td>\n",
       "      <td>0.098911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.780978</td>\n",
       "      <td>0.772395</td>\n",
       "      <td>0.885536</td>\n",
       "      <td>0.823416</td>\n",
       "      <td>0.821223</td>\n",
       "      <td>0.700270</td>\n",
       "      <td>0.103401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            acc      prec       rec        f1   roc_auc       scr        th\n",
       "count  3.000000  3.000000  3.000000  3.000000  3.000000  3.000000  3.000000\n",
       "mean   0.780200  0.769615  0.883301  0.822542  0.815059  0.658164  0.094295\n",
       "std    0.001347  0.002696  0.003479  0.000807  0.007415  0.047332  0.009170\n",
       "min    0.778644  0.767012  0.879292  0.821825  0.806830  0.606934  0.085063\n",
       "25%    0.779811  0.768226  0.882184  0.822105  0.811976  0.637111  0.089741\n",
       "50%    0.780978  0.769439  0.885075  0.822384  0.817122  0.667288  0.094420\n",
       "75%    0.780978  0.770917  0.885306  0.822900  0.819173  0.683779  0.098911\n",
       "max    0.780978  0.772395  0.885536  0.823416  0.821223  0.700270  0.103401"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from common import x_cols, y_cols, z_cols\n",
    "\n",
    "da_cols = ['DAngle[%d]' % i for i in range(1, 4)]\n",
    "\n",
    "def add_coses(data, features):\n",
    "    def get_layer_coords(data, i):\n",
    "        return data[[x_cols[i], y_cols[i], z_cols[i]]].values\n",
    "  \n",
    "    def dot(x, y):\n",
    "        return np.sum(x * y, axis=1, dtype=np.float32)\n",
    "    \n",
    "    def norm(x):\n",
    "        return np.sqrt(dot(x, x))\n",
    "\n",
    "    def get_cosine_dist(L1, L2, L1_norm, L2_norm):\n",
    "        cosines = dot(L1, L2) / L1_norm / L2_norm\n",
    "        return np.clip(cosines, -1., 1.)\n",
    "    \n",
    "    layers = np.array([get_layer_coords(data, i) for i in range(4)])\n",
    "    layers[1:] -= layers[:3]\n",
    "    layers[0] = get_zero_point(data)\n",
    "    \n",
    "    for i in range(3):\n",
    "        cur_layer = layers[i]\n",
    "        next_layer = layers[i+1]\n",
    "        nan_mask = np.isnan(next_layer[:, 0])\n",
    "        next_layer[nan_mask, :] = cur_layer[nan_mask, :]\n",
    "        \n",
    "        cosines = get_cosine_dist(cur_layer, next_layer, norm(cur_layer), norm(next_layer))\n",
    "        degrees = to_degrees(cosines)\n",
    "        cosines[nan_mask] = np.NaN\n",
    "        degrees[nan_mask] = np.NaN\n",
    "        data[da_cols[i]] = degrees\n",
    "        \n",
    "    features += da_cols\n",
    "    return data\n",
    "\n",
    "def to_degrees(cosine):\n",
    "    return np.arccos(cosine) / np.pi * 180.\n",
    "\n",
    "def _to_degrees(cosines):\n",
    "    angles = cosines.copy()\n",
    "    isn_mask = ~np.isnan(cosines)\n",
    "    angles[isn_mask] = np.arccos(cosines[isn_mask]) / np.pi * 180.\n",
    "    return angles\n",
    "\n",
    "def get_zero_point(data):\n",
    "    layers = [data[[ex_cols[i], ey_cols[i], z_cols[i]]].values for i in range(2)]\n",
    "    d = layers[1] - layers[0]\n",
    "    return d\n",
    "    \n",
    "df_scores, feature_importance = cross_validate(train, n_estimators=60, n_splits=3, n_rows=10000, transformer_cls=DataTransformer)\n",
    "display(df_scores.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>scr</th>\n",
       "      <th>th</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.756022</td>\n",
       "      <td>0.725857</td>\n",
       "      <td>0.902703</td>\n",
       "      <td>0.804413</td>\n",
       "      <td>0.806153</td>\n",
       "      <td>0.637157</td>\n",
       "      <td>0.122516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.027574</td>\n",
       "      <td>0.029352</td>\n",
       "      <td>0.014301</td>\n",
       "      <td>0.018448</td>\n",
       "      <td>0.032882</td>\n",
       "      <td>0.344359</td>\n",
       "      <td>0.008856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.733533</td>\n",
       "      <td>0.698347</td>\n",
       "      <td>0.886486</td>\n",
       "      <td>0.791569</td>\n",
       "      <td>0.782213</td>\n",
       "      <td>0.414045</td>\n",
       "      <td>0.112931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.740640</td>\n",
       "      <td>0.710407</td>\n",
       "      <td>0.897297</td>\n",
       "      <td>0.793843</td>\n",
       "      <td>0.787406</td>\n",
       "      <td>0.438858</td>\n",
       "      <td>0.118578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.747748</td>\n",
       "      <td>0.722467</td>\n",
       "      <td>0.908108</td>\n",
       "      <td>0.796117</td>\n",
       "      <td>0.792599</td>\n",
       "      <td>0.463672</td>\n",
       "      <td>0.124224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.767267</td>\n",
       "      <td>0.739612</td>\n",
       "      <td>0.910811</td>\n",
       "      <td>0.810835</td>\n",
       "      <td>0.818122</td>\n",
       "      <td>0.748714</td>\n",
       "      <td>0.127309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.786787</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>0.913514</td>\n",
       "      <td>0.825553</td>\n",
       "      <td>0.843645</td>\n",
       "      <td>1.033755</td>\n",
       "      <td>0.130394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            acc      prec       rec        f1   roc_auc       scr        th\n",
       "count  3.000000  3.000000  3.000000  3.000000  3.000000  3.000000  3.000000\n",
       "mean   0.756022  0.725857  0.902703  0.804413  0.806153  0.637157  0.122516\n",
       "std    0.027574  0.029352  0.014301  0.018448  0.032882  0.344359  0.008856\n",
       "min    0.733533  0.698347  0.886486  0.791569  0.782213  0.414045  0.112931\n",
       "25%    0.740640  0.710407  0.897297  0.793843  0.787406  0.438858  0.118578\n",
       "50%    0.747748  0.722467  0.908108  0.796117  0.792599  0.463672  0.124224\n",
       "75%    0.767267  0.739612  0.910811  0.810835  0.818122  0.748714  0.127309\n",
       "max    0.786787  0.756757  0.913514  0.825553  0.843645  1.033755  0.130394"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "err_x_cols = ['Err_X[%i]' % i for i in range(N_STATIONS)]\n",
    "err_y_cols = ['Err_Y[%i]' % i for i in range(N_STATIONS)]\n",
    "err_z_cols = ['Err_Z[%i]' % i for i in range(N_STATIONS)]\n",
    "err_xy_cols = err_x_cols + err_y_cols\n",
    "err_xyz_cols = err_xy_cols + err_z_cols\n",
    "ez = np.array([15270., 16470., 17670., 18870.])\n",
    "\n",
    "def add_errs(data, features):\n",
    "    for err_col, e_col, col in zip (err_xy_cols, exy_cols, xy_cols):\n",
    "        data.loc[:, err_col] = data[e_col].values - data[col].values\n",
    "        \n",
    "    for i in range(4):\n",
    "        data.loc[:, err_z_cols[i]] = ez[i] - data[z_cols[i]].values\n",
    "    \n",
    "    features += err_xyz_cols\n",
    "    \n",
    "df_scores, feature_importance = cross_validate(train, n_estimators=60, n_splits=3, n_rows=1000, transformer_cls=DataTransformer)\n",
    "display(df_scores.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_mse(data, features):\n",
    "#     dxy = (data.loc[:, xy_cols].values - data.loc[:, exy_cols].values) / data.loc[:, dx_cols + dy_cols].values / 2.\n",
    "    xy_vals = data.loc[:, xy_cols].values\n",
    "\n",
    "    dxy = (data.loc[:, xy_cols].values - data.loc[:, exy_cols].values) / data.loc[:, dx_cols + dy_cols].values / 2.\n",
    "    D = np.nanmean(dxy**2, axis=1)\n",
    "    \n",
    "    data.loc[:, err_cols[0]] = D\n",
    "    features += [err_cols[0]]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def sample(data, nrows):\n",
    "    return data.iloc[np.random.permutation(len(data.index))[:nrows], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>scr</th>\n",
       "      <th>th</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.782767</td>\n",
       "      <td>0.777908</td>\n",
       "      <td>0.879590</td>\n",
       "      <td>0.825623</td>\n",
       "      <td>0.815884</td>\n",
       "      <td>0.759522</td>\n",
       "      <td>0.092788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.002721</td>\n",
       "      <td>0.003603</td>\n",
       "      <td>0.000962</td>\n",
       "      <td>0.004565</td>\n",
       "      <td>0.073809</td>\n",
       "      <td>0.003352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.781178</td>\n",
       "      <td>0.775867</td>\n",
       "      <td>0.875834</td>\n",
       "      <td>0.824623</td>\n",
       "      <td>0.811292</td>\n",
       "      <td>0.683728</td>\n",
       "      <td>0.088951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.782250</td>\n",
       "      <td>0.776363</td>\n",
       "      <td>0.877876</td>\n",
       "      <td>0.825162</td>\n",
       "      <td>0.813617</td>\n",
       "      <td>0.723698</td>\n",
       "      <td>0.091609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.783322</td>\n",
       "      <td>0.776858</td>\n",
       "      <td>0.879918</td>\n",
       "      <td>0.825701</td>\n",
       "      <td>0.815942</td>\n",
       "      <td>0.763667</td>\n",
       "      <td>0.094266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.783561</td>\n",
       "      <td>0.778928</td>\n",
       "      <td>0.881467</td>\n",
       "      <td>0.826122</td>\n",
       "      <td>0.818181</td>\n",
       "      <td>0.797419</td>\n",
       "      <td>0.094707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.783800</td>\n",
       "      <td>0.780997</td>\n",
       "      <td>0.883017</td>\n",
       "      <td>0.826543</td>\n",
       "      <td>0.820420</td>\n",
       "      <td>0.831172</td>\n",
       "      <td>0.095147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            acc      prec       rec        f1   roc_auc       scr        th\n",
       "count  3.000000  3.000000  3.000000  3.000000  3.000000  3.000000  3.000000\n",
       "mean   0.782767  0.777908  0.879590  0.825623  0.815884  0.759522  0.092788\n",
       "std    0.001396  0.002721  0.003603  0.000962  0.004565  0.073809  0.003352\n",
       "min    0.781178  0.775867  0.875834  0.824623  0.811292  0.683728  0.088951\n",
       "25%    0.782250  0.776363  0.877876  0.825162  0.813617  0.723698  0.091609\n",
       "50%    0.783322  0.776858  0.879918  0.825701  0.815942  0.763667  0.094266\n",
       "75%    0.783561  0.778928  0.881467  0.826122  0.818181  0.797419  0.094707\n",
       "max    0.783800  0.780997  0.883017  0.826543  0.820420  0.831172  0.095147"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "def filter_unimportant_features(features):\n",
    "    if global_feature_importance is None:\n",
    "        return features\n",
    "    fscore = global_feature_importance\n",
    "    return [col for col in features if col not in fscore.index or fscore.loc[col, 'score'] > 0.01]\n",
    "#     return features\n",
    "\n",
    "class DataTransformer(TransformerMixin):\n",
    "    def __init__(self, *featurizers):\n",
    "        self.featurizers = featurizers\n",
    "    \n",
    "    def fit(self, data, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        data = data.copy()\n",
    "        features = [] + mom_cols + hit_type_cols + dxyz_cols + exy_cols + edxy_cols + hit_stats_cols + t_cols + ncl_cols + avg_cs_cols + xyz_cols\n",
    "        features = filter_unimportant_features(features)\n",
    "        self.origin_features = features.copy()\n",
    "\n",
    "#         add_is_muon(data, features)\n",
    "#         add_is_muon_tight(data, features)\n",
    "#         add_probability_hit_detector(data, features)\n",
    "        add_coses(data, features)\n",
    "        add_mse(data, features)\n",
    "        add_normed_err(data, features)\n",
    "        add_dll(data, features)\n",
    "        add_errs(data, features)\n",
    "        \n",
    "#         filter_data(data)\n",
    "        if not features:\n",
    "            raise('no features')\n",
    "    \n",
    "        features = filter_unimportant_features(features)\n",
    "        self.new_features = features[len(self.origin_features):]\n",
    "        self.features = self.origin_features + self.new_features\n",
    "#         print(len(features))\n",
    "        return data[features].values\n",
    "\n",
    "df_scores, feature_importance = cross_validate(train, n_estimators=60, n_splits=3, n_rows=30000, transformer_cls=DataTransformer)\n",
    "display(df_scores.describe())\n",
    "# display(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>scr</th>\n",
       "      <th>th</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.788380</td>\n",
       "      <td>0.781459</td>\n",
       "      <td>0.879963</td>\n",
       "      <td>0.827789</td>\n",
       "      <td>0.821117</td>\n",
       "      <td>0.749311</td>\n",
       "      <td>0.085629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.001846</td>\n",
       "      <td>0.001946</td>\n",
       "      <td>0.001930</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>0.002842</td>\n",
       "      <td>0.047672</td>\n",
       "      <td>0.001244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.786650</td>\n",
       "      <td>0.778951</td>\n",
       "      <td>0.876817</td>\n",
       "      <td>0.826114</td>\n",
       "      <td>0.818050</td>\n",
       "      <td>0.667882</td>\n",
       "      <td>0.083905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.786700</td>\n",
       "      <td>0.780684</td>\n",
       "      <td>0.879758</td>\n",
       "      <td>0.826825</td>\n",
       "      <td>0.819561</td>\n",
       "      <td>0.753003</td>\n",
       "      <td>0.085337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.787900</td>\n",
       "      <td>0.780954</td>\n",
       "      <td>0.880363</td>\n",
       "      <td>0.827533</td>\n",
       "      <td>0.819769</td>\n",
       "      <td>0.758626</td>\n",
       "      <td>0.085492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.790240</td>\n",
       "      <td>0.782709</td>\n",
       "      <td>0.880969</td>\n",
       "      <td>0.829121</td>\n",
       "      <td>0.823456</td>\n",
       "      <td>0.780465</td>\n",
       "      <td>0.086070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.790410</td>\n",
       "      <td>0.783996</td>\n",
       "      <td>0.881910</td>\n",
       "      <td>0.829354</td>\n",
       "      <td>0.824747</td>\n",
       "      <td>0.786578</td>\n",
       "      <td>0.087339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            acc      prec       rec        f1   roc_auc       scr        th\n",
       "count  5.000000  5.000000  5.000000  5.000000  5.000000  5.000000  5.000000\n",
       "mean   0.788380  0.781459  0.879963  0.827789  0.821117  0.749311  0.085629\n",
       "std    0.001846  0.001946  0.001930  0.001416  0.002842  0.047672  0.001244\n",
       "min    0.786650  0.778951  0.876817  0.826114  0.818050  0.667882  0.083905\n",
       "25%    0.786700  0.780684  0.879758  0.826825  0.819561  0.753003  0.085337\n",
       "50%    0.787900  0.780954  0.880363  0.827533  0.819769  0.758626  0.085492\n",
       "75%    0.790240  0.782709  0.880969  0.829121  0.823456  0.780465  0.086070\n",
       "max    0.790410  0.783996  0.881910  0.829354  0.824747  0.786578  0.087339"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_scores, feature_importance = cross_validate(train, n_estimators=120, n_splits=5, n_rows=100000, transformer_cls=DataTransformer)\n",
    "display(df_scores.describe())\n",
    "# display(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(global_feature_importance.score > .01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ErrMSE</th>\n",
       "      <td>0.136038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PT</th>\n",
       "      <td>0.094272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NShared</th>\n",
       "      <td>0.052506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ncl[0]</th>\n",
       "      <td>0.040573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DAngle[1]</th>\n",
       "      <td>0.039379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DAngle[2]</th>\n",
       "      <td>0.035800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P</th>\n",
       "      <td>0.033413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ncl[2]</th>\n",
       "      <td>0.031026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NErr_X[3]</th>\n",
       "      <td>0.029833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NErr_Y[3]</th>\n",
       "      <td>0.027446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DAngle[3]</th>\n",
       "      <td>0.026253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NErr_X[0]</th>\n",
       "      <td>0.023866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NErr_X[1]</th>\n",
       "      <td>0.021480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ncl[3]</th>\n",
       "      <td>0.019093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DLL</th>\n",
       "      <td>0.019093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lextra_Y[0]</th>\n",
       "      <td>0.017900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mextra_DX2[3]</th>\n",
       "      <td>0.016706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_cs[1]</th>\n",
       "      <td>0.016706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_Y[1]</th>\n",
       "      <td>0.016706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Err_Y[0]</th>\n",
       "      <td>0.015513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_cs[0]</th>\n",
       "      <td>0.014320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NErr_Y[0]</th>\n",
       "      <td>0.014320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_X[3]</th>\n",
       "      <td>0.013126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NErr_Y[1]</th>\n",
       "      <td>0.013126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mextra_DX2[0]</th>\n",
       "      <td>0.013126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_Y[3]</th>\n",
       "      <td>0.011933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ncl[1]</th>\n",
       "      <td>0.009547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_Y[0]</th>\n",
       "      <td>0.009547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mextra_DX2[2]</th>\n",
       "      <td>0.008353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_DX[3]</th>\n",
       "      <td>0.008353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_T[1]</th>\n",
       "      <td>0.002387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lextra_X[3]</th>\n",
       "      <td>0.002387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FOI_hits_N</th>\n",
       "      <td>0.002387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_TYPE[0]</th>\n",
       "      <td>0.002387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Err_X[0]</th>\n",
       "      <td>0.002387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Err_Y[3]</th>\n",
       "      <td>0.001193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Err_Y[2]</th>\n",
       "      <td>0.001193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_DY[3]</th>\n",
       "      <td>0.001193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mextra_DY2[1]</th>\n",
       "      <td>0.001193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lextra_Y[2]</th>\n",
       "      <td>0.001193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_DZ[0]</th>\n",
       "      <td>0.001193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_TYPE[3]</th>\n",
       "      <td>0.001193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lextra_X[2]</th>\n",
       "      <td>0.001193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Err_Z[0]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Err_Z[1]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_DZ[2]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Err_Z[2]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_DZ[1]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_DY[1]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lextra_Y[1]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_DZ[3]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_X[1]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mextra_DY2[2]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_DX[1]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ndof</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_TYPE[2]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_T[2]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_T[3]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_DX[0]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Err_Z[3]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       score\n",
       "ErrMSE              0.136038\n",
       "PT                  0.094272\n",
       "NShared             0.052506\n",
       "ncl[0]              0.040573\n",
       "DAngle[1]           0.039379\n",
       "DAngle[2]           0.035800\n",
       "P                   0.033413\n",
       "ncl[2]              0.031026\n",
       "NErr_X[3]           0.029833\n",
       "NErr_Y[3]           0.027446\n",
       "DAngle[3]           0.026253\n",
       "NErr_X[0]           0.023866\n",
       "NErr_X[1]           0.021480\n",
       "ncl[3]              0.019093\n",
       "DLL                 0.019093\n",
       "Lextra_Y[0]         0.017900\n",
       "Mextra_DX2[3]       0.016706\n",
       "avg_cs[1]           0.016706\n",
       "MatchedHit_Y[1]     0.016706\n",
       "Err_Y[0]            0.015513\n",
       "avg_cs[0]           0.014320\n",
       "NErr_Y[0]           0.014320\n",
       "MatchedHit_X[3]     0.013126\n",
       "NErr_Y[1]           0.013126\n",
       "Mextra_DX2[0]       0.013126\n",
       "MatchedHit_Y[3]     0.011933\n",
       "ncl[1]              0.009547\n",
       "MatchedHit_Y[0]     0.009547\n",
       "Mextra_DX2[2]       0.008353\n",
       "MatchedHit_DX[3]    0.008353\n",
       "...                      ...\n",
       "MatchedHit_T[1]     0.002387\n",
       "Lextra_X[3]         0.002387\n",
       "FOI_hits_N          0.002387\n",
       "MatchedHit_TYPE[0]  0.002387\n",
       "Err_X[0]            0.002387\n",
       "Err_Y[3]            0.001193\n",
       "Err_Y[2]            0.001193\n",
       "MatchedHit_DY[3]    0.001193\n",
       "Mextra_DY2[1]       0.001193\n",
       "Lextra_Y[2]         0.001193\n",
       "MatchedHit_DZ[0]    0.001193\n",
       "MatchedHit_TYPE[3]  0.001193\n",
       "Lextra_X[2]         0.001193\n",
       "Err_Z[0]            0.000000\n",
       "Err_Z[1]            0.000000\n",
       "MatchedHit_DZ[2]    0.000000\n",
       "Err_Z[2]            0.000000\n",
       "MatchedHit_DZ[1]    0.000000\n",
       "MatchedHit_DY[1]    0.000000\n",
       "Lextra_Y[1]         0.000000\n",
       "MatchedHit_DZ[3]    0.000000\n",
       "MatchedHit_X[1]     0.000000\n",
       "Mextra_DY2[2]       0.000000\n",
       "MatchedHit_DX[1]    0.000000\n",
       "ndof                0.000000\n",
       "MatchedHit_TYPE[2]  0.000000\n",
       "MatchedHit_T[2]     0.000000\n",
       "MatchedHit_T[3]     0.000000\n",
       "MatchedHit_DX[0]    0.000000\n",
       "Err_Z[3]            0.000000\n",
       "\n",
       "[86 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(global_feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_feature_importance = feature_importance.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = read_pub_test(used_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 24.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fit_predict_save(sample(train, 100000), test, \"out/09_importance_001_100.csv\", n_estimators=120, transformer_cls=DataTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fit_predict_save(sample(train, 1000000), test, \"out/09_plus_all_orig_features_1000_120.csv\", n_estimators=120, transformer_cls=DataTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fit_predict_save(sample(train, 1000000), test, \"out/09_plus_all_orig_features_1000_200.csv\", n_estimators=200, transformer_cls=DataTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fit_save_model(sample(train, 100000), \"models/07_dumb_cols.xgb\", n_estimators=120, transformer_cls=DataTransformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame([[1.0, np.NaN], [np.NaN, np.NaN], [2.0, 3.1]], columns=['a', 'b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.cosine import da_cols\n",
    "from transformers.err import err_cols, nerr_xy_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import repeat\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from common import (\n",
    "    SIMPLE_FEATURE_COLS, ARR_FEATURE_COLS, ALL_TRAIN_COLS,\n",
    "    xyz_cols, dxyz_cols, t_cols,\n",
    "    foi_ts_cols, unused_train_cols, train_cols, hit_stats_cols, ncl_cols, hit_type_cols\n",
    ")\n",
    "\n",
    "class DatasetMetaData:\n",
    "    def __init__(self, origin_csv_filenames, chunk_filenames_pattern, origin_col_set):\n",
    "        self.origin_csv_filenames = origin_csv_filenames\n",
    "        self.chunk_filenames_pattern = chunk_filenames_pattern\n",
    "        self.origin_col_set = origin_col_set\n",
    "        self.is_test = 'test_' in chunk_filenames_pattern\n",
    "\n",
    "meta_train = DatasetMetaData(\n",
    "    origin_csv_filenames=['data/train_part_1_v2.csv.gz', 'data/train_part_2_v2.csv.gz'],\n",
    "    chunk_filenames_pattern='data/train_{label}_{group}_{ind:03d}.pkl',\n",
    "    origin_col_set=SIMPLE_FEATURE_COLS + ARR_FEATURE_COLS + ALL_TRAIN_COLS\n",
    ")\n",
    "meta_pub_test = DatasetMetaData(\n",
    "    origin_csv_filenames=['data/test_public_v2.csv.gz'],\n",
    "    chunk_filenames_pattern='data/test_pub_{group}_{ind:03d}.pkl',\n",
    "    origin_col_set=SIMPLE_FEATURE_COLS + ARR_FEATURE_COLS\n",
    ")\n",
    "meta_pvt_test = DatasetMetaData(\n",
    "    origin_csv_filenames=[],\n",
    "    chunk_filenames_pattern='data/test_pvt_{group}_{ind:03d}.pkl',\n",
    "    origin_col_set=SIMPLE_FEATURE_COLS + ARR_FEATURE_COLS\n",
    ")\n",
    "\n",
    "col_groups = list(zip(['sf', 'af', 'tr'], [SIMPLE_FEATURE_COLS, ARR_FEATURE_COLS, ALL_TRAIN_COLS]))\n",
    "label_prefixes = ['L0', 'L1']\n",
    "\n",
    "\n",
    "class CsvDataReader:\n",
    "    int_dtype = np.int32\n",
    "    float_dtype = np.float32\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.na_values = ['-9999.0', '255']\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_read_stream(filenames, usecols, chunk_size=25000):\n",
    "        return CsvDataReader()._get_read_stream(filenames, usecols, chunk_size)\n",
    "    \n",
    "    def _get_read_stream(self, filenames, usecols, chunk_size):\n",
    "        if 'id' not in usecols:\n",
    "            usecols += ['id']\n",
    "            \n",
    "        for filename in filenames:\n",
    "            data_generator = pd.read_csv(\n",
    "                filename, usecols=usecols, chunksize=chunk_size, index_col='id', #nrows=400000,\n",
    "                na_values=self._get_na_values_dict(), keep_default_na=False,\n",
    "                converters=self._get_converters(), dtype=self._get_types()\n",
    "            )\n",
    "            for data in data_generator:\n",
    "                \n",
    "                yield data\n",
    "\n",
    "    def _get_na_values_dict(self):\n",
    "        float_cols = [(col, '-9999.0') for col in xyz_cols + dxyz_cols]\n",
    "        int_cols = [(col, '255') for col in t_cols]\n",
    "        return {k:v for k, v in float_cols+int_cols}\n",
    "\n",
    "    def _get_converters(self):\n",
    "        def parse_float_array(line):\n",
    "            arr = np.fromstring(line[1:-1], sep=\" \", dtype=self.float_dtype)\n",
    "            return arr\n",
    "\n",
    "        converters = dict(zip(ARR_FEATURE_COLS, repeat(parse_float_array)))\n",
    "        return converters\n",
    "    \n",
    "    def _get_types(self):\n",
    "        types = dict(zip(SIMPLE_FEATURE_COLS + ALL_TRAIN_COLS, repeat(self.float_dtype)))\n",
    "        for col in unused_train_cols[:1] + train_cols[:1] + hit_stats_cols + ncl_cols + hit_type_cols:\n",
    "            types[col] = self.int_dtype\n",
    "        types['id'] = self.int_dtype\n",
    "        return types\n",
    "\n",
    "\n",
    "class DataBuffer:\n",
    "    def __init__(self):\n",
    "        self._frames = []\n",
    "    \n",
    "    def append(self, frame):\n",
    "        self._frames.append(frame)\n",
    "    \n",
    "    def cut(self, nrows):\n",
    "        nrows = min(nrows, self.nrows)\n",
    "        merged = self._merge_frames()\n",
    "        head = merged.iloc[:nrows, :]\n",
    "        tail = merged.iloc[nrows:, :]\n",
    "        \n",
    "        self._frames = [tail]\n",
    "        return head, nrows\n",
    "    \n",
    "    def _merge_frames(self):\n",
    "        if len(self._frames) > 1:\n",
    "            merged = pd.concat(self._frames, axis=0, ignore_index=False)\n",
    "            self._frames = [merged]\n",
    "        return self._frames[0]\n",
    "    \n",
    "    @property\n",
    "    def nrows(self):\n",
    "        return sum([len(frame.index) for frame in self._frames])\n",
    "    \n",
    "    @property\n",
    "    def is_empty(self):\n",
    "        return self.nrows == 0\n",
    "\n",
    "\n",
    "class DataTank:\n",
    "    def __init__(self, max_volume, callback_on_full, early_stop=False):\n",
    "        self._max_volume = max_volume\n",
    "        self._buffer = DataBuffer()\n",
    "        self._on_full = callback_on_full\n",
    "        self._early_stop = early_stop\n",
    "    \n",
    "    def add(self, frame):\n",
    "        if frame is None:\n",
    "            return 0\n",
    "        \n",
    "        self._buffer.append(frame)\n",
    "        flushed, flushed_vol = False, 0\n",
    "        while self._is_full():\n",
    "            flushed_vol += self.flush()\n",
    "            flushed = True\n",
    "            if self._early_stop:\n",
    "                break\n",
    "        return flushed, flushed_vol\n",
    "        \n",
    "    def flush(self):\n",
    "        if self._buffer.is_empty:\n",
    "            return 0\n",
    "        flushed_data, flushed_vol = self._buffer.cut(self._max_volume)\n",
    "        self._on_full(flushed_data)\n",
    "        return flushed_vol\n",
    "    \n",
    "    def _is_full(self):\n",
    "        return self._buffer.nrows >= self._max_volume\n",
    "\n",
    "\n",
    "class TestDatasetHelper:\n",
    "    def __init__(self, filename_pattern):\n",
    "        self._filename_pattern = filename_pattern\n",
    "\n",
    "    def filter_frame(self, frame):\n",
    "        return frame\n",
    "        \n",
    "    def get_col_groups(self):\n",
    "        return col_groups[:-1]\n",
    "\n",
    "    def generate_chunk_filename(self, group_key, chunk_ind):\n",
    "        return self._filename_pattern.format(group=group_key, ind=chunk_ind)\n",
    "\n",
    "\n",
    "class TrainDatasetHelper:\n",
    "    def __init__(self, filename_pattern, label, label_key):\n",
    "        self._filename_pattern = filename_pattern\n",
    "        self._label = label\n",
    "        self._label_key = label_key\n",
    "\n",
    "    def filter_frame(self, frame):\n",
    "        return frame.loc[frame.label == self._label, :]\n",
    "\n",
    "    def get_col_groups(self):\n",
    "        return col_groups\n",
    "\n",
    "    def generate_chunk_filename(self, group_key, chunk_ind):\n",
    "        return self._filename_pattern.format(label=self._label_key, group=group_key, ind=chunk_ind)\n",
    "\n",
    "\n",
    "class PickleDataWriter:\n",
    "    def __init__(self, helper, chunk_size):\n",
    "        self._helper = helper\n",
    "        self._data_tank = DataTank(max_volume=chunk_size, callback_on_full=self._flush_chunk)\n",
    "        self._chunk_index = 0\n",
    "    \n",
    "    def store(self, frame):\n",
    "        filtered_frame = self._helper.filter_frame(frame)\n",
    "        return self._data_tank.add(filtered_frame)\n",
    "        \n",
    "    def flush(self):\n",
    "        return self._data_tank.flush()\n",
    "    \n",
    "    def _flush_chunk(self, chunk):\n",
    "        self._store_chunk(chunk, self._chunk_index)\n",
    "        self._chunk_index += 1\n",
    "        \n",
    "    def _store_chunk(self, chunk, chunk_index):\n",
    "        for group_key, col_group in self._helper.get_col_groups():\n",
    "            filename = self._helper.generate_chunk_filename(group_key, chunk_index)\n",
    "            chunk.loc[:, col_group].to_pickle(filename)\n",
    "            \n",
    "            if group_key == 'af':\n",
    "                filename = self._helper.generate_chunk_filename('afexp', chunk_index)\n",
    "                self._expand(chunk, col_group).to_pickle(filename)\n",
    "                \n",
    "    @staticmethod\n",
    "    def _expand(data, cols):\n",
    "        ids = np.repeat(data.index.values, data['FOI_hits_N'].values)\n",
    "        result = pd.DataFrame(data=ids, columns=['id'])\n",
    "        for col in cols:\n",
    "             result.loc[:, col] = np.hstack(data.loc[:, col])\n",
    "        return result\n",
    "    \n",
    "\n",
    "class PickleDataReader:\n",
    "    def __init__(self, helper, foi_expanded):\n",
    "        self._helper = helper\n",
    "        self._result = None\n",
    "        self._foi_result = None\n",
    "        self._foi_expanded = foi_expanded\n",
    "        \n",
    "    def read(self, nrows, cols):\n",
    "        data_tank = DataTank(nrows, self._set_read_result, early_stop=True)\n",
    "        foi_data_tank = DataTank(100000000, self._set_foi_read_result)\n",
    "        \n",
    "        for frame, foi_frame in self._read_chunks(cols):\n",
    "            foi_data_tank.add(foi_frame)\n",
    "            flushed, _ = data_tank.add(frame)\n",
    "            if flushed:\n",
    "                foi_data_tank.flush()\n",
    "                return self._result, self._foi_result\n",
    "\n",
    "        data_tank.flush()\n",
    "        foi_data_tank.flush()\n",
    "        return self._result, self._foi_result\n",
    "        \n",
    "    def _read_chunks(self, cols):\n",
    "        chunk_index = 0\n",
    "        while True:\n",
    "            frame, foi_frame = self._read_chunk(chunk_index, cols)\n",
    "            if frame is None:\n",
    "                break\n",
    "            \n",
    "            yield frame, foi_frame\n",
    "            chunk_index += 1\n",
    "            \n",
    "    def _set_read_result(self, data):\n",
    "        self._result = data\n",
    "        \n",
    "    def _set_foi_read_result(self, data):\n",
    "        self._foi_result = data\n",
    "    \n",
    "    def _read_chunk(self, chunk_index, cols):\n",
    "        chunk_parts = []\n",
    "        foi_dataframe = None        \n",
    "        for group_key, col_group in self._helper.get_col_groups():\n",
    "            cols_ = self._intersect_cols(cols, set(col_group))\n",
    "            if not cols_:\n",
    "                continue\n",
    "            \n",
    "            filename = self._helper.generate_chunk_filename(group_key, chunk_index)\n",
    "            if not os.path.exists(filename):\n",
    "                return None, None\n",
    "            \n",
    "            if group_key == 'af' and self._foi_expanded:\n",
    "                filename = self._helper.generate_chunk_filename('afexp', chunk_index)\n",
    "                foi_dataframe = pd.read_pickle(filename).loc[:, ['id'] + cols_]\n",
    "                continue\n",
    "            \n",
    "            chunk_part = pd.read_pickle(filename).loc[:, cols_]\n",
    "            chunk_parts.append(chunk_part)\n",
    "            \n",
    "        dataframe = pd.concat(chunk_parts, axis=1, sort=False)\n",
    "        return dataframe, foi_dataframe\n",
    "    \n",
    "    @staticmethod\n",
    "    def _intersect_cols(cols, col_subset):\n",
    "        return [col for col in cols if col in col_subset]\n",
    "\n",
    "\n",
    "class DatasetConverter:\n",
    "    def __init__(self):\n",
    "        self._stored = 0\n",
    "                \n",
    "    @staticmethod\n",
    "    def convert(data_set_meta: DatasetMetaData, chunk_size=50000):\n",
    "        dataframes_stream = CsvDataReader.get_read_stream(data_set_meta.origin_csv_filenames, data_set_meta.origin_col_set)\n",
    "        \n",
    "        filename_pattern = data_set_meta.chunk_filenames_pattern\n",
    "        if data_set_meta.is_test:\n",
    "            writers = [PickleDataWriter(TestDatasetHelper(filename_pattern), chunk_size)]\n",
    "        else: \n",
    "            writers = [PickleDataWriter(TrainDatasetHelper(filename_pattern, i, label_prefixes[i]), chunk_size) for i in range(2)]\n",
    "            \n",
    "        DatasetConverter()._store_chunkified(dataframes_stream, writers)\n",
    "        \n",
    "    def _store_chunkified(self, dataframes_stream, writers):\n",
    "        for data in dataframes_stream:\n",
    "            for writer in writers:\n",
    "                self._print_stored(writer.store(data))\n",
    "\n",
    "        for writer in writers:\n",
    "            self._print_stored(writer.flush())\n",
    "            \n",
    "    def _print_stored(self, stored):\n",
    "        if stored == 0:\n",
    "            return\n",
    "        self._stored += stored\n",
    "        if self._stored % 200000 == 0:\n",
    "            print('Stored: {0}M'.format(self._stored / 1000000.))\n",
    "\n",
    "\n",
    "class DatasetReader:\n",
    "    @staticmethod\n",
    "    def read_dataset(data_set_meta: DatasetMetaData, cols, nrows=None, prop_0=.5, foi_expanded=True):\n",
    "        nrows = nrows if nrows is not None else 100000000\n",
    "        filename_pattern = data_set_meta.chunk_filenames_pattern\n",
    "        if data_set_meta.is_test:\n",
    "            readers = [PickleDataReader(TestDatasetHelper(filename_pattern), foi_expanded=foi_expanded)]\n",
    "            proportions = [nrows]\n",
    "        else: \n",
    "            readers = [PickleDataReader(TrainDatasetHelper(filename_pattern, i, label_prefixes[i]), foi_expanded=foi_expanded) for i in range(2)]\n",
    "            nrows0 = int(nrows * prop_0)\n",
    "            proportions = [nrows0, nrows - nrows0]\n",
    "            \n",
    "        return DatasetReader()._read_dataset(readers, cols, proportions)\n",
    "            \n",
    "    def _read_dataset(self, readers, cols, proportions):\n",
    "        data_parts = []\n",
    "        foi_data_parts = []\n",
    "        delta = 0\n",
    "        col_delta = hit_stats_cols[:1] if hit_stats_cols[0] not in cols else []\n",
    "        for reader, nrows in zip(readers, proportions):\n",
    "            data_part, foi_data_part = reader.read(nrows + delta, cols + col_delta)\n",
    "            if col_delta:\n",
    "                data_part = data_part.drop(col_delta, axis=1)\n",
    "            data_parts.append(data_part)\n",
    "            if foi_data_part is not None:\n",
    "                ind = self._find_slice(foi_data_part.loc[:, 'id'].values, nrows + delta)\n",
    "                foi_data_part = foi_data_part.iloc[:ind, :]\n",
    "                foi_data_parts.append(foi_data_part)\n",
    "            delta = nrows - len(data_part.index)\n",
    "            \n",
    "        data = pd.concat(data_parts, axis=0, ignore_index=False)\n",
    "        foi_data = pd.concat(foi_data_parts, axis=0, ignore_index=True) if foi_data_parts else None\n",
    "        return data, foi_data\n",
    "    \n",
    "    @staticmethod\n",
    "    def _find_slice(data, n):\n",
    "        i, prev = 0, -1\n",
    "        while i < len(data):\n",
    "            if data[i] != prev:\n",
    "                prev, n = data[i], n-1\n",
    "            if n < 0:\n",
    "                break\n",
    "            i += 1\n",
    "        return i\n",
    "                \n",
    "def convert_train():\n",
    "    DatasetConverter.convert(meta_train)\n",
    "\n",
    "def convert_pub_test():\n",
    "    DatasetConverter.convert(meta_pub_test)\n",
    "    \n",
    "def convert_pvt_test():\n",
    "    DatasetConverter.convert(meta_pvt_test)\n",
    "    \n",
    "def read_train(cols, rows, foi_expanded=True):\n",
    "    return DatasetReader.read_dataset(meta_train, cols + train_cols, rows, foi_expanded=foi_expanded)\n",
    "\n",
    "def read_pub_test(cols, foi_expanded=True):\n",
    "    return DatasetReader.read_dataset(meta_pub_test, cols, foi_expanded=foi_expanded)\n",
    "\n",
    "def read_pvt_test(cols, foi_expanded=True):\n",
    "    return DatasetReader.read_dataset(meta_pvt_test, cols, foi_expanded=foi_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_foi_data, dt_foi = read_train(SIMPLE_FEATURE_COLS + ARR_FEATURE_COLS, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_closest_cols(cols):\n",
    "    return ['Cl_' + col for col in cols]\n",
    "\n",
    "cl_xyz_cols = to_closest_cols(xyz_cols)\n",
    "cl_t_cols = to_closest_cols(t_cols)\n",
    "cl_dxyz_cols = to_closest_cols(dxyz_cols)\n",
    "cl_cols = cl_xyz_cols + cl_t_cols + cl_dxyz_cols\n",
    "\n",
    "def get_closest(exy, foi_info, i):\n",
    "    hits = (foi_info[foi_ts_cols[2]] == i)\n",
    "    if not hits.any():\n",
    "        return None\n",
    "    dx = exy[exy_cols[i]] - foi_info.loc[hits, foi_xyz_cols[0]]\n",
    "    dy = exy[exy_cols[i+4]] - foi_info.loc[hits, foi_xyz_cols[1]]\n",
    "    dist2 = dx**2 + dy**2\n",
    "    j = np.argmin(dist2.values)\n",
    "    \n",
    "    row = foi_info.loc[hits, foi_xyz_cols + foi_ts_cols[:1] + foi_dxyz_cols].iloc[j, :]\n",
    "    return row\n",
    "\n",
    "def match_track_hits(exy, foi_info):\n",
    "    data = np.empty(len(cl_cols), dtype=np.float32)\n",
    "    for i in range(4):\n",
    "        hit = get_closest(exy, foi_info, i)\n",
    "        data[i::4] = hit.values if hit is not None else np.NaN\n",
    "    return data\n",
    "\n",
    "def fill_global_closest_matched_hits(data, data_foi, features):\n",
    "    matched_hits_data = np.vstack(\n",
    "        np.array([\n",
    "            match_track_hits(data.loc[i, exy_cols], data_foi.loc[data_foi.id == i, :])\n",
    "            for i in data.index\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    for i in range(len(cl_cols)):\n",
    "        data.loc[:, cl_cols[i]] = matched_hits_data[:, i]\n",
    "    features += cl_cols\n",
    "    return data\n",
    "\n",
    "def add_cl_hits(data, features):\n",
    "    for i in range(len(cl_cols)):\n",
    "        data.loc[:, cl_cols[i]] = global_foi_data[:, cl_cols[i]]\n",
    "    features += cl_cols\n",
    "    return data\n",
    "\n",
    "def add_cl_mse(data, features):\n",
    "    dxy = (data.loc[:, cl_xyz_cols[:8]].values - data.loc[:, exy_cols].values) / data.loc[:, cl_dxyz_cols[:8]].values / 2.\n",
    "    D = np.nanmean(dxy**2, axis=1)\n",
    "    \n",
    "    col = 'cl_' + err_cols[0]\n",
    "    data.loc[:, col] = D\n",
    "    features += [col]\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MatchedHit_X[0]</th>\n",
       "      <th>MatchedHit_X[1]</th>\n",
       "      <th>MatchedHit_X[2]</th>\n",
       "      <th>MatchedHit_X[3]</th>\n",
       "      <th>MatchedHit_Y[0]</th>\n",
       "      <th>MatchedHit_Y[1]</th>\n",
       "      <th>MatchedHit_Y[2]</th>\n",
       "      <th>MatchedHit_Y[3]</th>\n",
       "      <th>MatchedHit_Z[0]</th>\n",
       "      <th>MatchedHit_Z[1]</th>\n",
       "      <th>...</th>\n",
       "      <th>Cl_MatchedHit_DX[2]</th>\n",
       "      <th>Cl_MatchedHit_DX[3]</th>\n",
       "      <th>Cl_MatchedHit_DY[0]</th>\n",
       "      <th>Cl_MatchedHit_DY[1]</th>\n",
       "      <th>Cl_MatchedHit_DY[2]</th>\n",
       "      <th>Cl_MatchedHit_DY[3]</th>\n",
       "      <th>Cl_MatchedHit_DZ[0]</th>\n",
       "      <th>Cl_MatchedHit_DZ[1]</th>\n",
       "      <th>Cl_MatchedHit_DZ[2]</th>\n",
       "      <th>Cl_MatchedHit_DZ[3]</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-953.080017</td>\n",
       "      <td>-1028.400024</td>\n",
       "      <td>-1072.170044</td>\n",
       "      <td>-1145.569946</td>\n",
       "      <td>2164.736572</td>\n",
       "      <td>2332.150391</td>\n",
       "      <td>2500.885254</td>\n",
       "      <td>2671.054932</td>\n",
       "      <td>15410.896484</td>\n",
       "      <td>16615.699219</td>\n",
       "      <td>...</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>126.278549</td>\n",
       "      <td>136.278488</td>\n",
       "      <td>146.278412</td>\n",
       "      <td>156.278351</td>\n",
       "      <td>33.954948</td>\n",
       "      <td>33.990959</td>\n",
       "      <td>34.026966</td>\n",
       "      <td>34.062977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1935.099976</td>\n",
       "      <td>2140.560059</td>\n",
       "      <td>2310.600098</td>\n",
       "      <td>2467.820068</td>\n",
       "      <td>-1465.656616</td>\n",
       "      <td>-1580.002197</td>\n",
       "      <td>-1694.378418</td>\n",
       "      <td>-1809.069702</td>\n",
       "      <td>15399.722656</td>\n",
       "      <td>16606.611328</td>\n",
       "      <td>...</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>63.078957</td>\n",
       "      <td>68.078926</td>\n",
       "      <td>73.078896</td>\n",
       "      <td>78.078857</td>\n",
       "      <td>33.727364</td>\n",
       "      <td>33.745369</td>\n",
       "      <td>33.763374</td>\n",
       "      <td>33.781380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-62.421665</td>\n",
       "      <td>-178.475006</td>\n",
       "      <td>-464.503326</td>\n",
       "      <td>-1051.069946</td>\n",
       "      <td>-430.946503</td>\n",
       "      <td>-579.326843</td>\n",
       "      <td>-570.592285</td>\n",
       "      <td>-1062.020752</td>\n",
       "      <td>15401.548828</td>\n",
       "      <td>16404.212891</td>\n",
       "      <td>...</td>\n",
       "      <td>14.833333</td>\n",
       "      <td>31.500000</td>\n",
       "      <td>15.679264</td>\n",
       "      <td>33.979145</td>\n",
       "      <td>18.179249</td>\n",
       "      <td>38.979115</td>\n",
       "      <td>33.556679</td>\n",
       "      <td>33.622578</td>\n",
       "      <td>33.565681</td>\n",
       "      <td>33.640583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-181.130005</td>\n",
       "      <td>-123.625000</td>\n",
       "      <td>-23.803333</td>\n",
       "      <td>151.320007</td>\n",
       "      <td>-336.147125</td>\n",
       "      <td>-363.382050</td>\n",
       "      <td>-352.796204</td>\n",
       "      <td>-373.926514</td>\n",
       "      <td>15401.890625</td>\n",
       "      <td>16605.992188</td>\n",
       "      <td>...</td>\n",
       "      <td>14.833333</td>\n",
       "      <td>15.833333</td>\n",
       "      <td>15.679264</td>\n",
       "      <td>16.929256</td>\n",
       "      <td>18.179249</td>\n",
       "      <td>19.429239</td>\n",
       "      <td>33.556679</td>\n",
       "      <td>33.561180</td>\n",
       "      <td>33.565681</td>\n",
       "      <td>33.570183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>-255.330002</td>\n",
       "      <td>63.401669</td>\n",
       "      <td>109.533333</td>\n",
       "      <td>23.453333</td>\n",
       "      <td>420.254272</td>\n",
       "      <td>354.039093</td>\n",
       "      <td>306.612427</td>\n",
       "      <td>405.169739</td>\n",
       "      <td>15119.612305</td>\n",
       "      <td>16412.574219</td>\n",
       "      <td>...</td>\n",
       "      <td>29.500000</td>\n",
       "      <td>15.833333</td>\n",
       "      <td>15.679264</td>\n",
       "      <td>16.929256</td>\n",
       "      <td>36.479130</td>\n",
       "      <td>19.429239</td>\n",
       "      <td>33.556679</td>\n",
       "      <td>33.561180</td>\n",
       "      <td>33.631580</td>\n",
       "      <td>33.570183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    MatchedHit_X[0]  MatchedHit_X[1]  MatchedHit_X[2]  MatchedHit_X[3]  \\\n",
       "id                                                                       \n",
       "6       -953.080017     -1028.400024     -1072.170044     -1145.569946   \n",
       "19      1935.099976      2140.560059      2310.600098      2467.820068   \n",
       "43       -62.421665      -178.475006      -464.503326     -1051.069946   \n",
       "47      -181.130005      -123.625000       -23.803333       151.320007   \n",
       "51      -255.330002        63.401669       109.533333        23.453333   \n",
       "\n",
       "    MatchedHit_Y[0]  MatchedHit_Y[1]  MatchedHit_Y[2]  MatchedHit_Y[3]  \\\n",
       "id                                                                       \n",
       "6       2164.736572      2332.150391      2500.885254      2671.054932   \n",
       "19     -1465.656616     -1580.002197     -1694.378418     -1809.069702   \n",
       "43      -430.946503      -579.326843      -570.592285     -1062.020752   \n",
       "47      -336.147125      -363.382050      -352.796204      -373.926514   \n",
       "51       420.254272       354.039093       306.612427       405.169739   \n",
       "\n",
       "    MatchedHit_Z[0]  MatchedHit_Z[1]  ...  Cl_MatchedHit_DX[2]  \\\n",
       "id                                    ...                        \n",
       "6      15410.896484     16615.699219  ...           118.000000   \n",
       "19     15399.722656     16606.611328  ...            59.000000   \n",
       "43     15401.548828     16404.212891  ...            14.833333   \n",
       "47     15401.890625     16605.992188  ...            14.833333   \n",
       "51     15119.612305     16412.574219  ...            29.500000   \n",
       "\n",
       "    Cl_MatchedHit_DX[3]  Cl_MatchedHit_DY[0]  Cl_MatchedHit_DY[1]  \\\n",
       "id                                                                  \n",
       "6            126.000000           126.278549           136.278488   \n",
       "19            63.000000            63.078957            68.078926   \n",
       "43            31.500000            15.679264            33.979145   \n",
       "47            15.833333            15.679264            16.929256   \n",
       "51            15.833333            15.679264            16.929256   \n",
       "\n",
       "    Cl_MatchedHit_DY[2]  Cl_MatchedHit_DY[3]  Cl_MatchedHit_DZ[0]  \\\n",
       "id                                                                  \n",
       "6            146.278412           156.278351            33.954948   \n",
       "19            73.078896            78.078857            33.727364   \n",
       "43            18.179249            38.979115            33.556679   \n",
       "47            18.179249            19.429239            33.556679   \n",
       "51            36.479130            19.429239            33.556679   \n",
       "\n",
       "    Cl_MatchedHit_DZ[1]  Cl_MatchedHit_DZ[2]  Cl_MatchedHit_DZ[3]  \n",
       "id                                                                 \n",
       "6             33.990959            34.026966            34.062977  \n",
       "19            33.745369            33.763374            33.781380  \n",
       "43            33.622578            33.565681            33.640583  \n",
       "47            33.561180            33.565681            33.570183  \n",
       "51            33.561180            33.631580            33.570183  \n",
       "\n",
       "[5 rows x 96 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def to_closest_cols(cols):\n",
    "    return ['Cl_' + col for col in cols]\n",
    "\n",
    "cl_xyz_cols = to_closest_cols(xyz_cols)\n",
    "cl_t_cols = to_closest_cols(t_cols)\n",
    "cl_dxyz_cols = to_closest_cols(dxyz_cols)\n",
    "cl_cols = cl_xyz_cols + cl_t_cols + cl_dxyz_cols\n",
    "\n",
    "def match_track_hits(exy, foi_info):\n",
    "    data = np.empty(len(cl_cols), dtype=np.float32)\n",
    "    for i in range(4):\n",
    "        hit = get_closest(exy, foi_info, i)\n",
    "        data[i::4] = hit.values if hit is not None else np.NaN\n",
    "    return data\n",
    "\n",
    "def fill_global_closest_matched_hits2(data, data_foi, features):\n",
    "    data.loc[:, 'id'] = data.index.values\n",
    "    exy = data.loc[:, exy_cols]\n",
    "    data_foi = data_foi.astype({foi_ts_cols[2]: np.int16})\n",
    "    join = data_foi.join(exy, on='id', how='inner')\n",
    "    for i in range(4):\n",
    "        mask = join[foi_ts_cols[2]] == i\n",
    "        slc = join.loc[mask, ['id', foi_ts_cols[2], exy_cols[i], exy_cols[i+4], foi_xyz_cols[0], foi_xyz_cols[1]]]\n",
    "        dx = slc.loc[:, exy_cols[i]].values - slc.loc[:, foi_xyz_cols[0]].values\n",
    "        dy = slc.loc[:, exy_cols[i+4]].values - slc.loc[:, foi_xyz_cols[1]].values\n",
    "        join.loc[mask, 'D2'] = dx**2 + dy**2\n",
    "        \n",
    "    res = join.sort_values(by=['id', foi_ts_cols[2], 'D2'])\n",
    "    res = res.drop_duplicates(subset=['id', foi_ts_cols[2]])\n",
    "        \n",
    "    subcols = foi_xyz_cols + foi_ts_cols[:1] + foi_dxyz_cols\n",
    "    nsubcols = len(subcols)\n",
    "    for i, col in enumerate(cl_cols):\n",
    "        mask = res[foi_ts_cols[2]] == (i % 4)\n",
    "        indices = res.loc[mask, 'id']\n",
    "        data.loc[indices, col] = res.loc[mask, subcols[i // 4]].values\n",
    "    features += cl_cols\n",
    "    return data\n",
    "\n",
    "dt = global_foi_data.copy()\n",
    "dt = fill_global_closest_matched_hits2(dt, dt_foi, [])\n",
    "display(dt.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.06 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100, 95)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# dt = add_mse(dt, [])\n",
    "fill_global_closest_matched_hits(global_foi_data.copy(), dt_foi, []).shape\n",
    "# dt = add_cl_mse(dt, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ErrMSE</th>\n",
       "      <th>cl_ErrMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.124394</td>\n",
       "      <td>6.188394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.307621</td>\n",
       "      <td>12.719624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.009950</td>\n",
       "      <td>0.009950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.168854</td>\n",
       "      <td>0.159092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.582505</td>\n",
       "      <td>0.525479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.019982</td>\n",
       "      <td>5.274997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>99.498993</td>\n",
       "      <td>97.103577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ErrMSE    cl_ErrMSE\n",
       "count  1000.000000  1000.000000\n",
       "mean      7.124394     6.188394\n",
       "std      14.307621    12.719624\n",
       "min       0.009950     0.009950\n",
       "25%       0.168854     0.159092\n",
       "50%       0.582505     0.525479\n",
       "75%       6.019982     5.274997\n",
       "max      99.498993    97.103577"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.loc[:, [err_cols[0], 'cl_' + err_cols[0]]].describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
