{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABUkAAABECAYAAAC8urRAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAA4JJREFUeJzt2zFLVWEcx/H/DYmgSYoCc7Bw0KDFcyVoCaLNNxAS1FBRSw0NTYEUBA0NtRTVEoT0BtoqaAlCj0uQDlIOt6AonAKJ4PYG1Kmnp+7/8xnPs/wOHM7whafT7/cDAAAAACCrHbUHAAAAAADUJJICAAAAAKmJpAAAAABAaiIpAAAAAJCaSAoAAAAApDa03WHbtv2/NQQAAAAAoKSmaTqbPd82kkZEzM33/vyaf8Tc7GjcXn1ce0Yx18bPxcaNW7VnFPFy/GzMzI7E8/nPtacUMTM7EvH9We0Z5ew5FQ++bPpPGggX9/fj8NMXtWcU8/70yVhfOF97RjHD04/iZm8w3+/66KO4tLZSe0Yx98cm4sryWO0ZxdydXIsDDydqzyjm04WV6F5tas8oZvFOG0cPXq49o4i3H+/FaPd47RnF9BZfx6sz+2rPKObEk6/RbHRrzyim3bUY3d0D/G/50cabn4P5fsd2trEw/qH2jGKmVw9F826p9oxi2iNTMbQ+mN9mRMSv4TaaqeXaM4pplyajt3ek9owiRr9t3ZFctwcAAAAAUhNJAQAAAIDURFIAAAAAIDWRFAAAAABITSQFAAAAAFITSQEAAACA1ERSAAAAACA1kRQAAAAASE0kBQAAAABSE0kBAAAAgNREUgAAAAAgNZEUAAAAAEhNJAUAAAAAUhNJAQAAAIDURFIAAAAAIDWRFAAAAABITSQFAAAAAFITSQEAAACA1ERSAAAAACA1kRQAAAAASE0kBQAAAABSE0kBAAAAgNREUgAAAAAgNZEUAAAAAEhNJAUAAAAAUhNJAQAAAIDURFIAAAAAIDWRFAAAAABITSQFAAAAAFITSQEAAACA1ERSAAAAACA1kRQAAAAASE0kBQAAAABSE0kBAAAAgNREUgAAAAAgNZEUAAAAAEhNJAUAAAAAUhNJAQAAAIDURFIAAAAAIDWRFAAAAABITSQFAAAAAFITSQEAAACA1ERSAAAAACA1kRQAAAAASE0kBQAAAABSE0kBAAAAgNREUgAAAAAgNZEUAAAAAEhNJAUAAAAAUhNJAQAAAIDURFIAAAAAIDWRFAAAAABITSQFAAAAAFLr9Pv9LQ/btt36EAAAAADgP9I0TWez59tGUgAAAACAQee6PQAAAACQmkgKAAAAAKQmkgIAAAAAqYmkAAAAAEBqIikAAAAAkNpvuTVZgasyZKkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1728x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as sts\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from common import *\n",
    "import warnings\n",
    "from io_tools import (\n",
    "    read_train, read_pub_test, read_pvt_test,\n",
    "    convert_train, convert_pub_test, convert_pvt_test,\n",
    ")\n",
    "from pipeline import (\n",
    "    split_classes, count_classes, sample,\n",
    "    cross_validate, fit_predict_save, fit_save_model\n",
    ")\n",
    "from transformers.pca import pca_fit, pca_transform\n",
    "from transformers.cosine import add_coses, to_degrees\n",
    "from transformers.momentum import add_is_muon, add_is_muon_tight, add_probability_hit_detector\n",
    "from transformers.err import add_mse, add_normed_err, err_cols, add_errs, create_distr, get_dll_pdf, get_dll_cdf\n",
    "\n",
    "sns.set()\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "color_palette = sns.color_palette('deep') + sns.color_palette('husl', 6) + sns.color_palette('bright') + sns.color_palette('pastel')\n",
    "\n",
    "%matplotlib inline\n",
    "sns.palplot(color_palette)\n",
    "\n",
    "def ndprint(a, precision=3):\n",
    "    with np.printoptions(precision=precision, suppress=True):\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 63)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(421218, 578782)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "used_cols = xyz_cols + mom_cols + hit_type_cols + dxyz_cols + exy_cols + edxy_cols + hit_stats_cols + t_cols + ncl_cols + avg_cs_cols + foi_cols\n",
    "global_feature_importance = None\n",
    "train, train_foi = read_train(used_cols, 1000000)\n",
    "display(train.shape, count_classes(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Cl_MatchedHit_X[0]', 'Cl_MatchedHit_X[1]', 'Cl_MatchedHit_X[2]',\n",
      "       'Cl_MatchedHit_X[3]', 'Cl_MatchedHit_Y[0]', 'Cl_MatchedHit_Y[1]',\n",
      "       'Cl_MatchedHit_Y[2]', 'Cl_MatchedHit_Y[3]', 'Cl_MatchedHit_Z[0]',\n",
      "       'Cl_MatchedHit_Z[1]', 'Cl_MatchedHit_Z[2]', 'Cl_MatchedHit_Z[3]',\n",
      "       'Cl_MatchedHit_T[0]', 'Cl_MatchedHit_T[1]', 'Cl_MatchedHit_T[2]',\n",
      "       'Cl_MatchedHit_T[3]', 'Cl_MatchedHit_DX[0]', 'Cl_MatchedHit_DX[1]',\n",
      "       'Cl_MatchedHit_DX[2]', 'Cl_MatchedHit_DX[3]', 'Cl_MatchedHit_DY[0]',\n",
      "       'Cl_MatchedHit_DY[1]', 'Cl_MatchedHit_DY[2]', 'Cl_MatchedHit_DY[3]',\n",
      "       'Cl_MatchedHit_DZ[0]', 'Cl_MatchedHit_DZ[1]', 'Cl_MatchedHit_DZ[2]',\n",
      "       'Cl_MatchedHit_DZ[3]'],\n",
      "      dtype='object')\n",
      "Wall time: 3.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "closest_hits_filename = 'data/train_closest_hits_replaced.pkl'\n",
    "global_foi_train_data = pd.read_pickle(closest_hits_filename).loc[:, xyz_cols + t_cols + dxyz_cols].copy()\n",
    "global_foi_train_data.columns = cl_cols\n",
    "print(global_foi_train_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transformer\n",
    "\n",
    "Это по сути основная часть. Класс, который отбирает нужные столбцы, возможно что-то модифицирует или добавляет. На выходе - входные данные для модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# готовим данные для распределения DLL\n",
    "\n",
    "# либо загружаем уже готовое\n",
    "closest_hits_filename = 'data/train_closest_hits_replaced.pkl'\n",
    "# dll_train = pd.read_pickle(closest_hits_filename)\n",
    "# display(dll_train.columns)\n",
    "\n",
    "# либо считаем заново\n",
    "dll_train, _ = read_train(xy_cols + dx_cols + dy_cols + exy_cols, 10000000)\n",
    "dll_train = add_mse(dll_train, [])\n",
    "\n",
    "# опционально пересчитываем MatchedHits и заменяем ими координаты треков в dll_train, чтобы считать распределение на пересчитанных треках\n",
    "# dll_train = replace_hits(dll_train, [])\n",
    "\n",
    "# save DLL\n",
    "# display(dll_train.columns)\n",
    "# dll_train.to_pickle(closest_hits_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# если распределение для DLL уже есть сохраненное, то загружаем\n",
    "# cdfs, pdfs, bins = np.load('data/train_cdfs.pkl.npy'), np.load('data/train_pdfs.pkl.npy'), np.load('data/train_bins.pkl.npy')\n",
    "\n",
    "# либо считаем на основе загруженного dll_train\n",
    "cdfs, pdfs, bins = create_distr(dll_train)\n",
    "# np.save('data/train_cdfs.pkl.npy', cdfs)\n",
    "# np.save('data/train_pdfs.pkl.npy', pdfs)\n",
    "# np.save('data/train_bins.pkl.npy', bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_cols = ['V', 'VT', 'M']\n",
    "\n",
    "def add_velocity(data, features):\n",
    "    def get_layer_coords(data, cols, i):\n",
    "        return data[[cols[i], cols[i+4], cols[i+8]]].values\n",
    "    def dot(x, y):\n",
    "        return np.sum(x * y, axis=1, dtype=np.float32)\n",
    "    def norm(x):\n",
    "        return np.sqrt(dot(x, x))\n",
    "    \n",
    "    avg_vel = np.nanmean(data.loc[:, z_cols].values / data.loc[:, t_cols].values, axis=1)\n",
    "    data.loc[:, vm_cols[0]] = avg_vel\n",
    "    data.loc[:, vm_cols[2]] = data.loc[:, mom_cols[0]].values / avg_vel\n",
    "    features += [vm_cols[0], vm_cols[2]]\n",
    "    return data\n",
    "    \n",
    "    \n",
    "# dt = add_velocity(train.copy(), [])\n",
    "# dt.loc[:, 'V'].plot()\n",
    "# dt.loc[:, t_cols[0]].hist()\n",
    "\n",
    "# i = 2\n",
    "# plt.hist(np.nan_to_num(np.clip(dt.loc[:, z_cols[i]].values / dt.loc[:, t_cols[i]].values, 0, 1e+10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dll(data, features):\n",
    "    data[err_cols[1]] = get_dll_pdf(data.loc[:, err_cols[0]], pdfs, cdfs, bins)\n",
    "    features += err_cols[1:2]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(train, n_estimators, transformer_cls):\n",
    "    labels, weights = get_labels_weights(train.loc[:, train_cols])\n",
    "\n",
    "    # defined much later\n",
    "    transformer = transformer_cls().fit(train)\n",
    "    train_values = transformer.transform(train)\n",
    "    \n",
    "    estimator = xgb.XGBClassifier(n_estimators=n_estimators, n_jobs=3)\n",
    "    estimator.fit(train_values, labels, eval_metric=scoring.rejection90_sklearn) #, sample_weight=weights)\n",
    "    return transformer, estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FOI_hits_X</th>\n",
       "      <th>FOI_hits_Y</th>\n",
       "      <th>FOI_hits_DX</th>\n",
       "      <th>FOI_hits_DY</th>\n",
       "      <th>FOI_hits_T</th>\n",
       "      <th>FOI_hits_S</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3376.969971</td>\n",
       "      <td>1590.397461</td>\n",
       "      <td>118.0</td>\n",
       "      <td>146.278412</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3140.969971</td>\n",
       "      <td>1590.397461</td>\n",
       "      <td>118.0</td>\n",
       "      <td>146.278412</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>827.000000</td>\n",
       "      <td>2764.196533</td>\n",
       "      <td>118.0</td>\n",
       "      <td>146.278412</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1933.969971</td>\n",
       "      <td>-221.993256</td>\n",
       "      <td>59.0</td>\n",
       "      <td>73.078896</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1933.969971</td>\n",
       "      <td>-221.993256</td>\n",
       "      <td>59.0</td>\n",
       "      <td>73.078896</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1815.969971</td>\n",
       "      <td>1089.198242</td>\n",
       "      <td>59.0</td>\n",
       "      <td>73.078896</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     FOI_hits_X   FOI_hits_Y  FOI_hits_DX  FOI_hits_DY  FOI_hits_T  FOI_hits_S\n",
       "id                                                                            \n",
       "0  -3376.969971  1590.397461        118.0   146.278412         7.0         2.0\n",
       "0  -3140.969971  1590.397461        118.0   146.278412         7.0         2.0\n",
       "1    827.000000  2764.196533        118.0   146.278412         8.0         2.0\n",
       "2  -1933.969971  -221.993256         59.0    73.078896         3.0         2.0\n",
       "3  -1933.969971  -221.993256         59.0    73.078896         8.0         2.0\n",
       "4  -1815.969971  1089.198242         59.0    73.078896         5.0         2.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# здесь пока хуй пойми что :)\n",
    "\n",
    "hc_preimage_cols = xyz_cols + t_cols + dxyz_cols\n",
    "hc_image_cols = foi_xyz_cols + foi_ts_cols[:1] + foi_dxyz_cols\n",
    "seg_col = foi_ts_cols[2]\n",
    "nrows = 10\n",
    "train, train_foi = read_train(used_cols, nrows)\n",
    "\n",
    "# по идее должен заполнять датафрейм ближайшими хитами\n",
    "def fill_global_closest_matched_hits(data, data_foi):\n",
    "#     data_foi.astype({seg_col: np.int8}, copy=False)\n",
    "    cur_foi_cols = foi_xyz_cols[:2] + foi_dxyz_cols[:2] + foi_ts_cols[:1] + foi_ts_cols[-1:]\n",
    "    data_foi = data_foi.loc[:, ['id'] + cur_foi_cols]\n",
    "    data_foi.set_index('id', inplace=True)\n",
    "    \n",
    "    segs = [data_foi.loc[data_foi[seg_col] == i, :] for i in range(4)]\n",
    "    display(segs[2])\n",
    "    \n",
    "#     data_foi.set_index(['id', seg_col], inplace=True)\n",
    "    \n",
    "#     join = data_foi.join(data.loc[:, exy_cols], how='right')\n",
    "#     s0 = join.loc[(slice(None), 0), :]\n",
    "#     s0 = s0.set_index(s0.index.droplevel(1))\n",
    "#     s1 = join.loc[(slice(None), 1), :]\n",
    "#     s1 = s1.set_index(s1.index.droplevel(1))\n",
    "#     display(s0.join(s1, how='outer', lsuffix='_0', rsuffix='_1'))\n",
    "#     display(s0.set_index(['id']).join(s1.set_index(['id']), how='inner', lsuffix='_0', rsuffix='_1'))\n",
    "#     for i in range(4):\n",
    "#         join.loc[(slice(None), i), :]\n",
    "#         slc = join.loc[mask, ['id', foi_ts_cols[2], exy_cols[i], exy_cols[i+4], foi_xyz_cols[0], foi_xyz_cols[1]]]\n",
    "#         dx = slc.loc[:, exy_cols[i]].values - slc.loc[:, foi_xyz_cols[0]].values\n",
    "#         dy = slc.loc[:, exy_cols[i+4]].values - slc.loc[:, foi_xyz_cols[1]].values\n",
    "#         join.loc[mask, 'D2'] = dx**2 + dy**2\n",
    "        \n",
    "#     res = join.sort_values(by=['id', foi_ts_cols[2], 'D2'])\n",
    "#     res = res.drop_duplicates(subset=['id', foi_ts_cols[2]])\n",
    "        \n",
    "#     subcols = hc_image_cols\n",
    "#     nsubcols = len(subcols)\n",
    "#     for i, col in enumerate(cl_cols):\n",
    "#         mask = res[foi_ts_cols[2]] == (i % 4)\n",
    "#         indices = res.loc[mask, 'id']\n",
    "#         data.loc[indices, col] = res.loc[mask, subcols[i // 4]].values\n",
    "    return data\n",
    "\n",
    "global_foi_train_data = fill_global_closest_matched_hits(train, train_foi)\n",
    "# df_scores, feature_importance = cross_validate(train, n_estimators=20, n_splits=3, n_rows=nrows, transformer_cls=DataTransformer)\n",
    "# display(df_scores.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'add_cl_hits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\idao2019\\pipeline.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(train, n_estimators, n_splits, n_rows, transformer_cls)\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mtest_subset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[0mfit_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_subset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransformer_cls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_labels_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_subset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_cols\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\dev\\idao2019\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(train, n_estimators, transformer_cls)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# defined much later\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mtransformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer_cls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mtrain_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, data)\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'add_cl_hits' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "def filter_unimportant_features(features):\n",
    "    if global_feature_importance is None:\n",
    "        return features\n",
    "    fscore = global_feature_importance\n",
    "    return [col for col in features if col not in fscore.index or fscore.loc[col, 'score'] > 0.01]\n",
    "    return features\n",
    "\n",
    "def replace_hits(data, features):\n",
    "    global_foi_data = global_foi_train_data if train_cols[0] in data.columns else global_foi_test_data\n",
    "    for col in cl_cols:\n",
    "        data.loc[:, col[3:]] = global_foi_data.loc[data.index.values, col].values\n",
    "    return data\n",
    "\n",
    "class DataTransformer(TransformerMixin):\n",
    "    def __init__(self, *featurizers):\n",
    "        self.featurizers = featurizers\n",
    "    \n",
    "    def fit(self, data, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        data = data.copy()\n",
    "        features = [] + mom_cols + hit_type_cols + dxyz_cols + exy_cols + edxy_cols + hit_stats_cols + t_cols + ncl_cols + avg_cs_cols + xyz_cols\n",
    "        features = filter_unimportant_features(features)\n",
    "        self.origin_features = features.copy()\n",
    "\n",
    "#         data = replace_hits(data, features)\n",
    "        add_coses(data, features)\n",
    "        add_mse(data, features)\n",
    "        add_normed_err(data, features)\n",
    "        add_dll(data, features)\n",
    "        add_errs(data, features)\n",
    "        add_velocity(data, features)\n",
    "        add_cl_hits(data, features)\n",
    "        add_cl_mse(data, features)\n",
    "        \n",
    "#         filter_data(data)\n",
    "        if not features:\n",
    "            raise('no features')\n",
    "    \n",
    "        features = filter_unimportant_features(features)\n",
    "        self.new_features = features[len(self.origin_features):]\n",
    "        self.features = self.origin_features + self.new_features\n",
    "        return data[features].values\n",
    "\n",
    "df_scores, feature_importance = cross_validate(train, n_estimators=60, n_splits=3, n_rows=10000, transformer_cls=DataTransformer)\n",
    "display(df_scores.describe())\n",
    "# display(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>scr</th>\n",
       "      <th>th</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.791080</td>\n",
       "      <td>0.783935</td>\n",
       "      <td>0.881091</td>\n",
       "      <td>0.829675</td>\n",
       "      <td>0.823848</td>\n",
       "      <td>0.792446</td>\n",
       "      <td>0.090231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.002347</td>\n",
       "      <td>0.002945</td>\n",
       "      <td>0.001817</td>\n",
       "      <td>0.001631</td>\n",
       "      <td>0.004411</td>\n",
       "      <td>0.029143</td>\n",
       "      <td>0.001135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.789150</td>\n",
       "      <td>0.780936</td>\n",
       "      <td>0.878355</td>\n",
       "      <td>0.828575</td>\n",
       "      <td>0.819903</td>\n",
       "      <td>0.750774</td>\n",
       "      <td>0.089119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.789550</td>\n",
       "      <td>0.782281</td>\n",
       "      <td>0.880693</td>\n",
       "      <td>0.828584</td>\n",
       "      <td>0.821399</td>\n",
       "      <td>0.775989</td>\n",
       "      <td>0.089302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.790200</td>\n",
       "      <td>0.783514</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>0.828637</td>\n",
       "      <td>0.823087</td>\n",
       "      <td>0.799482</td>\n",
       "      <td>0.089898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.791550</td>\n",
       "      <td>0.784246</td>\n",
       "      <td>0.882424</td>\n",
       "      <td>0.830301</td>\n",
       "      <td>0.823549</td>\n",
       "      <td>0.813800</td>\n",
       "      <td>0.091168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.794950</td>\n",
       "      <td>0.788699</td>\n",
       "      <td>0.883030</td>\n",
       "      <td>0.832277</td>\n",
       "      <td>0.831305</td>\n",
       "      <td>0.822186</td>\n",
       "      <td>0.091669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            acc      prec       rec        f1   roc_auc       scr        th\n",
       "count  5.000000  5.000000  5.000000  5.000000  5.000000  5.000000  5.000000\n",
       "mean   0.791080  0.783935  0.881091  0.829675  0.823848  0.792446  0.090231\n",
       "std    0.002347  0.002945  0.001817  0.001631  0.004411  0.029143  0.001135\n",
       "min    0.789150  0.780936  0.878355  0.828575  0.819903  0.750774  0.089119\n",
       "25%    0.789550  0.782281  0.880693  0.828584  0.821399  0.775989  0.089302\n",
       "50%    0.790200  0.783514  0.880952  0.828637  0.823087  0.799482  0.089898\n",
       "75%    0.791550  0.784246  0.882424  0.830301  0.823549  0.813800  0.091168\n",
       "max    0.794950  0.788699  0.883030  0.832277  0.831305  0.822186  0.091669"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "global_feature_importance = None\n",
    "df_scores, feature_importance = cross_validate(train, n_estimators=120, n_splits=5, n_rows=100000, transformer_cls=DataTransformer)\n",
    "display(df_scores.describe())\n",
    "# display(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_feature_importance = feature_importance.copy()\n",
    "sum(global_feature_importance.score > .01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ErrMSE</th>\n",
       "      <td>0.137560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PT</th>\n",
       "      <td>0.077751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NShared</th>\n",
       "      <td>0.050239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ncl[2]</th>\n",
       "      <td>0.040670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DAngle[1]</th>\n",
       "      <td>0.035885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DAngle[2]</th>\n",
       "      <td>0.033493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NErr_X[3]</th>\n",
       "      <td>0.029904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DAngle[3]</th>\n",
       "      <td>0.028708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ncl[0]</th>\n",
       "      <td>0.028708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ncl[3]</th>\n",
       "      <td>0.027512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_Y[1]</th>\n",
       "      <td>0.026316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P</th>\n",
       "      <td>0.025120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NErr_X[1]</th>\n",
       "      <td>0.023923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NErr_Y[2]</th>\n",
       "      <td>0.023923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NErr_Y[0]</th>\n",
       "      <td>0.022727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M</th>\n",
       "      <td>0.021531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NErr_X[0]</th>\n",
       "      <td>0.021531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NErr_Y[3]</th>\n",
       "      <td>0.021531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_cs[0]</th>\n",
       "      <td>0.017943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mextra_DX2[3]</th>\n",
       "      <td>0.016746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lextra_X[3]</th>\n",
       "      <td>0.014354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lextra_Y[0]</th>\n",
       "      <td>0.013158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mextra_DY2[3]</th>\n",
       "      <td>0.013158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Err_Y[0]</th>\n",
       "      <td>0.011962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_X[3]</th>\n",
       "      <td>0.011962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_Y[2]</th>\n",
       "      <td>0.010766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_cs[1]</th>\n",
       "      <td>0.010766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_TYPE[1]</th>\n",
       "      <td>0.009569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DLL</th>\n",
       "      <td>0.009569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NErr_Y[1]</th>\n",
       "      <td>0.009569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_DX[3]</th>\n",
       "      <td>0.003589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lextra_X[2]</th>\n",
       "      <td>0.003589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ncl[1]</th>\n",
       "      <td>0.002392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mextra_DY2[2]</th>\n",
       "      <td>0.002392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_X[1]</th>\n",
       "      <td>0.002392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_DZ[0]</th>\n",
       "      <td>0.002392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_DX[2]</th>\n",
       "      <td>0.002392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Err_Y[2]</th>\n",
       "      <td>0.001196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_cs[3]</th>\n",
       "      <td>0.001196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Err_Z[2]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Err_Z[3]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Err_Z[0]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_DZ[1]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Err_Y[1]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_DZ[2]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_DZ[3]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Err_Z[1]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_DX[1]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lextra_Y[1]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lextra_Y[2]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_DY[3]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mextra_DX2[1]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_DY[1]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ndof</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_T[2]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_DY[0]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_TYPE[2]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_TYPE[3]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_DX[0]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lextra_Y[3]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       score\n",
       "ErrMSE              0.137560\n",
       "PT                  0.077751\n",
       "NShared             0.050239\n",
       "ncl[2]              0.040670\n",
       "DAngle[1]           0.035885\n",
       "DAngle[2]           0.033493\n",
       "NErr_X[3]           0.029904\n",
       "DAngle[3]           0.028708\n",
       "ncl[0]              0.028708\n",
       "ncl[3]              0.027512\n",
       "MatchedHit_Y[1]     0.026316\n",
       "P                   0.025120\n",
       "NErr_X[1]           0.023923\n",
       "NErr_Y[2]           0.023923\n",
       "NErr_Y[0]           0.022727\n",
       "M                   0.021531\n",
       "NErr_X[0]           0.021531\n",
       "NErr_Y[3]           0.021531\n",
       "avg_cs[0]           0.017943\n",
       "Mextra_DX2[3]       0.016746\n",
       "Lextra_X[3]         0.014354\n",
       "Lextra_Y[0]         0.013158\n",
       "Mextra_DY2[3]       0.013158\n",
       "Err_Y[0]            0.011962\n",
       "MatchedHit_X[3]     0.011962\n",
       "MatchedHit_Y[2]     0.010766\n",
       "avg_cs[1]           0.010766\n",
       "MatchedHit_TYPE[1]  0.009569\n",
       "DLL                 0.009569\n",
       "NErr_Y[1]           0.009569\n",
       "...                      ...\n",
       "MatchedHit_DX[3]    0.003589\n",
       "Lextra_X[2]         0.003589\n",
       "ncl[1]              0.002392\n",
       "Mextra_DY2[2]       0.002392\n",
       "MatchedHit_X[1]     0.002392\n",
       "MatchedHit_DZ[0]    0.002392\n",
       "MatchedHit_DX[2]    0.002392\n",
       "Err_Y[2]            0.001196\n",
       "avg_cs[3]           0.001196\n",
       "Err_Z[2]            0.000000\n",
       "Err_Z[3]            0.000000\n",
       "Err_Z[0]            0.000000\n",
       "MatchedHit_DZ[1]    0.000000\n",
       "Err_Y[1]            0.000000\n",
       "MatchedHit_DZ[2]    0.000000\n",
       "MatchedHit_DZ[3]    0.000000\n",
       "Err_Z[1]            0.000000\n",
       "MatchedHit_DX[1]    0.000000\n",
       "Lextra_Y[1]         0.000000\n",
       "Lextra_Y[2]         0.000000\n",
       "MatchedHit_DY[3]    0.000000\n",
       "Mextra_DX2[1]       0.000000\n",
       "MatchedHit_DY[1]    0.000000\n",
       "ndof                0.000000\n",
       "MatchedHit_T[2]     0.000000\n",
       "MatchedHit_DY[0]    0.000000\n",
       "MatchedHit_TYPE[2]  0.000000\n",
       "MatchedHit_TYPE[3]  0.000000\n",
       "MatchedHit_DX[0]    0.000000\n",
       "Lextra_Y[3]         0.000000\n",
       "\n",
       "[88 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(global_feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "test, test_foi = read_pub_test(used_cols)\n",
    "# global_foi_test_data = fill_global_closest_matched_hits(test.copy(), test_foi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 37.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "global_feature_importance = None\n",
    "fit_predict_save(sample(train, 100000), test, \"out/09_naive_v_m_100.csv\", n_estimators=120, transformer_cls=DataTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fit_predict_save(sample(train, 1000000), test, \"out/09_plus_all_orig_features_1000_120.csv\", n_estimators=120, transformer_cls=DataTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fit_predict_save(sample(train, 1000000), test, \"out/09_plus_all_orig_features_1000_200.csv\", n_estimators=200, transformer_cls=DataTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fit_save_model(sample(train, 100000), \"models/07_dumb_cols.xgb\", n_estimators=120, transformer_cls=DataTransformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame([[1.0, np.NaN], [np.NaN, np.NaN], [2.0, 3.1]], columns=['a', 'b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.cosine import da_cols\n",
    "from transformers.err import err_cols, nerr_xy_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import repeat\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from common import (\n",
    "    SIMPLE_FEATURE_COLS, ARR_FEATURE_COLS, ALL_TRAIN_COLS,\n",
    "    xyz_cols, dxyz_cols, t_cols,\n",
    "    foi_ts_cols, unused_train_cols, train_cols, hit_stats_cols, ncl_cols, hit_type_cols\n",
    ")\n",
    "\n",
    "class DatasetMetaData:\n",
    "    def __init__(self, origin_csv_filenames, chunk_filenames_pattern, origin_col_set):\n",
    "        self.origin_csv_filenames = origin_csv_filenames\n",
    "        self.chunk_filenames_pattern = chunk_filenames_pattern\n",
    "        self.origin_col_set = origin_col_set\n",
    "        self.is_test = 'test_' in chunk_filenames_pattern\n",
    "\n",
    "meta_train = DatasetMetaData(\n",
    "    origin_csv_filenames=['data/train_part_1_v2.csv.gz', 'data/train_part_2_v2.csv.gz'],\n",
    "    chunk_filenames_pattern='data/train_{label}_{group}_{ind:03d}.pkl',\n",
    "    origin_col_set=SIMPLE_FEATURE_COLS + ARR_FEATURE_COLS + ALL_TRAIN_COLS\n",
    ")\n",
    "meta_pub_test = DatasetMetaData(\n",
    "    origin_csv_filenames=['data/test_public_v2.csv.gz'],\n",
    "    chunk_filenames_pattern='data/test_pub_{group}_{ind:03d}.pkl',\n",
    "    origin_col_set=SIMPLE_FEATURE_COLS + ARR_FEATURE_COLS\n",
    ")\n",
    "meta_pvt_test = DatasetMetaData(\n",
    "    origin_csv_filenames=[],\n",
    "    chunk_filenames_pattern='data/test_pvt_{group}_{ind:03d}.pkl',\n",
    "    origin_col_set=SIMPLE_FEATURE_COLS + ARR_FEATURE_COLS\n",
    ")\n",
    "\n",
    "col_groups = list(zip(['sf', 'af', 'tr'], [SIMPLE_FEATURE_COLS, ARR_FEATURE_COLS, ALL_TRAIN_COLS]))\n",
    "label_prefixes = ['L0', 'L1']\n",
    "\n",
    "\n",
    "class CsvDataReader:\n",
    "    int_dtype = np.int32\n",
    "    float_dtype = np.float32\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.na_values = ['-9999.0', '255']\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_read_stream(filenames, usecols, chunk_size=25000):\n",
    "        return CsvDataReader()._get_read_stream(filenames, usecols, chunk_size)\n",
    "    \n",
    "    def _get_read_stream(self, filenames, usecols, chunk_size):\n",
    "        if 'id' not in usecols:\n",
    "            usecols += ['id']\n",
    "            \n",
    "        for filename in filenames:\n",
    "            data_generator = pd.read_csv(\n",
    "                filename, usecols=usecols, chunksize=chunk_size, index_col='id', #nrows=400000,\n",
    "                na_values=self._get_na_values_dict(), keep_default_na=False,\n",
    "                converters=self._get_converters(), dtype=self._get_types()\n",
    "            )\n",
    "            for data in data_generator:\n",
    "                \n",
    "                yield data\n",
    "\n",
    "    def _get_na_values_dict(self):\n",
    "        float_cols = [(col, '-9999.0') for col in xyz_cols + dxyz_cols]\n",
    "        int_cols = [(col, '255') for col in t_cols]\n",
    "        return {k:v for k, v in float_cols+int_cols}\n",
    "\n",
    "    def _get_converters(self):\n",
    "        def parse_float_array(line):\n",
    "            arr = np.fromstring(line[1:-1], sep=\" \", dtype=self.float_dtype)\n",
    "            return arr\n",
    "\n",
    "        converters = dict(zip(ARR_FEATURE_COLS, repeat(parse_float_array)))\n",
    "        return converters\n",
    "    \n",
    "    def _get_types(self):\n",
    "        types = dict(zip(SIMPLE_FEATURE_COLS + ALL_TRAIN_COLS, repeat(self.float_dtype)))\n",
    "        for col in unused_train_cols[:1] + train_cols[:1] + hit_stats_cols + ncl_cols + hit_type_cols:\n",
    "            types[col] = self.int_dtype\n",
    "        types['id'] = self.int_dtype\n",
    "        return types\n",
    "\n",
    "\n",
    "class DataBuffer:\n",
    "    def __init__(self):\n",
    "        self._frames = []\n",
    "    \n",
    "    def append(self, frame):\n",
    "        self._frames.append(frame)\n",
    "    \n",
    "    def cut(self, nrows):\n",
    "        nrows = min(nrows, self.nrows)\n",
    "        merged = self._merge_frames()\n",
    "        head = merged.iloc[:nrows, :]\n",
    "        tail = merged.iloc[nrows:, :]\n",
    "        \n",
    "        self._frames = [tail]\n",
    "        return head, nrows\n",
    "    \n",
    "    def _merge_frames(self):\n",
    "        if len(self._frames) > 1:\n",
    "            merged = pd.concat(self._frames, axis=0, ignore_index=False)\n",
    "            self._frames = [merged]\n",
    "        return self._frames[0]\n",
    "    \n",
    "    @property\n",
    "    def nrows(self):\n",
    "        return sum([len(frame.index) for frame in self._frames])\n",
    "    \n",
    "    @property\n",
    "    def is_empty(self):\n",
    "        return self.nrows == 0\n",
    "\n",
    "\n",
    "class DataTank:\n",
    "    def __init__(self, max_volume, callback_on_full, early_stop=False):\n",
    "        self._max_volume = max_volume\n",
    "        self._buffer = DataBuffer()\n",
    "        self._on_full = callback_on_full\n",
    "        self._early_stop = early_stop\n",
    "    \n",
    "    def add(self, frame):\n",
    "        if frame is None:\n",
    "            return 0\n",
    "        \n",
    "        self._buffer.append(frame)\n",
    "        flushed, flushed_vol = False, 0\n",
    "        while self._is_full():\n",
    "            flushed_vol += self.flush()\n",
    "            flushed = True\n",
    "            if self._early_stop:\n",
    "                break\n",
    "        return flushed, flushed_vol\n",
    "        \n",
    "    def flush(self):\n",
    "        if self._buffer.is_empty:\n",
    "            return 0\n",
    "        flushed_data, flushed_vol = self._buffer.cut(self._max_volume)\n",
    "        self._on_full(flushed_data)\n",
    "        return flushed_vol\n",
    "    \n",
    "    def _is_full(self):\n",
    "        return self._buffer.nrows >= self._max_volume\n",
    "\n",
    "\n",
    "class TestDatasetHelper:\n",
    "    def __init__(self, filename_pattern):\n",
    "        self._filename_pattern = filename_pattern\n",
    "\n",
    "    def filter_frame(self, frame):\n",
    "        return frame\n",
    "        \n",
    "    def get_col_groups(self):\n",
    "        return col_groups[:-1]\n",
    "\n",
    "    def generate_chunk_filename(self, group_key, chunk_ind):\n",
    "        return self._filename_pattern.format(group=group_key, ind=chunk_ind)\n",
    "\n",
    "\n",
    "class TrainDatasetHelper:\n",
    "    def __init__(self, filename_pattern, label, label_key):\n",
    "        self._filename_pattern = filename_pattern\n",
    "        self._label = label\n",
    "        self._label_key = label_key\n",
    "\n",
    "    def filter_frame(self, frame):\n",
    "        return frame.loc[frame.label == self._label, :]\n",
    "\n",
    "    def get_col_groups(self):\n",
    "        return col_groups\n",
    "\n",
    "    def generate_chunk_filename(self, group_key, chunk_ind):\n",
    "        return self._filename_pattern.format(label=self._label_key, group=group_key, ind=chunk_ind)\n",
    "\n",
    "\n",
    "class PickleDataWriter:\n",
    "    def __init__(self, helper, chunk_size):\n",
    "        self._helper = helper\n",
    "        self._data_tank = DataTank(max_volume=chunk_size, callback_on_full=self._flush_chunk)\n",
    "        self._chunk_index = 0\n",
    "    \n",
    "    def store(self, frame):\n",
    "        filtered_frame = self._helper.filter_frame(frame)\n",
    "        return self._data_tank.add(filtered_frame)\n",
    "        \n",
    "    def flush(self):\n",
    "        return self._data_tank.flush()\n",
    "    \n",
    "    def _flush_chunk(self, chunk):\n",
    "        self._store_chunk(chunk, self._chunk_index)\n",
    "        self._chunk_index += 1\n",
    "        \n",
    "    def _store_chunk(self, chunk, chunk_index):\n",
    "        for group_key, col_group in self._helper.get_col_groups():\n",
    "            filename = self._helper.generate_chunk_filename(group_key, chunk_index)\n",
    "            chunk.loc[:, col_group].to_pickle(filename)\n",
    "            \n",
    "            if group_key == 'af':\n",
    "                filename = self._helper.generate_chunk_filename('afexp', chunk_index)\n",
    "                self._expand(chunk, col_group).to_pickle(filename)\n",
    "                \n",
    "    @staticmethod\n",
    "    def _expand(data, cols):\n",
    "        ids = np.repeat(data.index.values, data['FOI_hits_N'].values)\n",
    "        result = pd.DataFrame(data=ids, columns=['id'])\n",
    "        for col in cols:\n",
    "             result.loc[:, col] = np.hstack(data.loc[:, col])\n",
    "        return result\n",
    "    \n",
    "\n",
    "class PickleDataReader:\n",
    "    def __init__(self, helper, foi_expanded):\n",
    "        self._helper = helper\n",
    "        self._result = None\n",
    "        self._foi_result = None\n",
    "        self._foi_expanded = foi_expanded\n",
    "        \n",
    "    def read(self, nrows, cols):\n",
    "        data_tank = DataTank(nrows, self._set_read_result, early_stop=True)\n",
    "        foi_data_tank = DataTank(100000000, self._set_foi_read_result)\n",
    "        \n",
    "        for frame, foi_frame in self._read_chunks(cols):\n",
    "            foi_data_tank.add(foi_frame)\n",
    "            flushed, _ = data_tank.add(frame)\n",
    "            if flushed:\n",
    "                foi_data_tank.flush()\n",
    "                return self._result, self._foi_result\n",
    "\n",
    "        data_tank.flush()\n",
    "        foi_data_tank.flush()\n",
    "        return self._result, self._foi_result\n",
    "        \n",
    "    def _read_chunks(self, cols):\n",
    "        chunk_index = 0\n",
    "        while True:\n",
    "            frame, foi_frame = self._read_chunk(chunk_index, cols)\n",
    "            if frame is None:\n",
    "                break\n",
    "            \n",
    "            yield frame, foi_frame\n",
    "            chunk_index += 1\n",
    "            \n",
    "    def _set_read_result(self, data):\n",
    "        self._result = data\n",
    "        \n",
    "    def _set_foi_read_result(self, data):\n",
    "        self._foi_result = data\n",
    "    \n",
    "    def _read_chunk(self, chunk_index, cols):\n",
    "        chunk_parts = []\n",
    "        foi_dataframe = None        \n",
    "        for group_key, col_group in self._helper.get_col_groups():\n",
    "            cols_ = self._intersect_cols(cols, set(col_group))\n",
    "            if not cols_:\n",
    "                continue\n",
    "            \n",
    "            filename = self._helper.generate_chunk_filename(group_key, chunk_index)\n",
    "            if not os.path.exists(filename):\n",
    "                return None, None\n",
    "            \n",
    "            if group_key == 'af' and self._foi_expanded:\n",
    "                filename = self._helper.generate_chunk_filename('afexp', chunk_index)\n",
    "                foi_dataframe = pd.read_pickle(filename).loc[:, ['id'] + cols_]\n",
    "                continue\n",
    "            \n",
    "            chunk_part = pd.read_pickle(filename).loc[:, cols_]\n",
    "            chunk_parts.append(chunk_part)\n",
    "            \n",
    "        dataframe = pd.concat(chunk_parts, axis=1, sort=False)\n",
    "        return dataframe, foi_dataframe\n",
    "    \n",
    "    @staticmethod\n",
    "    def _intersect_cols(cols, col_subset):\n",
    "        return [col for col in cols if col in col_subset]\n",
    "\n",
    "\n",
    "class DatasetConverter:\n",
    "    def __init__(self):\n",
    "        self._stored = 0\n",
    "                \n",
    "    @staticmethod\n",
    "    def convert(data_set_meta: DatasetMetaData, chunk_size=50000):\n",
    "        dataframes_stream = CsvDataReader.get_read_stream(data_set_meta.origin_csv_filenames, data_set_meta.origin_col_set)\n",
    "        \n",
    "        filename_pattern = data_set_meta.chunk_filenames_pattern\n",
    "        if data_set_meta.is_test:\n",
    "            writers = [PickleDataWriter(TestDatasetHelper(filename_pattern), chunk_size)]\n",
    "        else: \n",
    "            writers = [PickleDataWriter(TrainDatasetHelper(filename_pattern, i, label_prefixes[i]), chunk_size) for i in range(2)]\n",
    "            \n",
    "        DatasetConverter()._store_chunkified(dataframes_stream, writers)\n",
    "        \n",
    "    def _store_chunkified(self, dataframes_stream, writers):\n",
    "        for data in dataframes_stream:\n",
    "            for writer in writers:\n",
    "                self._print_stored(writer.store(data))\n",
    "\n",
    "        for writer in writers:\n",
    "            self._print_stored(writer.flush())\n",
    "            \n",
    "    def _print_stored(self, stored):\n",
    "        if stored == 0:\n",
    "            return\n",
    "        self._stored += stored\n",
    "        if self._stored % 200000 == 0:\n",
    "            print('Stored: {0}M'.format(self._stored / 1000000.))\n",
    "\n",
    "\n",
    "class DatasetReader:\n",
    "    @staticmethod\n",
    "    def read_dataset(data_set_meta: DatasetMetaData, cols, nrows=None, prop_0=.5, foi_expanded=True):\n",
    "        nrows = nrows if nrows is not None else 100000000\n",
    "        filename_pattern = data_set_meta.chunk_filenames_pattern\n",
    "        if data_set_meta.is_test:\n",
    "            readers = [PickleDataReader(TestDatasetHelper(filename_pattern), foi_expanded=foi_expanded)]\n",
    "            proportions = [nrows]\n",
    "        else: \n",
    "            readers = [PickleDataReader(TrainDatasetHelper(filename_pattern, i, label_prefixes[i]), foi_expanded=foi_expanded) for i in range(2)]\n",
    "            nrows0 = int(nrows * prop_0)\n",
    "            proportions = [nrows0, nrows - nrows0]\n",
    "            \n",
    "        return DatasetReader()._read_dataset(readers, cols, proportions)\n",
    "            \n",
    "    def _read_dataset(self, readers, cols, proportions):\n",
    "        data_parts = []\n",
    "        foi_data_parts = []\n",
    "        delta = 0\n",
    "        col_delta = hit_stats_cols[:1] if hit_stats_cols[0] not in cols else []\n",
    "        for reader, nrows in zip(readers, proportions):\n",
    "            data_part, foi_data_part = reader.read(nrows + delta, cols + col_delta)\n",
    "            if col_delta:\n",
    "                data_part = data_part.drop(col_delta, axis=1)\n",
    "            data_parts.append(data_part)\n",
    "            if foi_data_part is not None:\n",
    "                ind = self._find_slice(foi_data_part.loc[:, 'id'].values, nrows + delta)\n",
    "                foi_data_part = foi_data_part.iloc[:ind, :]\n",
    "                foi_data_parts.append(foi_data_part)\n",
    "            delta = nrows - len(data_part.index)\n",
    "            \n",
    "        data = pd.concat(data_parts, axis=0, ignore_index=False)\n",
    "        foi_data = pd.concat(foi_data_parts, axis=0, ignore_index=True) if foi_data_parts else None\n",
    "        return data, foi_data\n",
    "    \n",
    "    @staticmethod\n",
    "    def _find_slice(data, n):\n",
    "        i, prev = 0, -1\n",
    "        while i < len(data):\n",
    "            if data[i] != prev:\n",
    "                prev, n = data[i], n-1\n",
    "            if n < 0:\n",
    "                break\n",
    "            i += 1\n",
    "        return i\n",
    "                \n",
    "def convert_train():\n",
    "    DatasetConverter.convert(meta_train)\n",
    "\n",
    "def convert_pub_test():\n",
    "    DatasetConverter.convert(meta_pub_test)\n",
    "    \n",
    "def convert_pvt_test():\n",
    "    DatasetConverter.convert(meta_pvt_test)\n",
    "    \n",
    "def read_train(cols, rows, foi_expanded=True):\n",
    "    return DatasetReader.read_dataset(meta_train, cols + train_cols, rows, foi_expanded=foi_expanded)\n",
    "\n",
    "def read_pub_test(cols, foi_expanded=True):\n",
    "    return DatasetReader.read_dataset(meta_pub_test, cols, foi_expanded=foi_expanded)\n",
    "\n",
    "def read_pvt_test(cols, foi_expanded=True):\n",
    "    return DatasetReader.read_dataset(meta_pvt_test, cols, foi_expanded=foi_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_closest_cols(cols):\n",
    "    return ['Cl_' + col for col in cols]\n",
    "\n",
    "cl_xyz_cols = to_closest_cols(xyz_cols)\n",
    "cl_t_cols = to_closest_cols(t_cols)\n",
    "cl_dxyz_cols = to_closest_cols(dxyz_cols)\n",
    "cl_cols = cl_xyz_cols + cl_t_cols + cl_dxyz_cols\n",
    "\n",
    "def fill_global_closest_matched_hits(data, data_foi):\n",
    "    data.loc[:, 'id'] = data.index.values\n",
    "    exy = data.loc[:, exy_cols]\n",
    "    data_foi = data_foi.astype({foi_ts_cols[2]: np.int16})\n",
    "    join = data_foi.join(exy, on='id', how='inner')\n",
    "    for i in range(4):\n",
    "        mask = join[foi_ts_cols[2]] == i\n",
    "        slc = join.loc[mask, ['id', foi_ts_cols[2], exy_cols[i], exy_cols[i+4], foi_xyz_cols[0], foi_xyz_cols[1]]]\n",
    "        dx = slc.loc[:, exy_cols[i]].values - slc.loc[:, foi_xyz_cols[0]].values\n",
    "        dy = slc.loc[:, exy_cols[i+4]].values - slc.loc[:, foi_xyz_cols[1]].values\n",
    "        join.loc[mask, 'D2'] = dx**2 + dy**2\n",
    "        \n",
    "    res = join.sort_values(by=['id', foi_ts_cols[2], 'D2'])\n",
    "    res = res.drop_duplicates(subset=['id', foi_ts_cols[2]])\n",
    "        \n",
    "    subcols = foi_xyz_cols + foi_ts_cols[:1] + foi_dxyz_cols\n",
    "    nsubcols = len(subcols)\n",
    "    for i, col in enumerate(cl_cols):\n",
    "        mask = res[foi_ts_cols[2]] == (i % 4)\n",
    "        indices = res.loc[mask, 'id']\n",
    "        data.loc[indices, col] = res.loc[mask, subcols[i // 4]].values\n",
    "    return data\n",
    "\n",
    "def add_cl_hits(data, features):\n",
    "    for col in cl_cols:\n",
    "        data.loc[:, col] = global_foi_data.loc[data.index.values, col].values        \n",
    "    features += cl_cols\n",
    "    return data\n",
    "\n",
    "def add_cl_mse(data, features):\n",
    "    dxy = (data.loc[:, cl_xyz_cols[:8]].values - data.loc[:, exy_cols].values) / data.loc[:, cl_dxyz_cols[:8]].values / 2.\n",
    "    D = np.nanmean(dxy**2, axis=1)\n",
    "    \n",
    "    col = 'cl_' + err_cols[0]\n",
    "    data.loc[:, col] = D\n",
    "    features += [col]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_dll(data):\n",
    "    DLL = get_dll_cdf(data, pdfs, cdfs, bins)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings('ignore')\n",
    "        _ = sns.distplot(DLL, bins=len(bins), kde=False, norm_hist=True)\n",
    "\n",
    "dts = [dt.loc[:, err_cols[0]] for dt in split_classes(dll_train)]\n",
    "plot_dll(dts[0].values)\n",
    "plot_dll(dts[1].values)\n",
    "# plot_dll(dll_train.loc[:, err_cols[0]].values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
