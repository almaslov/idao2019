{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABUkAAABECAYAAAC8urRAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAA4JJREFUeJzt2zFLVWEcx/H/DYmgSYoCc7Bw0KDFcyVoCaLNNxAS1FBRSw0NTYEUBA0NtRTVEoT0BtoqaAlCj0uQDlIOt6AonAKJ4PYG1Kmnp+7/8xnPs/wOHM7whafT7/cDAAAAACCrHbUHAAAAAADUJJICAAAAAKmJpAAAAABAaiIpAAAAAJCaSAoAAAAApDa03WHbtv2/NQQAAAAAoKSmaTqbPd82kkZEzM33/vyaf8Tc7GjcXn1ce0Yx18bPxcaNW7VnFPFy/GzMzI7E8/nPtacUMTM7EvH9We0Z5ew5FQ++bPpPGggX9/fj8NMXtWcU8/70yVhfOF97RjHD04/iZm8w3+/66KO4tLZSe0Yx98cm4sryWO0ZxdydXIsDDydqzyjm04WV6F5tas8oZvFOG0cPXq49o4i3H+/FaPd47RnF9BZfx6sz+2rPKObEk6/RbHRrzyim3bUY3d0D/G/50cabn4P5fsd2trEw/qH2jGKmVw9F826p9oxi2iNTMbQ+mN9mRMSv4TaaqeXaM4pplyajt3ek9owiRr9t3ZFctwcAAAAAUhNJAQAAAIDURFIAAAAAIDWRFAAAAABITSQFAAAAAFITSQEAAACA1ERSAAAAACA1kRQAAAAASE0kBQAAAABSE0kBAAAAgNREUgAAAAAgNZEUAAAAAEhNJAUAAAAAUhNJAQAAAIDURFIAAAAAIDWRFAAAAABITSQFAAAAAFITSQEAAACA1ERSAAAAACA1kRQAAAAASE0kBQAAAABSE0kBAAAAgNREUgAAAAAgNZEUAAAAAEhNJAUAAAAAUhNJAQAAAIDURFIAAAAAIDWRFAAAAABITSQFAAAAAFITSQEAAACA1ERSAAAAACA1kRQAAAAASE0kBQAAAABSE0kBAAAAgNREUgAAAAAgNZEUAAAAAEhNJAUAAAAAUhNJAQAAAIDURFIAAAAAIDWRFAAAAABITSQFAAAAAFITSQEAAACA1ERSAAAAACA1kRQAAAAASE0kBQAAAABSE0kBAAAAgNREUgAAAAAgNZEUAAAAAEhNJAUAAAAAUhNJAQAAAIDURFIAAAAAIDWRFAAAAABITSQFAAAAAFLr9Pv9LQ/btt36EAAAAADgP9I0TWez59tGUgAAAACAQee6PQAAAACQmkgKAAAAAKQmkgIAAAAAqYmkAAAAAEBqIikAAAAAkNpvuTVZgasyZKkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1728x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as sts\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from common import *\n",
    "from io_tools import (\n",
    "    read_train, read_pub_test, read_pvt_test,\n",
    "    convert_train, convert_pub_test, convert_pvt_test,\n",
    ")\n",
    "from pipeline import (\n",
    "    split_classes, count_classes, sample,\n",
    "    cross_validate, fit_predict_save, fit_save_model\n",
    ")\n",
    "from transformers.pca import pca_fit, pca_transform\n",
    "from transformers.cosine import add_coses, to_degrees\n",
    "from transformers.momentum import add_is_muon, add_is_muon_tight, add_probability_hit_detector\n",
    "from transformers.err import add_mse, add_normed_err, err_cols\n",
    "\n",
    "sns.set()\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "color_palette = sns.color_palette('deep') + sns.color_palette('husl', 6) + sns.color_palette('bright') + sns.color_palette('pastel')\n",
    "\n",
    "%matplotlib inline\n",
    "sns.palplot(color_palette)\n",
    "\n",
    "def ndprint(a, precision=3):\n",
    "    with np.printoptions(precision=precision, suppress=True):\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored: 0.2M\n",
      "Stored: 0.4M\n",
      "Stored: 0.6M\n",
      "Stored: 0.8M\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "convert_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test, test_foi = read_pub_test(SIMPLE_FEATURE_COLS + ARR_FEATURE_COLS, foi_expanded=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 726095 entries, 0 to 726094\n",
      "Empty DataFrame"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test.index)):\n",
    "    t = test.iloc[i, :][foi_ts_cols[0]]\n",
    "    if np.any(np.isnan(t)):\n",
    "        print(i, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3 entries, 10 to 30\n",
      "Data columns (total 2 columns):\n",
      "a    3 non-null float64\n",
      "c    3 non-null int64\n",
      "dtypes: float64(1), int64(1)\n",
      "memory usage: 72.0 bytes\n"
     ]
    }
   ],
   "source": [
    "xx = pd.DataFrame(index=[10, 20, 30])\n",
    "xx.loc[:, 'a'] = [1., 2., 3.]\n",
    "xx.loc[:, 'c'] = [1, 2, 3]\n",
    "\n",
    "xx.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = np.array([\n",
    "    np.array([1, 2,]),\n",
    "    np.array([3, 4, 5])\n",
    "])\n",
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, xy = DatasetReader.read_dataset(meta_pub_test, SIMPLE_FEATURE_COLS + ARR_FEATURE_COLS, nrows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 6)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(xx[hit_stats_cols[0]]), len(xy.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import repeat\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from common import (\n",
    "    SIMPLE_FEATURE_COLS, ARR_FEATURE_COLS, ALL_TRAIN_COLS,\n",
    "    xyz_cols, dxyz_cols, t_cols,\n",
    "    foi_ts_cols, unused_train_cols, train_cols, hit_stats_cols, ncl_cols, hit_type_cols\n",
    ")\n",
    "\n",
    "class DatasetMetaData:\n",
    "    def __init__(self, origin_csv_filenames, chunk_filenames_pattern, origin_col_set):\n",
    "        self.origin_csv_filenames = origin_csv_filenames\n",
    "        self.chunk_filenames_pattern = chunk_filenames_pattern\n",
    "        self.origin_col_set = origin_col_set\n",
    "        self.is_test = 'test_' in chunk_filenames_pattern\n",
    "\n",
    "meta_train = DatasetMetaData(\n",
    "    origin_csv_filenames=['data/train_part_1_v2.csv.gz', 'data/train_part_2_v2.csv.gz'],\n",
    "    chunk_filenames_pattern='data/train_{label}_{group}_{ind:03d}.pkl',\n",
    "    origin_col_set=SIMPLE_FEATURE_COLS + ARR_FEATURE_COLS + ALL_TRAIN_COLS\n",
    ")\n",
    "meta_pub_test = DatasetMetaData(\n",
    "    origin_csv_filenames=['data/test_public_v2.csv.gz'],\n",
    "    chunk_filenames_pattern='data/test_pub_{group}_{ind:03d}.pkl',\n",
    "    origin_col_set=SIMPLE_FEATURE_COLS + ARR_FEATURE_COLS\n",
    ")\n",
    "meta_pvt_test = DatasetMetaData(\n",
    "    origin_csv_filenames=[],\n",
    "    chunk_filenames_pattern='data/test_pvt_{group}_{ind:03d}.pkl',\n",
    "    origin_col_set=SIMPLE_FEATURE_COLS + ARR_FEATURE_COLS\n",
    ")\n",
    "\n",
    "col_groups = list(zip(['sf', 'af', 'tr'], [SIMPLE_FEATURE_COLS, ARR_FEATURE_COLS, ALL_TRAIN_COLS]))\n",
    "label_prefixes = ['L0', 'L1']\n",
    "\n",
    "\n",
    "class CsvDataReader:\n",
    "    int_dtype = np.int32\n",
    "    float_dtype = np.float32\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.na_values = ['-9999.0', '255']\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_read_stream(filenames, usecols, chunk_size=25000):\n",
    "        return CsvDataReader()._get_read_stream(filenames, usecols, chunk_size)\n",
    "    \n",
    "    def _get_read_stream(self, filenames, usecols, chunk_size):\n",
    "        if 'id' not in usecols:\n",
    "            usecols += ['id']\n",
    "            \n",
    "        for filename in filenames:\n",
    "            data_generator = pd.read_csv(\n",
    "                filename, usecols=usecols, chunksize=chunk_size, index_col='id', #nrows=400000,\n",
    "                na_values=self._get_na_values_dict(), keep_default_na=False,\n",
    "                converters=self._get_converters(), dtype=self._get_types()\n",
    "            )\n",
    "            for data in data_generator:\n",
    "                \n",
    "                yield data\n",
    "\n",
    "    def _get_na_values_dict(self):\n",
    "        float_cols = [(col, '-9999.0') for col in xyz_cols + dxyz_cols]\n",
    "        int_cols = [(col, '255') for col in t_cols]\n",
    "        return {k:v for k, v in float_cols+int_cols}\n",
    "\n",
    "    def _get_converters(self):\n",
    "        def parse_float_array(line):\n",
    "            arr = np.fromstring(line[1:-1], sep=\" \", dtype=self.float_dtype)\n",
    "            return arr\n",
    "\n",
    "        converters = dict(zip(ARR_FEATURE_COLS, repeat(parse_float_array)))\n",
    "        return converters\n",
    "    \n",
    "    def _get_types(self):\n",
    "        types = dict(zip(SIMPLE_FEATURE_COLS + ALL_TRAIN_COLS, repeat(self.float_dtype)))\n",
    "        for col in unused_train_cols[:1] + train_cols[:1] + hit_stats_cols + ncl_cols + hit_type_cols:\n",
    "            types[col] = self.int_dtype\n",
    "        types['id'] = self.int_dtype\n",
    "        return types\n",
    "\n",
    "\n",
    "class DataBuffer:\n",
    "    def __init__(self):\n",
    "        self._frames = []\n",
    "    \n",
    "    def append(self, frame):\n",
    "        self._frames.append(frame)\n",
    "    \n",
    "    def cut(self, nrows):\n",
    "        nrows = min(nrows, self.nrows)\n",
    "        merged = self._merge_frames()\n",
    "        head = merged.iloc[:nrows, :]\n",
    "        tail = merged.iloc[nrows:, :]\n",
    "        \n",
    "        self._frames = [tail]\n",
    "        return head, nrows\n",
    "    \n",
    "    def _merge_frames(self):\n",
    "        if len(self._frames) > 1:\n",
    "            merged = pd.concat(self._frames, axis=0, ignore_index=False)\n",
    "            self._frames = [merged]\n",
    "        return self._frames[0]\n",
    "    \n",
    "    @property\n",
    "    def nrows(self):\n",
    "        return sum([len(frame.index) for frame in self._frames])\n",
    "    \n",
    "    @property\n",
    "    def is_empty(self):\n",
    "        return self.nrows == 0\n",
    "\n",
    "\n",
    "class DataTank:\n",
    "    def __init__(self, max_volume, callback_on_full, early_stop=False):\n",
    "        self._max_volume = max_volume\n",
    "        self._buffer = DataBuffer()\n",
    "        self._on_full = callback_on_full\n",
    "        self._early_stop = early_stop\n",
    "    \n",
    "    def add(self, frame):\n",
    "        if frame is None:\n",
    "            return 0\n",
    "        \n",
    "        self._buffer.append(frame)\n",
    "        flushed = 0\n",
    "        while self._is_full():\n",
    "            flushed += self.flush()\n",
    "            if self._early_stop:\n",
    "                break\n",
    "        return flushed\n",
    "        \n",
    "    def flush(self):\n",
    "        if self._buffer.is_empty:\n",
    "            return 0\n",
    "        flushed_data, flushed_vol = self._buffer.cut(self._max_volume)\n",
    "        self._on_full(flushed_data)\n",
    "        return flushed_vol\n",
    "    \n",
    "    def _is_full(self):\n",
    "        return self._buffer.nrows >= self._max_volume\n",
    "\n",
    "\n",
    "class TestDatasetHelper:\n",
    "    def __init__(self, filename_pattern):\n",
    "        self._filename_pattern = filename_pattern\n",
    "\n",
    "    def filter_frame(self, frame):\n",
    "        return frame\n",
    "        \n",
    "    def get_col_groups(self):\n",
    "        return col_groups[:-1]\n",
    "\n",
    "    def generate_chunk_filename(self, group_key, chunk_ind):\n",
    "        return self._filename_pattern.format(group=group_key, ind=chunk_ind)\n",
    "\n",
    "\n",
    "class TrainDatasetHelper:\n",
    "    def __init__(self, filename_pattern, label, label_key):\n",
    "        self._filename_pattern = filename_pattern\n",
    "        self._label = label\n",
    "        self._label_key = label_key\n",
    "\n",
    "    def filter_frame(self, frame):\n",
    "        return frame.loc[frame.label == self._label, :]\n",
    "\n",
    "    def get_col_groups(self):\n",
    "        return col_groups\n",
    "\n",
    "    def generate_chunk_filename(self, group_key, chunk_ind):\n",
    "        return self._filename_pattern.format(label=self._label_key, group=group_key, ind=chunk_ind)\n",
    "\n",
    "\n",
    "class PickleDataWriter:\n",
    "    def __init__(self, helper, chunk_size):\n",
    "        self._helper = helper\n",
    "        self._data_tank = DataTank(max_volume=chunk_size, callback_on_full=self._flush_chunk)\n",
    "        self._chunk_index = 0\n",
    "    \n",
    "    def store(self, frame):\n",
    "        filtered_frame = self._helper.filter_frame(frame)\n",
    "        return self._data_tank.add(filtered_frame)\n",
    "        \n",
    "    def flush(self):\n",
    "        return self._data_tank.flush()\n",
    "    \n",
    "    def _flush_chunk(self, chunk):\n",
    "        self._store_chunk(chunk, self._chunk_index)\n",
    "        self._chunk_index += 1\n",
    "        \n",
    "    def _store_chunk(self, chunk, chunk_index):\n",
    "        for group_key, col_group in self._helper.get_col_groups():\n",
    "            filename = self._helper.generate_chunk_filename(group_key, chunk_index)\n",
    "            chunk.loc[:, col_group].to_pickle(filename)\n",
    "            \n",
    "            if group_key == 'af':\n",
    "                filename = self._helper.generate_chunk_filename('afexp', chunk_index)\n",
    "                self._expand(chunk, col_group).to_pickle(filename)\n",
    "                \n",
    "    @staticmethod\n",
    "    def _expand(data, cols):\n",
    "        ids = np.repeat(data.index.values, data['FOI_hits_N'].values)\n",
    "        result = pd.DataFrame(data=ids, columns=['id'])\n",
    "        for col in cols:\n",
    "             result.loc[:, col] = np.hstack(data.loc[:, col])\n",
    "        return result\n",
    "    \n",
    "\n",
    "class PickleDataReader:\n",
    "    def __init__(self, helper, foi_expanded):\n",
    "        self._helper = helper\n",
    "        self._result = None\n",
    "        self._foi_result = None\n",
    "        self._foi_expanded = foi_expanded\n",
    "        \n",
    "    def read(self, nrows, cols):\n",
    "        data_tank = DataTank(nrows, self._set_read_result, early_stop=True)\n",
    "        foi_data_tank = DataTank(100000000, self._set_foi_read_result)\n",
    "        \n",
    "        for frame, foi_frame in self._read_chunks(cols):\n",
    "            foi_data_tank.add(foi_frame)\n",
    "            if data_tank.add(frame) > 0:\n",
    "                foi_data_tank.flush()\n",
    "                return self._result, self._foi_result\n",
    "\n",
    "        data_tank.flush()\n",
    "        foi_data_tank.flush()\n",
    "        return self._result, self._foi_result\n",
    "        \n",
    "    def _read_chunks(self, cols):\n",
    "        chunk_index = 0\n",
    "        while True:\n",
    "            frame, foi_frame = self._read_chunk(chunk_index, cols)\n",
    "            if frame is None:\n",
    "                break\n",
    "            \n",
    "            yield frame, foi_frame\n",
    "            chunk_index += 1\n",
    "            \n",
    "    def _set_read_result(self, data):\n",
    "        self._result = data\n",
    "        \n",
    "    def _set_foi_read_result(self, data):\n",
    "        self._foi_result = data\n",
    "    \n",
    "    def _read_chunk(self, chunk_index, cols):\n",
    "        chunk_parts = []\n",
    "        foi_dataframe = None        \n",
    "        for group_key, col_group in self._helper.get_col_groups():\n",
    "            cols_ = self._intersect_cols(cols, set(col_group))\n",
    "            if not cols_:\n",
    "                continue\n",
    "            \n",
    "            filename = self._helper.generate_chunk_filename(group_key, chunk_index)\n",
    "            if not os.path.exists(filename):\n",
    "                return None, None\n",
    "            \n",
    "            if group_key == 'af' and self._foi_expanded:\n",
    "                filename = self._helper.generate_chunk_filename('afexp', chunk_index)\n",
    "                foi_dataframe = pd.read_pickle(filename).loc[:, ['id'] + cols_]\n",
    "                continue\n",
    "            \n",
    "            chunk_part = pd.read_pickle(filename).loc[:, cols_]\n",
    "            chunk_parts.append(chunk_part)\n",
    "            \n",
    "        dataframe = pd.concat(chunk_parts, axis=1, sort=False)\n",
    "        return dataframe, foi_dataframe\n",
    "    \n",
    "    @staticmethod\n",
    "    def _intersect_cols(cols, col_subset):\n",
    "        return [col for col in cols if col in col_subset]\n",
    "\n",
    "\n",
    "class DatasetConverter:\n",
    "    def __init__(self):\n",
    "        self._stored = 0\n",
    "                \n",
    "    @staticmethod\n",
    "    def convert(data_set_meta: DatasetMetaData, chunk_size=50000):\n",
    "        dataframes_stream = CsvDataReader.get_read_stream(data_set_meta.origin_csv_filenames, data_set_meta.origin_col_set)\n",
    "        \n",
    "        filename_pattern = data_set_meta.chunk_filenames_pattern\n",
    "        if data_set_meta.is_test:\n",
    "            writers = [PickleDataWriter(TestDatasetHelper(filename_pattern), chunk_size)]\n",
    "        else: \n",
    "            writers = [PickleDataWriter(TrainDatasetHelper(filename_pattern, i, label_prefixes[i]), chunk_size) for i in range(2)]\n",
    "            \n",
    "        DatasetConverter()._store_chunkified(dataframes_stream, writers)\n",
    "        \n",
    "    def _store_chunkified(self, dataframes_stream, writers):\n",
    "        for data in dataframes_stream:\n",
    "            for writer in writers:\n",
    "                self._print_stored(writer.store(data))\n",
    "\n",
    "        for writer in writers:\n",
    "            self._print_stored(writer.flush())\n",
    "            \n",
    "    def _print_stored(self, stored):\n",
    "        if stored == 0:\n",
    "            return\n",
    "        self._stored += stored\n",
    "        if self._stored % 200000 == 0:\n",
    "            print('Stored: {0}M'.format(self._stored / 1000000.))\n",
    "\n",
    "\n",
    "class DatasetReader:\n",
    "    @staticmethod\n",
    "    def read_dataset(data_set_meta: DatasetMetaData, cols, nrows=None, prop_0=.5, foi_expanded=True):\n",
    "        nrows = nrows if nrows is not None else 100000000\n",
    "        filename_pattern = data_set_meta.chunk_filenames_pattern\n",
    "        if data_set_meta.is_test:\n",
    "            readers = [PickleDataReader(TestDatasetHelper(filename_pattern), foi_expanded=foi_expanded)]\n",
    "            proportions = [nrows]\n",
    "        else: \n",
    "            readers = [PickleDataReader(TrainDatasetHelper(filename_pattern, i, label_prefixes[i]), foi_expanded=foi_expanded) for i in range(2)]\n",
    "            nrows0 = int(nrows * prop_0)\n",
    "            proportions = [nrows0, nrows - nrows0]\n",
    "            \n",
    "        return DatasetReader()._read_dataset(readers, cols, proportions)\n",
    "            \n",
    "    def _read_dataset(self, readers, cols, proportions):\n",
    "        data_parts = []\n",
    "        foi_data_parts = []\n",
    "        delta = 0\n",
    "        col_delta = hit_stats_cols[:1] if hit_stats_cols[0] not in cols else []\n",
    "        for reader, nrows in zip(readers, proportions):\n",
    "            data_part, foi_data_part = reader.read(nrows + delta, cols + col_delta)\n",
    "            if col_delta:\n",
    "                data_part = data_part.drop(col_delta, axis=1)\n",
    "            data_parts.append(data_part)\n",
    "            if foi_data_part is not None:\n",
    "                foi_data_part = foi_data_part.loc[foi_data_part.id < nrows, :]\n",
    "                foi_data_parts.append(foi_data_part)\n",
    "            delta = nrows - len(data_part.index)\n",
    "            \n",
    "        data = pd.concat(data_parts, axis=0, ignore_index=False)\n",
    "        foi_data = pd.concat(foi_data_parts, axis=0, ignore_index=False) if foi_data_parts else None\n",
    "        return data, foi_data\n",
    "\n",
    "\n",
    "def convert_train():\n",
    "    DatasetConverter.convert(meta_train)\n",
    "\n",
    "def convert_pub_test():\n",
    "    DatasetConverter.convert(meta_pub_test)\n",
    "    \n",
    "def convert_pvt_test():\n",
    "    DatasetConverter.convert(meta_pvt_test)\n",
    "    \n",
    "def read_train(cols, rows, foi_expanded=True):\n",
    "    return DatasetReader.read_dataset(meta_train, cols + train_cols, rows, foi_expanded=foi_expanded)\n",
    "\n",
    "def read_pub_test(cols, foi_expanded=True):\n",
    "    return DatasetReader.read_dataset(meta_pub_test, cols, foi_expanded=foi_expanded)\n",
    "\n",
    "def read_pvt_test(cols, foi_expanded=True):\n",
    "    return DatasetReader.read_dataset(meta_pvt_test, cols, foi_expanded=foi_expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 63)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(421218, 578782)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "used_cols = xyz_cols + mom_cols + hit_type_cols + dxyz_cols + exy_cols + edxy_cols + hit_stats_cols + t_cols + ncl_cols + avg_cs_cols\n",
    "global_feature_importance = None\n",
    "train = read_train(used_cols, 1000000)\n",
    "display(train.shape, count_classes(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transformer\n",
    "\n",
    "Это по сути основная часть. Класс, который отбирает нужные столбцы, возможно что-то модифицирует или добавляет. На выходе - входные данные для модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dll_train = read_train(xy_cols + dx_cols + dy_cols + exy_cols, 10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dll_train = add_mse(dll_train, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233\n"
     ]
    }
   ],
   "source": [
    "def create_pdfs(data):\n",
    "    dts = [dt.loc[:, err_cols[0]] for dt in split_classes(data)]\n",
    "    min_len = min(map(len, dts))\n",
    "    nbins = int(round(np.sqrt(min_len) / np.pi))\n",
    "    nbins = 230\n",
    "    \n",
    "    l, r = np.min(data[err_cols[0]]) - 1e-5, np.max(data[err_cols[0]]) + 1e-5\n",
    "    k = 9\n",
    "    m = (r - l) / k\n",
    "    m = 20\n",
    "    bins = np.concatenate((\n",
    "        np.arange(l, 1, .02),\n",
    "        np.arange(1, 3, .04),\n",
    "        np.arange(3, 10, .1),\n",
    "        np.arange(10, 16, .4),\n",
    "        np.arange(16, 34, 1.),\n",
    "        np.arange(34, 66, 2),\n",
    "        np.arange(66, 120, 5.),\n",
    "        np.linspace(120, r, 3),\n",
    "    ))\n",
    "    nbins = len(bins)\n",
    "    print(nbins)\n",
    "#     bins = np.concatenate((\n",
    "#         np.linspace(l, m, (k-1) * nbins // k, endpoint=False), \n",
    "#         np.linspace(m, r, nbins // k)\n",
    "#     ))\n",
    "    pdfs = []\n",
    "    \n",
    "    for i in range(2):\n",
    "        pdf, _ = np.histogram(dts[i], bins=bins)\n",
    "        pdfs.append(pdf)\n",
    "                       \n",
    "    return pdfs, bins\n",
    "                       \n",
    "pdfs, bins = create_pdfs(dll_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pdfs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-122-1db5d3b41e20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpdfs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pdfs' is not defined"
     ]
    }
   ],
   "source": [
    "pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>scr</th>\n",
       "      <th>th</th>\n",
       "      <th>dTh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.779999</td>\n",
       "      <td>0.772095</td>\n",
       "      <td>0.880412</td>\n",
       "      <td>0.822643</td>\n",
       "      <td>0.812830</td>\n",
       "      <td>0.708636</td>\n",
       "      <td>0.092701</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.007281</td>\n",
       "      <td>0.009568</td>\n",
       "      <td>0.010734</td>\n",
       "      <td>0.005255</td>\n",
       "      <td>0.014001</td>\n",
       "      <td>0.048772</td>\n",
       "      <td>0.007831</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.774445</td>\n",
       "      <td>0.761525</td>\n",
       "      <td>0.868462</td>\n",
       "      <td>0.818848</td>\n",
       "      <td>0.798002</td>\n",
       "      <td>0.652489</td>\n",
       "      <td>0.087179</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.775878</td>\n",
       "      <td>0.768060</td>\n",
       "      <td>0.876001</td>\n",
       "      <td>0.819644</td>\n",
       "      <td>0.806334</td>\n",
       "      <td>0.692708</td>\n",
       "      <td>0.088220</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.777311</td>\n",
       "      <td>0.774596</td>\n",
       "      <td>0.883540</td>\n",
       "      <td>0.820439</td>\n",
       "      <td>0.814666</td>\n",
       "      <td>0.732927</td>\n",
       "      <td>0.089260</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.782777</td>\n",
       "      <td>0.777380</td>\n",
       "      <td>0.886387</td>\n",
       "      <td>0.824540</td>\n",
       "      <td>0.820245</td>\n",
       "      <td>0.736710</td>\n",
       "      <td>0.095462</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.788242</td>\n",
       "      <td>0.780165</td>\n",
       "      <td>0.889234</td>\n",
       "      <td>0.828641</td>\n",
       "      <td>0.825823</td>\n",
       "      <td>0.740492</td>\n",
       "      <td>0.101663</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            acc      prec       rec        f1   roc_auc       scr        th  \\\n",
       "count  3.000000  3.000000  3.000000  3.000000  3.000000  3.000000  3.000000   \n",
       "mean   0.779999  0.772095  0.880412  0.822643  0.812830  0.708636  0.092701   \n",
       "std    0.007281  0.009568  0.010734  0.005255  0.014001  0.048772  0.007831   \n",
       "min    0.774445  0.761525  0.868462  0.818848  0.798002  0.652489  0.087179   \n",
       "25%    0.775878  0.768060  0.876001  0.819644  0.806334  0.692708  0.088220   \n",
       "50%    0.777311  0.774596  0.883540  0.820439  0.814666  0.732927  0.089260   \n",
       "75%    0.782777  0.777380  0.886387  0.824540  0.820245  0.736710  0.095462   \n",
       "max    0.788242  0.780165  0.889234  0.828641  0.825823  0.740492  0.101663   \n",
       "\n",
       "       dTh  \n",
       "count  3.0  \n",
       "mean   0.0  \n",
       "std    0.0  \n",
       "min    0.0  \n",
       "25%    0.0  \n",
       "50%    0.0  \n",
       "75%    0.0  \n",
       "max    0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def add_dll(data, features):\n",
    "    def get_probs(pdf, x):\n",
    "        indices = np.digitize(x, bins) - 1\n",
    "        wbin = (bins[indices + 1] - bins[indices]) / (np.max(bins) - np.min(bins))\n",
    "        prob = pdf[indices] / pdf.sum()\n",
    "        return prob #* wbin\n",
    "\n",
    "    def get_dll(x):\n",
    "        probs = [get_probs(pdf, x) for pdf in pdfs]\n",
    "        DLL = np.log(probs[1]) - np.log(probs[0])\n",
    "        return DLL\n",
    "\n",
    "    data[err_cols[1]] = get_dll(data.loc[:, err_cols[0]])\n",
    "    features += err_cols[1:2]\n",
    "    return data\n",
    "\n",
    "df_scores, feature_importance = cross_validate(train, n_estimators=60, n_splits=3, n_rows=10000, transformer_cls=DataTransformer)\n",
    "display(df_scores.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>scr</th>\n",
       "      <th>th</th>\n",
       "      <th>dTh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.772400</td>\n",
       "      <td>0.764811</td>\n",
       "      <td>0.874285</td>\n",
       "      <td>0.815872</td>\n",
       "      <td>0.804206</td>\n",
       "      <td>0.689943</td>\n",
       "      <td>0.097152</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.005951</td>\n",
       "      <td>0.007636</td>\n",
       "      <td>0.002642</td>\n",
       "      <td>0.003701</td>\n",
       "      <td>0.007953</td>\n",
       "      <td>0.084526</td>\n",
       "      <td>0.006686</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.767177</td>\n",
       "      <td>0.757412</td>\n",
       "      <td>0.872075</td>\n",
       "      <td>0.812922</td>\n",
       "      <td>0.796970</td>\n",
       "      <td>0.600755</td>\n",
       "      <td>0.090074</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.769161</td>\n",
       "      <td>0.760885</td>\n",
       "      <td>0.872822</td>\n",
       "      <td>0.813796</td>\n",
       "      <td>0.799948</td>\n",
       "      <td>0.650480</td>\n",
       "      <td>0.094047</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.771146</td>\n",
       "      <td>0.764357</td>\n",
       "      <td>0.873569</td>\n",
       "      <td>0.814671</td>\n",
       "      <td>0.802925</td>\n",
       "      <td>0.700206</td>\n",
       "      <td>0.098020</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.775012</td>\n",
       "      <td>0.768511</td>\n",
       "      <td>0.875390</td>\n",
       "      <td>0.817348</td>\n",
       "      <td>0.807824</td>\n",
       "      <td>0.734537</td>\n",
       "      <td>0.100691</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.778878</td>\n",
       "      <td>0.772665</td>\n",
       "      <td>0.877211</td>\n",
       "      <td>0.820024</td>\n",
       "      <td>0.812722</td>\n",
       "      <td>0.768869</td>\n",
       "      <td>0.103361</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            acc      prec       rec        f1   roc_auc       scr        th  \\\n",
       "count  3.000000  3.000000  3.000000  3.000000  3.000000  3.000000  3.000000   \n",
       "mean   0.772400  0.764811  0.874285  0.815872  0.804206  0.689943  0.097152   \n",
       "std    0.005951  0.007636  0.002642  0.003701  0.007953  0.084526  0.006686   \n",
       "min    0.767177  0.757412  0.872075  0.812922  0.796970  0.600755  0.090074   \n",
       "25%    0.769161  0.760885  0.872822  0.813796  0.799948  0.650480  0.094047   \n",
       "50%    0.771146  0.764357  0.873569  0.814671  0.802925  0.700206  0.098020   \n",
       "75%    0.775012  0.768511  0.875390  0.817348  0.807824  0.734537  0.100691   \n",
       "max    0.778878  0.772665  0.877211  0.820024  0.812722  0.768869  0.103361   \n",
       "\n",
       "       dTh  \n",
       "count  3.0  \n",
       "mean   0.0  \n",
       "std    0.0  \n",
       "min    0.0  \n",
       "25%    0.0  \n",
       "50%    0.0  \n",
       "75%    0.0  \n",
       "max    0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from common import x_cols, y_cols, z_cols\n",
    "\n",
    "da_cols = ['DAngle[%d]' % i for i in range(1, 4)]\n",
    "\n",
    "def add_coses(data, features):\n",
    "    def get_layer_coords(data, i):\n",
    "        return data[[x_cols[i], y_cols[i], z_cols[i]]].values\n",
    "  \n",
    "    def dot(x, y):\n",
    "        return np.sum(x * y, axis=1, dtype=np.float32)\n",
    "    \n",
    "    def norm(x):\n",
    "        return np.sqrt(dot(x, x))\n",
    "\n",
    "    def get_cosine_dist(L1, L2, L1_norm, L2_norm):\n",
    "        cosines = dot(L1, L2) / L1_norm / L2_norm\n",
    "        return np.clip(cosines, -1., 1.)\n",
    "    \n",
    "    layers = np.array([get_layer_coords(data, i) for i in range(4)])\n",
    "    layers[1:] -= layers[:3]\n",
    "    layers[0] = get_zero_point(data)\n",
    "    \n",
    "    for i in range(3):\n",
    "        cur_layer = layers[i]\n",
    "        next_layer = layers[i+1]\n",
    "        nan_mask = np.isnan(next_layer[:, 0])\n",
    "        next_layer[nan_mask, :] = cur_layer[nan_mask, :]\n",
    "        \n",
    "        cosines = get_cosine_dist(cur_layer, next_layer, norm(cur_layer), norm(next_layer))\n",
    "        degrees = to_degrees(cosines)\n",
    "        cosines[nan_mask] = np.NaN\n",
    "        degrees[nan_mask] = np.NaN\n",
    "        data[da_cols[i]] = degrees\n",
    "        \n",
    "    features += da_cols\n",
    "    return data\n",
    "\n",
    "def to_degrees(cosine):\n",
    "    return np.arccos(cosine) / np.pi * 180.\n",
    "\n",
    "def _to_degrees(cosines):\n",
    "    angles = cosines.copy()\n",
    "    isn_mask = ~np.isnan(cosines)\n",
    "    angles[isn_mask] = np.arccos(cosines[isn_mask]) / np.pi * 180.\n",
    "    return angles\n",
    "\n",
    "def get_zero_point(data):\n",
    "    layers = [data[[ex_cols[i], ey_cols[i], z_cols[i]]].values for i in range(2)]\n",
    "    d = layers[1] - layers[0]\n",
    "    return d\n",
    "    \n",
    "df_scores, feature_importance = cross_validate(train, n_estimators=60, n_splits=3, n_rows=10000, transformer_cls=DataTransformer)\n",
    "display(df_scores.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>scr</th>\n",
       "      <th>th</th>\n",
       "      <th>dTh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.767981</td>\n",
       "      <td>0.754840</td>\n",
       "      <td>0.899815</td>\n",
       "      <td>0.820640</td>\n",
       "      <td>0.796658</td>\n",
       "      <td>0.590769</td>\n",
       "      <td>0.200585</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.025779</td>\n",
       "      <td>0.029614</td>\n",
       "      <td>0.014865</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.019841</td>\n",
       "      <td>0.358864</td>\n",
       "      <td>0.029793</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.738739</td>\n",
       "      <td>0.720648</td>\n",
       "      <td>0.882653</td>\n",
       "      <td>0.803612</td>\n",
       "      <td>0.775845</td>\n",
       "      <td>0.346106</td>\n",
       "      <td>0.166246</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.758258</td>\n",
       "      <td>0.746100</td>\n",
       "      <td>0.895408</td>\n",
       "      <td>0.813711</td>\n",
       "      <td>0.787308</td>\n",
       "      <td>0.384785</td>\n",
       "      <td>0.191099</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.771552</td>\n",
       "      <td>0.908163</td>\n",
       "      <td>0.823810</td>\n",
       "      <td>0.798770</td>\n",
       "      <td>0.423464</td>\n",
       "      <td>0.215953</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.782601</td>\n",
       "      <td>0.771937</td>\n",
       "      <td>0.908396</td>\n",
       "      <td>0.829154</td>\n",
       "      <td>0.807064</td>\n",
       "      <td>0.713100</td>\n",
       "      <td>0.217754</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.787425</td>\n",
       "      <td>0.772321</td>\n",
       "      <td>0.908629</td>\n",
       "      <td>0.834499</td>\n",
       "      <td>0.815358</td>\n",
       "      <td>1.002735</td>\n",
       "      <td>0.219556</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            acc      prec       rec        f1   roc_auc       scr        th  \\\n",
       "count  3.000000  3.000000  3.000000  3.000000  3.000000  3.000000  3.000000   \n",
       "mean   0.767981  0.754840  0.899815  0.820640  0.796658  0.590769  0.200585   \n",
       "std    0.025779  0.029614  0.014865  0.015686  0.019841  0.358864  0.029793   \n",
       "min    0.738739  0.720648  0.882653  0.803612  0.775845  0.346106  0.166246   \n",
       "25%    0.758258  0.746100  0.895408  0.813711  0.787308  0.384785  0.191099   \n",
       "50%    0.777778  0.771552  0.908163  0.823810  0.798770  0.423464  0.215953   \n",
       "75%    0.782601  0.771937  0.908396  0.829154  0.807064  0.713100  0.217754   \n",
       "max    0.787425  0.772321  0.908629  0.834499  0.815358  1.002735  0.219556   \n",
       "\n",
       "       dTh  \n",
       "count  3.0  \n",
       "mean   0.0  \n",
       "std    0.0  \n",
       "min    0.0  \n",
       "25%    0.0  \n",
       "50%    0.0  \n",
       "75%    0.0  \n",
       "max    0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "err_x_cols = ['Err_X[%i]' % i for i in range(N_STATIONS)]\n",
    "err_y_cols = ['Err_Y[%i]' % i for i in range(N_STATIONS)]\n",
    "err_z_cols = ['Err_Z[%i]' % i for i in range(N_STATIONS)]\n",
    "err_xy_cols = err_x_cols + err_y_cols\n",
    "err_xyz_cols = err_xy_cols + err_z_cols\n",
    "ez = np.array([15270., 16470., 17670., 18870.])\n",
    "\n",
    "def add_errs(data, features):\n",
    "    for err_col, e_col, col in zip (err_xy_cols, exy_cols, xy_cols):\n",
    "        data.loc[:, err_col] = data[e_col].values - data[col].values\n",
    "        \n",
    "    for i in range(4):\n",
    "        data.loc[:, err_z_cols[i]] = ez[i] - data[z_cols[i]].values\n",
    "    \n",
    "    features += err_xyz_cols\n",
    "    \n",
    "df_scores, feature_importance = cross_validate(train, n_estimators=60, n_splits=3, n_rows=1000, transformer_cls=DataTransformer)\n",
    "display(df_scores.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_mse(data, features):\n",
    "#     dxy = (data.loc[:, xy_cols].values - data.loc[:, exy_cols].values) / data.loc[:, dx_cols + dy_cols].values / 2.\n",
    "    xy_vals = data.loc[:, xy_cols].values\n",
    "\n",
    "    dxy = (data.loc[:, xy_cols].values - data.loc[:, exy_cols].values) / data.loc[:, dx_cols + dy_cols].values / 2.\n",
    "    D = np.nanmean(dxy**2, axis=1)\n",
    "    \n",
    "    data.loc[:, err_cols[0]] = D\n",
    "    features += [err_cols[0]]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def sample(data, nrows):\n",
    "    return data.iloc[np.random.permutation(len(data.index))[:nrows], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>scr</th>\n",
       "      <th>th</th>\n",
       "      <th>dTh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.781600</td>\n",
       "      <td>0.774079</td>\n",
       "      <td>0.881269</td>\n",
       "      <td>0.824194</td>\n",
       "      <td>0.811250</td>\n",
       "      <td>0.734905</td>\n",
       "      <td>0.092396</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.006525</td>\n",
       "      <td>0.007069</td>\n",
       "      <td>0.002465</td>\n",
       "      <td>0.004571</td>\n",
       "      <td>0.009118</td>\n",
       "      <td>0.023829</td>\n",
       "      <td>0.005642</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.776400</td>\n",
       "      <td>0.769335</td>\n",
       "      <td>0.878464</td>\n",
       "      <td>0.820286</td>\n",
       "      <td>0.805473</td>\n",
       "      <td>0.707415</td>\n",
       "      <td>0.088129</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.777939</td>\n",
       "      <td>0.770017</td>\n",
       "      <td>0.880358</td>\n",
       "      <td>0.821681</td>\n",
       "      <td>0.805994</td>\n",
       "      <td>0.727519</td>\n",
       "      <td>0.089198</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.779478</td>\n",
       "      <td>0.770699</td>\n",
       "      <td>0.882252</td>\n",
       "      <td>0.823076</td>\n",
       "      <td>0.806516</td>\n",
       "      <td>0.747622</td>\n",
       "      <td>0.090268</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.784200</td>\n",
       "      <td>0.776451</td>\n",
       "      <td>0.882672</td>\n",
       "      <td>0.826149</td>\n",
       "      <td>0.814138</td>\n",
       "      <td>0.748650</td>\n",
       "      <td>0.094530</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.788921</td>\n",
       "      <td>0.782204</td>\n",
       "      <td>0.883092</td>\n",
       "      <td>0.829221</td>\n",
       "      <td>0.821761</td>\n",
       "      <td>0.749678</td>\n",
       "      <td>0.098793</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            acc      prec       rec        f1   roc_auc       scr        th  \\\n",
       "count  3.000000  3.000000  3.000000  3.000000  3.000000  3.000000  3.000000   \n",
       "mean   0.781600  0.774079  0.881269  0.824194  0.811250  0.734905  0.092396   \n",
       "std    0.006525  0.007069  0.002465  0.004571  0.009118  0.023829  0.005642   \n",
       "min    0.776400  0.769335  0.878464  0.820286  0.805473  0.707415  0.088129   \n",
       "25%    0.777939  0.770017  0.880358  0.821681  0.805994  0.727519  0.089198   \n",
       "50%    0.779478  0.770699  0.882252  0.823076  0.806516  0.747622  0.090268   \n",
       "75%    0.784200  0.776451  0.882672  0.826149  0.814138  0.748650  0.094530   \n",
       "max    0.788921  0.782204  0.883092  0.829221  0.821761  0.749678  0.098793   \n",
       "\n",
       "       dTh  \n",
       "count  3.0  \n",
       "mean   0.0  \n",
       "std    0.0  \n",
       "min    0.0  \n",
       "25%    0.0  \n",
       "50%    0.0  \n",
       "75%    0.0  \n",
       "max    0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "def filter_unimportant_features(features):\n",
    "    if global_feature_importance is None:\n",
    "        return features\n",
    "    fscore = global_feature_importance\n",
    "    return [col for col in features if col not in fscore.index or fscore.loc[col, 'score'] > 0.01]\n",
    "#     return features\n",
    "\n",
    "class DataTransformer(TransformerMixin):\n",
    "    def __init__(self, *featurizers):\n",
    "        self.featurizers = featurizers\n",
    "    \n",
    "    def fit(self, data, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        data = data.copy()\n",
    "        features = [] + mom_cols + hit_type_cols + dxyz_cols + exy_cols + edxy_cols + hit_stats_cols + t_cols + ncl_cols + avg_cs_cols + xyz_cols\n",
    "        features = filter_unimportant_features(features)\n",
    "        self.origin_features = features.copy()\n",
    "\n",
    "#         add_is_muon(data, features)\n",
    "#         add_is_muon_tight(data, features)\n",
    "#         add_probability_hit_detector(data, features)\n",
    "        add_coses(data, features)\n",
    "        add_mse(data, features)\n",
    "        add_normed_err(data, features)\n",
    "        add_dll(data, features)\n",
    "        add_errs(data, features)\n",
    "        \n",
    "#         filter_data(data)\n",
    "        if not features:\n",
    "            raise('no features')\n",
    "    \n",
    "        features = filter_unimportant_features(features)\n",
    "        self.new_features = features[len(self.origin_features):]\n",
    "        self.features = self.origin_features + self.new_features\n",
    "#         print(len(features))\n",
    "        return data[features].values\n",
    "\n",
    "df_scores, feature_importance = cross_validate(train, n_estimators=60, n_splits=3, n_rows=30000, transformer_cls=DataTransformer)\n",
    "display(df_scores.describe())\n",
    "# display(feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>prec</th>\n",
       "      <th>rec</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>scr</th>\n",
       "      <th>th</th>\n",
       "      <th>dTh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.787160</td>\n",
       "      <td>0.780853</td>\n",
       "      <td>0.881545</td>\n",
       "      <td>0.828147</td>\n",
       "      <td>0.819279</td>\n",
       "      <td>0.748660</td>\n",
       "      <td>0.082433</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.001963</td>\n",
       "      <td>0.002090</td>\n",
       "      <td>0.002466</td>\n",
       "      <td>0.001535</td>\n",
       "      <td>0.002022</td>\n",
       "      <td>0.036916</td>\n",
       "      <td>0.001952</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.784500</td>\n",
       "      <td>0.778890</td>\n",
       "      <td>0.877697</td>\n",
       "      <td>0.825746</td>\n",
       "      <td>0.817059</td>\n",
       "      <td>0.714958</td>\n",
       "      <td>0.079617</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.786050</td>\n",
       "      <td>0.779597</td>\n",
       "      <td>0.880695</td>\n",
       "      <td>0.827620</td>\n",
       "      <td>0.817730</td>\n",
       "      <td>0.725527</td>\n",
       "      <td>0.081694</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.787161</td>\n",
       "      <td>0.779601</td>\n",
       "      <td>0.882424</td>\n",
       "      <td>0.828547</td>\n",
       "      <td>0.819175</td>\n",
       "      <td>0.737313</td>\n",
       "      <td>0.082687</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.789000</td>\n",
       "      <td>0.782605</td>\n",
       "      <td>0.882853</td>\n",
       "      <td>0.829300</td>\n",
       "      <td>0.820341</td>\n",
       "      <td>0.756992</td>\n",
       "      <td>0.083289</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.789089</td>\n",
       "      <td>0.783573</td>\n",
       "      <td>0.884057</td>\n",
       "      <td>0.829523</td>\n",
       "      <td>0.822088</td>\n",
       "      <td>0.808510</td>\n",
       "      <td>0.084876</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            acc      prec       rec        f1   roc_auc       scr        th  \\\n",
       "count  5.000000  5.000000  5.000000  5.000000  5.000000  5.000000  5.000000   \n",
       "mean   0.787160  0.780853  0.881545  0.828147  0.819279  0.748660  0.082433   \n",
       "std    0.001963  0.002090  0.002466  0.001535  0.002022  0.036916  0.001952   \n",
       "min    0.784500  0.778890  0.877697  0.825746  0.817059  0.714958  0.079617   \n",
       "25%    0.786050  0.779597  0.880695  0.827620  0.817730  0.725527  0.081694   \n",
       "50%    0.787161  0.779601  0.882424  0.828547  0.819175  0.737313  0.082687   \n",
       "75%    0.789000  0.782605  0.882853  0.829300  0.820341  0.756992  0.083289   \n",
       "max    0.789089  0.783573  0.884057  0.829523  0.822088  0.808510  0.084876   \n",
       "\n",
       "       dTh  \n",
       "count  5.0  \n",
       "mean   0.0  \n",
       "std    0.0  \n",
       "min    0.0  \n",
       "25%    0.0  \n",
       "50%    0.0  \n",
       "75%    0.0  \n",
       "max    0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 52.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_scores, feature_importance = cross_validate(train, n_estimators=120, n_splits=5, n_rows=100000, transformer_cls=DataTransformer)\n",
    "display(df_scores.describe())\n",
    "# display(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(global_feature_importance.score > .01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ErrMSE</th>\n",
       "      <td>0.136038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PT</th>\n",
       "      <td>0.094272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NShared</th>\n",
       "      <td>0.052506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ncl[0]</th>\n",
       "      <td>0.040573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DAngle[1]</th>\n",
       "      <td>0.039379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DAngle[2]</th>\n",
       "      <td>0.035800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P</th>\n",
       "      <td>0.033413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ncl[2]</th>\n",
       "      <td>0.031026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NErr_X[3]</th>\n",
       "      <td>0.029833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NErr_Y[3]</th>\n",
       "      <td>0.027446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DAngle[3]</th>\n",
       "      <td>0.026253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NErr_X[0]</th>\n",
       "      <td>0.023866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NErr_X[1]</th>\n",
       "      <td>0.021480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ncl[3]</th>\n",
       "      <td>0.019093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DLL</th>\n",
       "      <td>0.019093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lextra_Y[0]</th>\n",
       "      <td>0.017900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mextra_DX2[3]</th>\n",
       "      <td>0.016706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_cs[1]</th>\n",
       "      <td>0.016706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_Y[1]</th>\n",
       "      <td>0.016706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Err_Y[0]</th>\n",
       "      <td>0.015513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_cs[0]</th>\n",
       "      <td>0.014320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NErr_Y[0]</th>\n",
       "      <td>0.014320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_X[3]</th>\n",
       "      <td>0.013126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NErr_Y[1]</th>\n",
       "      <td>0.013126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mextra_DX2[0]</th>\n",
       "      <td>0.013126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_Y[3]</th>\n",
       "      <td>0.011933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ncl[1]</th>\n",
       "      <td>0.009547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_Y[0]</th>\n",
       "      <td>0.009547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mextra_DX2[2]</th>\n",
       "      <td>0.008353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_DX[3]</th>\n",
       "      <td>0.008353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_T[1]</th>\n",
       "      <td>0.002387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lextra_X[3]</th>\n",
       "      <td>0.002387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FOI_hits_N</th>\n",
       "      <td>0.002387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_TYPE[0]</th>\n",
       "      <td>0.002387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Err_X[0]</th>\n",
       "      <td>0.002387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Err_Y[3]</th>\n",
       "      <td>0.001193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Err_Y[2]</th>\n",
       "      <td>0.001193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_DY[3]</th>\n",
       "      <td>0.001193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mextra_DY2[1]</th>\n",
       "      <td>0.001193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lextra_Y[2]</th>\n",
       "      <td>0.001193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_DZ[0]</th>\n",
       "      <td>0.001193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_TYPE[3]</th>\n",
       "      <td>0.001193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lextra_X[2]</th>\n",
       "      <td>0.001193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Err_Z[0]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Err_Z[1]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_DZ[2]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Err_Z[2]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_DZ[1]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_DY[1]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lextra_Y[1]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_DZ[3]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_X[1]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mextra_DY2[2]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_DX[1]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ndof</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_TYPE[2]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_T[2]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_T[3]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MatchedHit_DX[0]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Err_Z[3]</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       score\n",
       "ErrMSE              0.136038\n",
       "PT                  0.094272\n",
       "NShared             0.052506\n",
       "ncl[0]              0.040573\n",
       "DAngle[1]           0.039379\n",
       "DAngle[2]           0.035800\n",
       "P                   0.033413\n",
       "ncl[2]              0.031026\n",
       "NErr_X[3]           0.029833\n",
       "NErr_Y[3]           0.027446\n",
       "DAngle[3]           0.026253\n",
       "NErr_X[0]           0.023866\n",
       "NErr_X[1]           0.021480\n",
       "ncl[3]              0.019093\n",
       "DLL                 0.019093\n",
       "Lextra_Y[0]         0.017900\n",
       "Mextra_DX2[3]       0.016706\n",
       "avg_cs[1]           0.016706\n",
       "MatchedHit_Y[1]     0.016706\n",
       "Err_Y[0]            0.015513\n",
       "avg_cs[0]           0.014320\n",
       "NErr_Y[0]           0.014320\n",
       "MatchedHit_X[3]     0.013126\n",
       "NErr_Y[1]           0.013126\n",
       "Mextra_DX2[0]       0.013126\n",
       "MatchedHit_Y[3]     0.011933\n",
       "ncl[1]              0.009547\n",
       "MatchedHit_Y[0]     0.009547\n",
       "Mextra_DX2[2]       0.008353\n",
       "MatchedHit_DX[3]    0.008353\n",
       "...                      ...\n",
       "MatchedHit_T[1]     0.002387\n",
       "Lextra_X[3]         0.002387\n",
       "FOI_hits_N          0.002387\n",
       "MatchedHit_TYPE[0]  0.002387\n",
       "Err_X[0]            0.002387\n",
       "Err_Y[3]            0.001193\n",
       "Err_Y[2]            0.001193\n",
       "MatchedHit_DY[3]    0.001193\n",
       "Mextra_DY2[1]       0.001193\n",
       "Lextra_Y[2]         0.001193\n",
       "MatchedHit_DZ[0]    0.001193\n",
       "MatchedHit_TYPE[3]  0.001193\n",
       "Lextra_X[2]         0.001193\n",
       "Err_Z[0]            0.000000\n",
       "Err_Z[1]            0.000000\n",
       "MatchedHit_DZ[2]    0.000000\n",
       "Err_Z[2]            0.000000\n",
       "MatchedHit_DZ[1]    0.000000\n",
       "MatchedHit_DY[1]    0.000000\n",
       "Lextra_Y[1]         0.000000\n",
       "MatchedHit_DZ[3]    0.000000\n",
       "MatchedHit_X[1]     0.000000\n",
       "Mextra_DY2[2]       0.000000\n",
       "MatchedHit_DX[1]    0.000000\n",
       "ndof                0.000000\n",
       "MatchedHit_TYPE[2]  0.000000\n",
       "MatchedHit_T[2]     0.000000\n",
       "MatchedHit_T[3]     0.000000\n",
       "MatchedHit_DX[0]    0.000000\n",
       "Err_Z[3]            0.000000\n",
       "\n",
       "[86 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(global_feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_feature_importance = feature_importance.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = read_pub_test(used_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 24.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fit_predict_save(sample(train, 100000), test, \"out/09_importance_001_100.csv\", n_estimators=120, transformer_cls=DataTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fit_predict_save(sample(train, 1000000), test, \"out/09_plus_all_orig_features_1000_120.csv\", n_estimators=120, transformer_cls=DataTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fit_predict_save(sample(train, 1000000), test, \"out/09_plus_all_orig_features_1000_200.csv\", n_estimators=200, transformer_cls=DataTransformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fit_save_model(sample(train, 100000), \"models/07_dumb_cols.xgb\", n_estimators=120, transformer_cls=DataTransformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame([[1.0, np.NaN], [np.NaN, np.NaN], [2.0, 3.1]], columns=['a', 'b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.cosine import da_cols\n",
    "from transformers.err import err_cols, nerr_xy_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = read_train(SIMPLE_FEATURE_COLS + ARR_FEATURE_COLS, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89534,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = np.hstack(dt.loc[:, foi_cols[0]].values)\n",
    "xx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'any'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-fabb4ed90258>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mget_matched_hits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mexy_cols\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfoi_cols\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-94-fabb4ed90258>\u001b[0m in \u001b[0;36mget_matched_hits\u001b[1;34m(exy, foi_info)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mhits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfoi_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfoi_ts_cols\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mhits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m             \u001b[0mx_distances_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mexy_cols\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mfoi_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfoi_xyz_cols\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhits\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0my_distances_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mexy_cols\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mfoi_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfoi_xyz_cols\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhits\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'bool' object has no attribute 'any'"
     ]
    }
   ],
   "source": [
    "def to_closest_cols(cols):\n",
    "    return ['Cl_' + col for col in cols]\n",
    "\n",
    "cl_xyz_cols = to_closest_cols(xyz_cols)\n",
    "cl_t_cols = to_closest_cols(t_cols)\n",
    "cl_dxyz_cols = to_closest_cols(dxyz_cols)\n",
    "cl_cols = cl_xyz_cols + cl_t_cols + cl_dxyz_cols\n",
    "\n",
    "# def fill_station()\n",
    "\n",
    "def get_matched_hits(exy, foi_info):\n",
    "    print(type(foi_info[foi_ts_cols[-1]]))\n",
    "    result = np.empty(len(cl_cols), dtype=np.float32)\n",
    "    for i in range(4):\n",
    "        hits = (foi_info[foi_ts_cols[2]] == i)\n",
    "        if hits.any():\n",
    "            x_distances_2 = (exy[exy_cols[i]] - foi_info[foi_xyz_cols[i]][hits])**2\n",
    "            y_distances_2 = (exy[exy_cols[i+4]] - foi_info[foi_xyz_cols[i+4]][hits])**2\n",
    "            distances_2 = x_distances_2 + y_distances_2\n",
    "            \n",
    "            closest_hit = np.argmin(distances_2)\n",
    "            return closest_hit\n",
    "#             closest_x_per_station[station] = x_distances_2[closest_hit]\n",
    "#             closest_y_per_station[station] = y_distances_2[closest_hit]\n",
    "#             closest_T_per_station[station] = row[\"FOI_hits_T\"][hits][closest_hit]\n",
    "#             closest_z_per_station[station] = row[\"FOI_hits_Z\"][hits][closest_hit]\n",
    "#             closest_dx_per_station[station] = row[\"FOI_hits_DX\"][hits][closest_hit]\n",
    "#             closest_dy_per_station[station] = row[\"FOI_hits_DY\"][hits][closest_hit]\n",
    "\n",
    "for i in range(len(dt.index)):\n",
    "    row = dt.iloc[i, :]\n",
    "    get_matched_hits(row[exy_cols], row[foi_cols])\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
