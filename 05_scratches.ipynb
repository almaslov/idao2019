{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABUkAAABECAYAAAC8urRAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAA4JJREFUeJzt2zFLVWEcx/H/DYmgSYoCc7Bw0KDFcyVoCaLNNxAS1FBRSw0NTYEUBA0NtRTVEoT0BtoqaAlCj0uQDlIOt6AonAKJ4PYG1Kmnp+7/8xnPs/wOHM7whafT7/cDAAAAACCrHbUHAAAAAADUJJICAAAAAKmJpAAAAABAaiIpAAAAAJCaSAoAAAAApDa03WHbtv2/NQQAAAAAoKSmaTqbPd82kkZEzM33/vyaf8Tc7GjcXn1ce0Yx18bPxcaNW7VnFPFy/GzMzI7E8/nPtacUMTM7EvH9We0Z5ew5FQ++bPpPGggX9/fj8NMXtWcU8/70yVhfOF97RjHD04/iZm8w3+/66KO4tLZSe0Yx98cm4sryWO0ZxdydXIsDDydqzyjm04WV6F5tas8oZvFOG0cPXq49o4i3H+/FaPd47RnF9BZfx6sz+2rPKObEk6/RbHRrzyim3bUY3d0D/G/50cabn4P5fsd2trEw/qH2jGKmVw9F826p9oxi2iNTMbQ+mN9mRMSv4TaaqeXaM4pplyajt3ek9owiRr9t3ZFctwcAAAAAUhNJAQAAAIDURFIAAAAAIDWRFAAAAABITSQFAAAAAFITSQEAAACA1ERSAAAAACA1kRQAAAAASE0kBQAAAABSE0kBAAAAgNREUgAAAAAgNZEUAAAAAEhNJAUAAAAAUhNJAQAAAIDURFIAAAAAIDWRFAAAAABITSQFAAAAAFITSQEAAACA1ERSAAAAACA1kRQAAAAASE0kBQAAAABSE0kBAAAAgNREUgAAAAAgNZEUAAAAAEhNJAUAAAAAUhNJAQAAAIDURFIAAAAAIDWRFAAAAABITSQFAAAAAFITSQEAAACA1ERSAAAAACA1kRQAAAAASE0kBQAAAABSE0kBAAAAgNREUgAAAAAgNZEUAAAAAEhNJAUAAAAAUhNJAQAAAIDURFIAAAAAIDWRFAAAAABITSQFAAAAAFITSQEAAACA1ERSAAAAACA1kRQAAAAASE0kBQAAAABSE0kBAAAAgNREUgAAAAAgNZEUAAAAAEhNJAUAAAAAUhNJAQAAAIDURFIAAAAAIDWRFAAAAABITSQFAAAAAFLr9Pv9LQ/btt36EAAAAADgP9I0TWez59tGUgAAAACAQee6PQAAAACQmkgKAAAAAKQmkgIAAAAAqYmkAAAAAEBqIikAAAAAkNpvuTVZgasyZKkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1728x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as sts\n",
    "import seaborn as sns\n",
    "import utils\n",
    "import scoring\n",
    "\n",
    "sns.set()\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "color_palette = sns.color_palette('deep') + sns.color_palette('husl', 6) + sns.color_palette('bright') + sns.color_palette('pastel')\n",
    "\n",
    "%matplotlib inline\n",
    "sns.palplot(color_palette)\n",
    "\n",
    "def ndprint(a, precision=3):\n",
    "    with np.printoptions(precision=precision, suppress=True):\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_features(data):\n",
    "    return data.loc[:, FEATURES_COLS]\n",
    "\n",
    "x_cols = ['MatchedHit_X[%i]' % i for i in range(utils.N_STATIONS)]\n",
    "y_cols = ['MatchedHit_Y[%i]' % i for i in range(utils.N_STATIONS)]\n",
    "z_cols = ['MatchedHit_Z[%i]' % i for i in range(utils.N_STATIONS)]\n",
    "\n",
    "da_cols = ['MatchedHit_DA[%d]' % i for i in range(1, 4)]\n",
    "da_cols = ['MatchedHit_DA[%d]' % i for i in range(1, 4)]\n",
    "\n",
    "FEATURES_COLS = [\n",
    "#     'MatchedHit_TYPE[0]', 'MatchedHit_TYPE[1]', 'MatchedHit_TYPE[2]', 'MatchedHit_TYPE[3]',\n",
    "    'MatchedHit_X[0]', 'MatchedHit_X[1]', 'MatchedHit_X[2]', 'MatchedHit_X[3]',\n",
    "    'MatchedHit_Y[0]', 'MatchedHit_Y[1]', 'MatchedHit_Y[2]', 'MatchedHit_Y[3]', \n",
    "    'MatchedHit_Z[0]', 'MatchedHit_Z[1]', 'MatchedHit_Z[2]', 'MatchedHit_Z[3]',\n",
    "#     'MatchedHit_DX[0]', 'MatchedHit_DX[1]', 'MatchedHit_DX[2]', 'MatchedHit_DX[3]',\n",
    "#     'MatchedHit_DY[0]', 'MatchedHit_DY[1]', 'MatchedHit_DY[2]', 'MatchedHit_DY[3]',\n",
    "#     'MatchedHit_DZ[0]', 'MatchedHit_DZ[1]', 'MatchedHit_DZ[2]', 'MatchedHit_DZ[3]',\n",
    "#     'MatchedHit_T[0]', 'MatchedHit_T[1]', 'MatchedHit_T[2]', 'MatchedHit_T[3]',\n",
    "#     'MatchedHit_DT[0]', 'MatchedHit_DT[1]', 'MatchedHit_DT[2]', 'MatchedHit_DT[3]',\n",
    "#     'Lextra_X[0]', 'Lextra_X[1]', 'Lextra_X[2]', 'Lextra_X[3]',\n",
    "#     'Lextra_Y[0]', 'Lextra_Y[1]', 'Lextra_Y[2]', 'Lextra_Y[3]',\n",
    "#     'Mextra_DX2[0]', 'Mextra_DX2[1]', 'Mextra_DX2[2]', 'Mextra_DX2[3]',\n",
    "#     'Mextra_DY2[0]', 'Mextra_DY2[1]', 'Mextra_DY2[2]', 'Mextra_DY2[3]',\n",
    "#     'FOI_hits_N', 'PT', 'P'\n",
    "]\n",
    "\n",
    "TRAINSET_COLS = [utils.ID_COLUMN] + FEATURES_COLS + utils.TRAIN_COLUMNS\n",
    "TESTSET_COLS = [utils.ID_COLUMN] + FEATURES_COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 14) (726095, 12)\n",
      "Wall time: 8.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_00 = pd.read_csv('data/train_part_1_v2.csv.gz', nrows=200000, na_values='-9999.0', usecols=TRAINSET_COLS, index_col=utils.ID_COLUMN)\n",
    "train_01 = pd.read_csv('data/train_part_2_v2.csv.gz', nrows=200000, na_values='-9999.0', usecols=TRAINSET_COLS, index_col=utils.ID_COLUMN)\n",
    "train_0 = pd.concat([train_00, train_01], axis=0, ignore_index=False)\n",
    "\n",
    "# test_0 = pd.read_csv('data/test_public_v2.csv.gz', na_values='-9999.0', usecols=TESTSET_COLS, index_col=utils.ID_COLUMN)\n",
    "\n",
    "print(train_0.shape, test_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_0\n",
    "test = test_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30588 369412\n"
     ]
    }
   ],
   "source": [
    "def get_class(i):\n",
    "    return train.index[train.label == i]\n",
    "\n",
    "# Gets `n_rows` random samples from `data`\n",
    "def get_samples(data, n_rows):\n",
    "    indices = np.random.randint(len(data), size=n_rows)\n",
    "    indices = np.sort(indices)\n",
    "    return data.iloc[indices]\n",
    "\n",
    "# Gets `n_rows` random samples from `data` with specified class proportions.\n",
    "# If `prop_0` is None, it keeps `data`s natural proportions.\n",
    "def get_samples_w_proptions(data, n_rows, prop_0=None):\n",
    "    def get_class_samples(class_i, n_rows):\n",
    "        class_indices = np.random.randint(len(class_i), size=n_rows)\n",
    "        return class_i[class_indices]\n",
    "    \n",
    "    if prop_0 is None:\n",
    "        prop_0 = len(class_0) / (len(class_0) + len(class_1))\n",
    "    cnt_0 = int(n_rows * prop_0)\n",
    "    class_0_indices = get_class_samples(class_0, cnt_0)\n",
    "    class_1_indices = get_class_samples(class_1, n_rows - cnt_0)\n",
    "    indices = np.concatenate((class_0_indices, class_1_indices))\n",
    "    indices = np.sort(indices)\n",
    "    return data.loc[indices]\n",
    "\n",
    "# Gets `n_rows` head samples from `data` with specified class proportions.\n",
    "# If `prop_0` is None, it keeps `data`s natural proportions.\n",
    "# If `data` samples is not enough to fulfil proportions for any `class_i` then random sampling from the `class_i` is applied.\n",
    "def get_head_w_proportions(data, n_rows, prop_0=None):\n",
    "    def get_class_samples_head_smart(class_i, n_rows):\n",
    "        cls_len = len(class_i)\n",
    "        if cls_len > n_rows:\n",
    "            return class_i[:n_rows]\n",
    "        class_indices = np.concatenate((np.arange(cls_len),  np.random.randint(cls_len, size=n_rows-cls_len)))\n",
    "        return class_i[class_indices]\n",
    "    \n",
    "    if prop_0 is None:\n",
    "        prop_0 = len(class_0) / (len(class_0) + len(class_1))\n",
    "    cnt_0 = int(n_rows * prop_0)\n",
    "    class_0_indices = get_class_samples_head_smart(class_0, cnt_0)\n",
    "    class_1_indices = get_class_samples_head_smart(class_1, n_rows - cnt_0)\n",
    "    indices = np.concatenate((class_0_indices, class_1_indices))\n",
    "    indices = np.sort(indices)\n",
    "    return data.loc[indices]\n",
    "    \n",
    "print(np.count_nonzero(train.label == 0), np.count_nonzero(train.label == 1))\n",
    "\n",
    "class_0 = get_class(0)\n",
    "class_1 = get_class(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn import model_selection as mdsel\n",
    "\n",
    "def fit(train):\n",
    "    target_train = train.loc[:, utils.TRAIN_COLUMNS]\n",
    "    \n",
    "    # defined much later\n",
    "    transformer = DataTransformer().fit(train)\n",
    "    train = transformer.transform(train)\n",
    "    \n",
    "    estimator = xgb.XGBClassifier(n_estimators=150, n_jobs=3)\n",
    "    estimator.fit(train.values, target_train.label.values, sample_weight=target_train.weight.values, eval_metric=scoring.rejection90_sklearn)\n",
    "    return transformer, estimator\n",
    "    \n",
    "def predict(fitted_state, test):\n",
    "    transformer, estimator = fitted_state\n",
    "    \n",
    "    test = transformer.transform(test)\n",
    "    predictions = estimator.predict_proba(test.values)[:, 1]\n",
    "    return predictions\n",
    "\n",
    "def score(fitted_state, test):\n",
    "    target_test = test.loc[:, utils.TRAIN_COLUMNS]\n",
    "    predictions = predict(fitted_state, test)\n",
    "    score = scoring.rejection90(target_test.label.values, predictions, sample_weight=target_test.weight.values)\n",
    "    return score\n",
    "\n",
    "def fit_predict_save(train, test, filename):\n",
    "    fitted_state = fit(train)\n",
    "    predictions = predict(fitted_state, test)\n",
    "    \n",
    "    pd.DataFrame(data={\"prediction\": predictions}, index=test.index).to_csv(\n",
    "        filename, index_label=utils.ID_COLUMN\n",
    "    )\n",
    "    \n",
    "def cross_validate(train, n_splits, n_rows=1000):\n",
    "    # сделать честную случайную подвыборку\n",
    "    train = get_head_w_proportions(train, n_rows, .5)\n",
    "    \n",
    "    splitter = mdsel.StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "    scores = []\n",
    "    for train_indices, test_indices in splitter.split(train, train.label):\n",
    "        train_subset = train.iloc[train_indices]\n",
    "        test_subset = train.iloc[test_indices]\n",
    "        \n",
    "        fit_state = fit(train_subset)\n",
    "        scores += [score(fit_state, test_subset)]\n",
    "\n",
    "    return pd.DataFrame(scores, columns=['Score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transformer\n",
    "\n",
    "Это по сути основная часть. Класс, который отбирает нужные столбцы, возможно что-то модифицирует или добавляет. На выходе - входные данные для модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def fill_na(data):\n",
    "    mask = data.isna()\n",
    "    means = data.mean(skipna=True)\n",
    "    data.fillna(means, inplace=True)\n",
    "    return mask\n",
    "\n",
    "def restore_na(data, mask):\n",
    "    data.mask(mask, other=np.NaN, inplace=True)\n",
    "    \n",
    "def get_nth_detector_coords(i):\n",
    "    return [x_cols[i], y_cols[i], z_cols[i]]\n",
    "    \n",
    "def fit_pca(data):\n",
    "    cols = get_nth_detector_coords(0)\n",
    "    data = data.loc[:, cols].copy()\n",
    "    \n",
    "    fill_na(data)\n",
    "    pca_model = PCA(n_components=3)\n",
    "    pca_model.fit(data)\n",
    "    return pca_model\n",
    "\n",
    "def transform_pca(pca_model, data):\n",
    "    for i in range(4):\n",
    "        cols = get_nth_detector_coords(i)\n",
    "        data_detector = data.loc[:, cols]\n",
    "        \n",
    "        mask = fill_na(data_detector)\n",
    "        data_detector.update(pca_model.transform(data_detector.values))\n",
    "        restore_na(data_detector, mask)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cosine dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_coses(data):\n",
    "    def get_layer_coords(data, i):\n",
    "        return data[[x_cols[i], y_cols[i], z_cols[i]]].values\n",
    "\n",
    "    def dot(x, y):\n",
    "        return np.sum(x * y, axis=1)\n",
    "    \n",
    "    def norm(x):\n",
    "        return np.sqrt(dot(x, x))\n",
    "\n",
    "    def get_cosine_dist(L1, L2, L1_norm, L2_norm):\n",
    "        return dot(L1, L2) / L1_norm / L2_norm\n",
    "    \n",
    "    def get_angle(cosines):\n",
    "        return np.arccos(cosines, dtype=np.float32) / np.pi * 180\n",
    "    \n",
    "    layers = np.array([get_layer_coords(data, i) for i in range(4)])\n",
    "    layers[1:] -= layers[:3]\n",
    "    norms = list(map(norm, layers))\n",
    "    \n",
    "    for i in range(3):\n",
    "        cosines = get_cosine_dist(layers[i], layers[i+1], norms[i], norms[i+1])\n",
    "        angles = get_angle(cosines)\n",
    "        data[da_cols[i]] = angles\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class DataTransformer(TransformerMixin):\n",
    "    def __init__(self, *featurizers):\n",
    "        self.featurizers = featurizers\n",
    "\n",
    "    def fit(self, data, y=None):\n",
    "        self.pca_model = fit_pca(data)\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        data = get_simple_features(data)\n",
    "        \n",
    "        transform_pca(self.pca_model, data)\n",
    "        add_coses(data)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Score\n",
      "count  5.000000\n",
      "mean   0.630520\n",
      "std    0.028313\n",
      "min    0.605589\n",
      "25%    0.615881\n",
      "50%    0.622852\n",
      "75%    0.629685\n",
      "max    0.678594\n",
      "Wall time: 33.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_scores = cross_validate(train, n_splits=5, n_rows=100000)\n",
    "print(df_scores.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fit_predict_save(get_head_w_proportions(train, 100000), test, \"out/05_no_props_4m_4m.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fit_predict_save(get_samples_w_proptions(train, 100000, .8), test, \"out/04_prop_80_20_100_800.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fit_predict_save(train.iloc[:100000], test, \"out/03_baseline_head_100.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame([[1.0, np.NaN], [np.NaN, np.NaN], [2.0, 3.1]], columns=['a', 'b'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
